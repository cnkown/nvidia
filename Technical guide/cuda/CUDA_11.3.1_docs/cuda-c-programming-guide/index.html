<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-us" xml:lang="en-us">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>
      <meta name="copyright" content="(C) Copyright 2005"></meta>
      <meta name="DC.rights.owner" content="(C) Copyright 2005"></meta>
      <meta name="DC.Type" content="concept"></meta>
      <meta name="DC.Title" content="CUDA C++ Programming Guide"></meta>
      <meta name="abstract" content="The programming guide to the CUDA model and interface."></meta>
      <meta name="description" content="The programming guide to the CUDA model and interface."></meta>
      <meta name="DC.Coverage" content="Programming Guides"></meta>
      <meta name="DC.subject" content="CUDA C++, CUDA C++ programming model, CUDA C++ programming interface, CUDA C++ performance guidelines, CUDA C++ language extensions, CUDA C++ mathematical functions"></meta>
      <meta name="keywords" content="CUDA C++, CUDA C++ programming model, CUDA C++ programming interface, CUDA C++ performance guidelines, CUDA C++ language extensions, CUDA C++ mathematical functions"></meta>
      <meta name="DC.Format" content="XHTML"></meta>
      <meta name="DC.Identifier" content="abstract"></meta>
      <link rel="stylesheet" type="text/css" href="../common/formatting/commonltr.css"></link>
      <link rel="stylesheet" type="text/css" href="../common/formatting/site.css"></link>
      <title>Programming Guide :: CUDA Toolkit Documentation</title>
      <!--[if lt IE 9]>
      <script src="../common/formatting/html5shiv-printshiv.min.js"></script>
      <![endif]-->
      <script type="text/javascript" charset="utf-8" src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script type="text/javascript" charset="utf-8" src="../common/scripts/tynt/tynt.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.ba-hashchange.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.scrollintoview.min.js"></script>
      <script type="text/javascript" src="../search/htmlFileList.js"></script>
      <script type="text/javascript" src="../search/htmlFileInfoList.js"></script>
      <script type="text/javascript" src="../search/nwSearchFnt.min.js"></script>
      <script type="text/javascript" src="../search/stemmers/en_stemmer.min.js"></script>
      <script type="text/javascript" src="../search/index-1.js"></script>
      <script type="text/javascript" src="../search/index-2.js"></script>
      <script type="text/javascript" src="../search/index-3.js"></script>
      <link rel="canonical" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html"></link>
      <link rel="stylesheet" type="text/css" href="../common/formatting/qwcode.highlight.css"></link>
   </head>
   <body>
      
      <header id="header"><span id="company">NVIDIA</span><span id="site-title">CUDA Toolkit Documentation</span><form id="search" method="get" action="search">
            <input type="text" name="search-text"></input><fieldset id="search-location">
               <legend>Search In:</legend>
               <label><input type="radio" name="search-type" value="site"></input>Entire Site</label>
               <label><input type="radio" name="search-type" value="document"></input>Just This Document</label></fieldset>
            <button type="reset">clear search</button>
            <button id="submit" type="submit">search</button></form>
      </header>
      <div id="site-content">
         <nav id="site-nav">
            <div class="category closed"><a href="../index.html" title="The root of the site.">CUDA Toolkit 
                  
                  
                  v11.3.1</a></div>
            <div class="category"><a href="index.html" title="Programming Guide">Programming Guide</a></div>
            <ul>
               <li>
                  <div class="section-link"><a href="#introduction">1.&nbsp;Introduction</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#from-graphics-processing-to-general-purpose-parallel-computing">1.1.&nbsp;The Benefits of Using GPUs</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#cuda-general-purpose-parallel-computing-architecture">1.2.&nbsp;CUDA: A General-Purpose Parallel Computing Platform and Programming Model</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#scalable-programming-model">1.3.&nbsp;A Scalable Programming Model</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#document-structure">1.4.&nbsp;Document Structure</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#programming-model">2.&nbsp;Programming Model</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#kernels">2.1.&nbsp;Kernels</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#thread-hierarchy">2.2.&nbsp;Thread Hierarchy</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#memory-hierarchy">2.3.&nbsp;Memory Hierarchy</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#heterogeneous-programming">2.4.&nbsp;Heterogeneous Programming</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-capability">2.5.&nbsp;Compute Capability</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#programming-interface">3.&nbsp;Programming Interface</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#compilation-with-nvcc">3.1.&nbsp;Compilation with NVCC</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#compilation-workflow">3.1.1.&nbsp;Compilation Workflow</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#offline-compilation">3.1.1.1.&nbsp;Offline Compilation</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#just-in-time-compilation">3.1.1.2.&nbsp;Just-in-Time Compilation</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#binary-compatibility">3.1.2.&nbsp;Binary Compatibility</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#ptx-compatibility">3.1.3.&nbsp;PTX Compatibility</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#application-compatibility">3.1.4.&nbsp;Application Compatibility</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#c-cplusplus-compatibility">3.1.5.&nbsp;C++ Compatibility</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#sixtyfour-bit-compatibility">3.1.6.&nbsp;64-Bit Compatibility</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#cuda-c-runtime">3.2.&nbsp;CUDA Runtime</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#initialization">3.2.1.&nbsp;Initialization</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#device-memory">3.2.2.&nbsp;Device Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#L2_access_intro">3.2.3.&nbsp;Device Memory L2 Access Management</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#L2_set_aside">3.2.3.1.&nbsp;L2 cache Set-Aside for Persisting Accesses</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_access_policy">3.2.3.2.&nbsp;L2 Policy for Persisting Accesses</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_access_prop">3.2.3.3.&nbsp;L2 Access Properties</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_simple_example">3.2.3.4.&nbsp;L2 Persistence Example</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_reset_to_normal">3.2.3.5.&nbsp;Reset L2 Access to Normal</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_managing_utilization">3.2.3.6.&nbsp;Manage Utilization of L2 set-aside cache</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_cache_query">3.2.3.7.&nbsp;Query L2 cache Properties</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#L2_cache_getset_size">3.2.3.8.&nbsp;Control L2 Cache Set-Aside Size for Persisting Memory Access</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory">3.2.4.&nbsp;Shared Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#page-locked-host-memory">3.2.5.&nbsp;Page-Locked Host Memory</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#portable-memory">3.2.5.1.&nbsp;Portable Memory</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#write-combining-memory">3.2.5.2.&nbsp;Write-Combining Memory</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#mapped-memory">3.2.5.3.&nbsp;Mapped Memory</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#asynchronous-concurrent-execution">3.2.6.&nbsp;Asynchronous Concurrent Execution</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#concurrent-execution-host-device">3.2.6.1.&nbsp;Concurrent Execution between Host and Device</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#concurrent-kernel-execution">3.2.6.2.&nbsp;Concurrent Kernel Execution</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#overlap-of-data-transfer-and-kernel-execution">3.2.6.3.&nbsp;Overlap of Data Transfer and Kernel Execution</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#concurrent-data-transfers">3.2.6.4.&nbsp;Concurrent Data Transfers</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#streams">3.2.6.5.&nbsp;Streams</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#creation-and-destruction-streams">3.2.6.5.1.&nbsp;Creation and Destruction</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#default-stream">3.2.6.5.2.&nbsp;Default Stream</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#explicit-synchronization">3.2.6.5.3.&nbsp;Explicit Synchronization</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#implicit-synchronization">3.2.6.5.4.&nbsp;Implicit Synchronization</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#overlapping-behavior">3.2.6.5.5.&nbsp;Overlapping Behavior</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#stream-callbacks">3.2.6.5.6.&nbsp;Host Functions (Callbacks)</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#stream-priorities">3.2.6.5.7.&nbsp;Stream Priorities</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#cuda-graphs">3.2.6.6.&nbsp;CUDA Graphs</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#graph-structure">3.2.6.6.1.&nbsp;Graph Structure</a></div>
                                          <ul>
                                             <li>
                                                <div class="section-link"><a href="#node-types">3.2.6.6.1.1.&nbsp;Node Types</a></div>
                                             </li>
                                          </ul>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#creating-a-graph-using-api">3.2.6.6.2.&nbsp;Creating a Graph Using Graph APIs</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#creating-a-graph-using-stream-capture">3.2.6.6.3.&nbsp;Creating a Graph Using Stream Capture</a></div>
                                          <ul>
                                             <li>
                                                <div class="section-link"><a href="#cross-stream-dependencies">3.2.6.6.3.1.&nbsp;Cross-stream Dependencies and Events</a></div>
                                             </li>
                                             <li>
                                                <div class="section-link"><a href="#prohibited-unhandled-operations">3.2.6.6.3.2.&nbsp;Prohibited and Unhandled Operations </a></div>
                                             </li>
                                             <li>
                                                <div class="section-link"><a href="#invalidation">3.2.6.6.3.3.&nbsp;Invalidation </a></div>
                                             </li>
                                          </ul>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#updating-instantiated-graphs">3.2.6.6.4.&nbsp;Updating Instantiated Graphs</a></div>
                                          <ul>
                                             <li>
                                                <div class="section-link"><a href="#graph-update-limitations">3.2.6.6.4.1.&nbsp;Graph Update Limitations</a></div>
                                             </li>
                                             <li>
                                                <div class="section-link"><a href="#whole-graph-update">3.2.6.6.4.2.&nbsp;Whole Graph Update</a></div>
                                             </li>
                                             <li>
                                                <div class="section-link"><a href="#individual-node-update">3.2.6.6.4.3.&nbsp;Individual node update</a></div>
                                             </li>
                                          </ul>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#using-graph-apis">3.2.6.6.5.&nbsp;Using Graph APIs </a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#events">3.2.6.7.&nbsp;Events</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#creation-and-destruction-events">3.2.6.7.1.&nbsp;Creation and Destruction</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#elapsed-time">3.2.6.7.2.&nbsp;Elapsed Time</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#synchronous-calls">3.2.6.8.&nbsp;Synchronous Calls</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#multi-device-system">3.2.7.&nbsp;Multi-Device System</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#device-enumeration">3.2.7.1.&nbsp;Device Enumeration</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#device-selection">3.2.7.2.&nbsp;Device Selection</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#stream-and-event-behavior">3.2.7.3.&nbsp;Stream and Event Behavior</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#peer-to-peer-memory-access">3.2.7.4.&nbsp;Peer-to-Peer Memory Access</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#iommu-on-linux">3.2.7.4.1.&nbsp;IOMMU on Linux</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#peer-to-peer-memory-copy">3.2.7.5.&nbsp;Peer-to-Peer Memory Copy</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#unified-virtual-address-space">3.2.8.&nbsp;Unified Virtual Address Space</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#interprocess-communication">3.2.9.&nbsp;Interprocess Communication</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#error-checking">3.2.10.&nbsp;Error Checking</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#call-stack">3.2.11.&nbsp;Call Stack</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#texture-and-surface-memory">3.2.12.&nbsp;Texture and Surface Memory</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#texture-memory">3.2.12.1.&nbsp;Texture Memory</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#texture-object-api">3.2.12.1.1.&nbsp;Texture Object API</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#texture-reference-api">3.2.12.1.2.&nbsp;[[DEPRECATED]] Texture Reference API</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#sixteen-bit-floating-point-textures">3.2.12.1.3.&nbsp;16-Bit Floating-Point Textures</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#layered-textures">3.2.12.1.4.&nbsp;Layered Textures</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#cubemap-textures">3.2.12.1.5.&nbsp;Cubemap Textures</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#cubemap-layered-textures">3.2.12.1.6.&nbsp;Cubemap Layered Textures</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#texture-gather">3.2.12.1.7.&nbsp;Texture Gather</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surface-memory">3.2.12.2.&nbsp;Surface Memory</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#surface-object-api">3.2.12.2.1.&nbsp;Surface Object API</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#surface-reference-api">3.2.12.2.2.&nbsp;[[DEPRECATED]] Surface Reference API</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#cubemap-surfaces">3.2.12.2.3.&nbsp;Cubemap Surfaces</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#cubemap-layered-surfaces">3.2.12.2.4.&nbsp;Cubemap Layered Surfaces</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#cuda-arrays">3.2.12.3.&nbsp;CUDA Arrays</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#read-write-coherency">3.2.12.4.&nbsp;Read/Write Coherency</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#graphics-interoperability">3.2.13.&nbsp;Graphics Interoperability</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#opengl-interoperability">3.2.13.1.&nbsp;OpenGL Interoperability</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#direct3d-interoperability">3.2.13.2.&nbsp;Direct3D Interoperability</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#direct3d-9-version">3.2.13.2.1.&nbsp;Direct3D 9 Version</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#direct3d-10-version">3.2.13.2.2.&nbsp;Direct3D 10 Version</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#direct3d-11-version">3.2.13.2.3.&nbsp;Direct3D 11 Version</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#sli-interoperability">3.2.13.3.&nbsp;SLI Interoperability</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#external-resource-interoperability">3.2.14.&nbsp;External Resource Interoperability</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#vulkan-interoperability">3.2.14.1.&nbsp;Vulkan Interoperability</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#matching-device-uuids-vul-int">3.2.14.1.1.&nbsp;Matching device UUIDs</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-memory-objects-vul-int">3.2.14.1.2.&nbsp;Importing memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-buffers-onto-imported-memory-objects-vul-int">3.2.14.1.3.&nbsp;Mapping buffers onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-vul-int">3.2.14.1.4.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-synchronization-objects-vul-int">3.2.14.1.5.&nbsp;Importing synchronization objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#signaling-waiting-on-imported-synchronization-objects-vul-int">3.2.14.1.6.&nbsp;Signaling/waiting on imported synchronization objects</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#opengl-interoperability-ext-res-int">3.2.14.2.&nbsp;OpenGL Interoperability</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#direct3d-12-interoperability">3.2.14.3.&nbsp;Direct3D 12 Interoperability</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#matching-device-luids-dir3d-12-int">3.2.14.3.1.&nbsp;Matching device LUIDs</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-memory-objects-dir3d-12-int">3.2.14.3.2.&nbsp;Importing memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-buffers-onto-imported-memory-objects-dir3d-12-int">3.2.14.3.3.&nbsp;Mapping buffers onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-12-int">3.2.14.3.4.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-synchronization-objects-dir3d-12-int">3.2.14.3.5.&nbsp;Importing synchronization objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#signaling-waiting-on-imported-synchronization-objects-dir3d-12-int">3.2.14.3.6.&nbsp;Signaling/waiting on imported synchronization objects</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#direct3d11-interoperability">3.2.14.4.&nbsp;Direct3D 11 Interoperability</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#matching-device-luids-dir3d-11-int">3.2.14.4.1.&nbsp;Matching device LUIDs</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-memory-objects-dir3d-11-int">3.2.14.4.2.&nbsp;Importing memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-buffers-onto-imported-memory-objects-dir3d-11-int">3.2.14.4.3.&nbsp;Mapping buffers onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-11-int">3.2.14.4.4.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-synchronization-objects-dir3d-11-int">3.2.14.4.5.&nbsp;Importing synchronization objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#signaling-waiting-on-imported-synchronization-objects-dir3d-11-int">3.2.14.4.6.&nbsp;Signaling/waiting on imported synchronization objects</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#nvidia-softwarcommunication-interface-interoperability-nvsci">3.2.14.5.&nbsp;NVIDIA Software Communication Interface Interoperability (NVSCI)</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#importing-memory-objects-nvsci">3.2.14.5.1.&nbsp;Importing memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-buffers-onto-imported-memory-objects-nvsci">3.2.14.5.2.&nbsp;Mapping buffers onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-nvsci">3.2.14.5.3.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#importing-synchronization-objects-nvsci">3.2.14.5.4.&nbsp;Importing synchronization objects</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#signaling-waiting-on-imported-synchronization-objects-nvsci">3.2.14.5.5.&nbsp;Signaling/waiting on imported synchronization objects</a></div>
                                       </li>
                                    </ul>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#cuda-user-objects">3.2.15.&nbsp;CUDA User Objects</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#versioning-and-compatibility">3.3.&nbsp;Versioning and Compatibility</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-modes">3.4.&nbsp;Compute Modes</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#mode-switches">3.5.&nbsp;Mode Switches</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#tesla-compute-cluster-mode-for-windows">3.6.&nbsp;Tesla Compute Cluster Mode for Windows</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#hardware-implementation">4.&nbsp;Hardware Implementation</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#simt-architecture">4.1.&nbsp;SIMT Architecture</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#hardware-multithreading">4.2.&nbsp;Hardware Multithreading</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#performance-guidelines">5.&nbsp;Performance Guidelines</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#overall-performance-optimization-strategies">5.1.&nbsp;Overall Performance Optimization Strategies</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#maximize-utilization">5.2.&nbsp;Maximize Utilization</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#application-level">5.2.1.&nbsp;Application Level</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#device-level">5.2.2.&nbsp;Device Level</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#multiprocessor-level">5.2.3.&nbsp;Multiprocessor Level</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#occupancy-calculator">5.2.3.1.&nbsp;Occupancy Calculator</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#maximize-memory-throughput">5.3.&nbsp;Maximize Memory Throughput</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#data-transfer-between-host-and-device">5.3.1.&nbsp;Data Transfer between Host and Device</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#device-memory-accesses">5.3.2.&nbsp;Device Memory Accesses</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#maximize-instruction-throughput">5.4.&nbsp;Maximize Instruction Throughput</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#arithmetic-instructions">5.4.1.&nbsp;Arithmetic Instructions</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#control-flow-instructions">5.4.2.&nbsp;Control Flow Instructions</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#synchronization-instruction">5.4.3.&nbsp;Synchronization Instruction</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#minimize-memory-thrashing">5.5.&nbsp;Minimize Memory Thrashing</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#cuda-enabled-gpus">A.&nbsp;CUDA-Enabled GPUs</a></div>
               </li>
               <li>
                  <div class="section-link"><a href="#c-language-extensions">B.&nbsp;C++ Language Extensions</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#function-declaration-specifiers">B.1.&nbsp;Function Execution Space Specifiers</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#global">B.1.1.&nbsp;__global__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#device-function-specifier">B.1.2.&nbsp;__device__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#host">B.1.3.&nbsp;__host__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#cross-execution-undefined-behavior">B.1.4.&nbsp;Undefined behavior</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#noinline-and-forceinline">B.1.5.&nbsp;__noinline__ and __forceinline__</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#variable-memory-space-specifiers">B.2.&nbsp;Variable Memory Space Specifiers</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#device-variable-specifier">B.2.1.&nbsp;__device__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#constant">B.2.2.&nbsp;__constant__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared">B.2.3.&nbsp;__shared__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#managed">B.2.4.&nbsp;__managed__</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#restrict">B.2.5.&nbsp;__restrict__</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#built-in-vector-types">B.3.&nbsp;Built-in Vector Types</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#vector-types">B.3.1.&nbsp;char, short, int, long, longlong, float, double</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#dim3">B.3.2.&nbsp;dim3</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#built-in-variables">B.4.&nbsp;Built-in Variables</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#griddim">B.4.1.&nbsp;gridDim</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#blockidx">B.4.2.&nbsp;blockIdx</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#blockdim">B.4.3.&nbsp;blockDim</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#threadidx">B.4.4.&nbsp;threadIdx</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warpsize">B.4.5.&nbsp;warpSize</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#memory-fence-functions">B.5.&nbsp;Memory Fence Functions</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#synchronization-functions">B.6.&nbsp;Synchronization Functions</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#mathematical-functions">B.7.&nbsp;Mathematical Functions</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#texture-functions">B.8.&nbsp;Texture Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#texture-object-api-appendix">B.8.1.&nbsp;Texture Object API</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#tex1dfetch-object">B.8.1.1.&nbsp;tex1Dfetch()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1d-object">B.8.1.2.&nbsp;tex1D()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlod-object">B.8.1.3.&nbsp;tex1DLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dgrad-object">B.8.1.4.&nbsp;tex1DGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2d-object">B.8.1.5.&nbsp;tex2D()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlod-object">B.8.1.6.&nbsp;tex2DLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dgrad-object">B.8.1.7.&nbsp;tex2DGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex3d-object">B.8.1.8.&nbsp;tex3D()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex3dlod-object">B.8.1.9.&nbsp;tex3DLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex3dgrad-object">B.8.1.10.&nbsp;tex3DGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlayered-object">B.8.1.11.&nbsp;tex1DLayered()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlayeredlod-object">B.8.1.12.&nbsp;tex1DLayeredLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlayeredgrad-object">B.8.1.13.&nbsp;tex1DLayeredGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlayered-object">B.8.1.14.&nbsp;tex2DLayered()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlayeredlod-object">B.8.1.15.&nbsp;tex2DLayeredLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlayeredgrad-object">B.8.1.16.&nbsp;tex2DLayeredGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemap-object">B.8.1.17.&nbsp;texCubemap()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemaplod-object">B.8.1.18.&nbsp;texCubemapLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemaplayered-object">B.8.1.19.&nbsp;texCubemapLayered()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemaplayeredlod-object">B.8.1.20.&nbsp;texCubemapLayeredLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dgather-object">B.8.1.21.&nbsp;tex2Dgather()</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#texture-reference-api-appendix">B.8.2.&nbsp;Texture Reference API</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#tex1dfetch">B.8.2.1.&nbsp;tex1Dfetch()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1d">B.8.2.2.&nbsp;tex1D()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlod">B.8.2.3.&nbsp;tex1DLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dgrad">B.8.2.4.&nbsp;tex1DGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2d">B.8.2.5.&nbsp;tex2D()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlod">B.8.2.6.&nbsp;tex2DLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dgrad">B.8.2.7.&nbsp;tex2DGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex3d">B.8.2.8.&nbsp;tex3D()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex3dlod">B.8.2.9.&nbsp;tex3DLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex3dgrad">B.8.2.10.&nbsp;tex3DGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlayered">B.8.2.11.&nbsp;tex1DLayered()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlayeredlod">B.8.2.12.&nbsp;tex1DLayeredLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex1dlayeredgrad">B.8.2.13.&nbsp;tex1DLayeredGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlayered">B.8.2.14.&nbsp;tex2DLayered()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlayeredlod">B.8.2.15.&nbsp;tex2DLayeredLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dlayeredgrad">B.8.2.16.&nbsp;tex2DLayeredGrad()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemap">B.8.2.17.&nbsp;texCubemap()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemaplod">B.8.2.18.&nbsp;texCubemapLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemaplayered">B.8.2.19.&nbsp;texCubemapLayered()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#texcubemaplayeredlod">B.8.2.20.&nbsp;texCubemapLayeredLod()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#tex2dgather">B.8.2.21.&nbsp;tex2Dgather()</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#surface-functions">B.9.&nbsp;Surface Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#surface-object-api-appendix">B.9.1.&nbsp;Surface Object API</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#surf1dread-object">B.9.1.1.&nbsp;surf1Dread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf1dwrite-object">B.9.1.2.&nbsp;surf1Dwrite</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dread-object">B.9.1.3.&nbsp;surf2Dread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dwrite-object">B.9.1.4.&nbsp;surf2Dwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf3dread-object">B.9.1.5.&nbsp;surf3Dread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf3dwrite-object">B.9.1.6.&nbsp;surf3Dwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf1dlayeredread-object">B.9.1.7.&nbsp;surf1DLayeredread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf1dlayeredwrite-object">B.9.1.8.&nbsp;surf1DLayeredwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dlayeredread-object">B.9.1.9.&nbsp;surf2DLayeredread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dlayeredwrite-object">B.9.1.10.&nbsp;surf2DLayeredwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemapread-object">B.9.1.11.&nbsp;surfCubemapread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemapwrite-object">B.9.1.12.&nbsp;surfCubemapwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemaplayeredread-object">B.9.1.13.&nbsp;surfCubemapLayeredread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemaplayeredwrite-object">B.9.1.14.&nbsp;surfCubemapLayeredwrite()</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#surface-reference-api-appendix">B.9.2.&nbsp;Surface Reference API</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#surf1dread">B.9.2.1.&nbsp;surf1Dread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf1dwrite">B.9.2.2.&nbsp;surf1Dwrite</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dread">B.9.2.3.&nbsp;surf2Dread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dwrite">B.9.2.4.&nbsp;surf2Dwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf3dread">B.9.2.5.&nbsp;surf3Dread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf3dwrite">B.9.2.6.&nbsp;surf3Dwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf1dlayeredread">B.9.2.7.&nbsp;surf1DLayeredread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf1dlayeredwrite">B.9.2.8.&nbsp;surf1DLayeredwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dlayeredread">B.9.2.9.&nbsp;surf2DLayeredread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surf2dlayeredwrite">B.9.2.10.&nbsp;surf2DLayeredwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemapread">B.9.2.11.&nbsp;surfCubemapread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemapwrite">B.9.2.12.&nbsp;surfCubemapwrite()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemaplayeredread">B.9.2.13.&nbsp;surfCubemapLayeredread()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#surfcubemaplayeredwrite">B.9.2.14.&nbsp;surfCubemapLayeredwrite()</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#ldg-function">B.10.&nbsp;Read-Only Data Cache Load Function</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#ldx-functions">B.11.&nbsp;Load Functions Using Cache Hints</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stx-functions">B.12.&nbsp;Store Functions Using Cache Hints</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#time-function">B.13.&nbsp;Time Function</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#atomic-functions">B.14.&nbsp;Atomic Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#arithmetic-functions">B.14.1.&nbsp;Arithmetic Functions</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#atomicadd">B.14.1.1.&nbsp;atomicAdd()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicsub">B.14.1.2.&nbsp;atomicSub()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicexch">B.14.1.3.&nbsp;atomicExch()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicmin">B.14.1.4.&nbsp;atomicMin()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicmax">B.14.1.5.&nbsp;atomicMax()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicinc">B.14.1.6.&nbsp;atomicInc()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicdec">B.14.1.7.&nbsp;atomicDec()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomiccas">B.14.1.8.&nbsp;atomicCAS()</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#bitwise-functions">B.14.2.&nbsp;Bitwise Functions</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#atomicand">B.14.2.1.&nbsp;atomicAnd()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicor">B.14.2.2.&nbsp;atomicOr()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#atomicxor">B.14.2.3.&nbsp;atomicXor()</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#address-space-predicate-functions">B.15.&nbsp;Address Space Predicate Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#isGlobal">B.15.1.&nbsp;__isGlobal()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#isShared">B.15.2.&nbsp;__isShared()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#isConstant">B.15.3.&nbsp;__isConstant()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#isLocal">B.15.4.&nbsp;__isLocal()</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#address-space-conversion-functions">B.16.&nbsp;Address Space Conversion Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#__cvta_generic_to_global">B.16.1.&nbsp;__cvta_generic_to_global()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_generic_to_shared">B.16.2.&nbsp;__cvta_generic_to_shared()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_generic_to_constant">B.16.3.&nbsp;__cvta_generic_to_constant()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_generic_to_local">B.16.4.&nbsp;__cvta_generic_to_local()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_global_to_generic">B.16.5.&nbsp;__cvta_global_to_generic()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_shared_to_generic">B.16.6.&nbsp;__cvta_shared_to_generic()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_constant_to_generic">B.16.7.&nbsp;__cvta_constant_to_generic()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__cvta_local_to_generic">B.16.8.&nbsp;__cvta_local_to_generic()</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#alloca-function">B.17.&nbsp;Alloca Function</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#alloca-synopsis">B.17.1.&nbsp;Synopsis</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#alloca-description">B.17.2.&nbsp;Description</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#alloca-example">B.17.3.&nbsp;Example</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compiler-optimization-hint-functions">B.18.&nbsp;Compiler Optimization Hint Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#__builtin_assume_aligned">B.18.1.&nbsp;__builtin_assume_aligned()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__builtin_assume">B.18.2.&nbsp;__builtin_assume()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__assume">B.18.3.&nbsp;__assume()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__builtin_expect">B.18.4.&nbsp;__builtin_expect()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#__builtin_unreachable">B.18.5.&nbsp;__builtin_unreachable()</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#compiler-optimization-hint-functions-restrictions">B.18.6.&nbsp;Restrictions</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#warp-vote-functions">B.19.&nbsp;Warp Vote Functions</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#warp-match-functions">B.20.&nbsp;Warp Match Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#synopsis-match">B.20.1.&nbsp;Synopsys</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warp-description-match">B.20.2.&nbsp;Description</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#warp-reduce-functions">B.21.&nbsp;Warp Reduce Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#warp-reduce-synopsis">B.21.1.&nbsp;Synopsys</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warp-reduce-description">B.21.2.&nbsp;Description</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#warp-shuffle-functions">B.22.&nbsp;Warp Shuffle Functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#synopsis">B.22.1.&nbsp;Synopsis</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warp-description">B.22.2.&nbsp;Description</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warp-notes">B.22.3.&nbsp;Notes</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warp-examples">B.22.4.&nbsp;Examples</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#warp-examples-broadcast">B.22.4.1.&nbsp;Broadcast of a single value across a warp</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#warp-examples-inclusive">B.22.4.2.&nbsp;Inclusive plus-scan across sub-partitions of 8 threads</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#warp-examples-reduction">B.22.4.3.&nbsp;Reduction across a warp</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#scheduling">B.23.&nbsp;Nanosleep Function</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#scheduling-synopsis">B.23.1.&nbsp;Synopsis</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#nanosleep-description">B.23.2.&nbsp;Description</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#scheduling-example">B.23.3.&nbsp;Example</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#wmma">B.24.&nbsp;Warp matrix functions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#wmma-description">B.24.1.&nbsp;Description</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#wmma-altfp">B.24.2.&nbsp;Alternate Floating Point</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#wmma-double">B.24.3.&nbsp;Double Precision</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#wmma-subbyte">B.24.4.&nbsp;Sub-byte Operations</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#wmma-restrictions">B.24.5.&nbsp;Restrictions</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#wmma-type-sizes">B.24.6.&nbsp;Element Types &amp; Matrix Sizes</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#wmma-example">B.24.7.&nbsp;Example</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#aw-barrier">B.25.&nbsp;Asynchronous Barrier</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#split_arrive_then_wait">B.25.1.&nbsp;Simple Synchronization Pattern</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#temporal_split_5_stage_sync">B.25.2.&nbsp;Temporal Splitting and Five Stages of Synchronization</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#bootstrap_init_expected_arrive_count">B.25.3.&nbsp;Bootstrap Initialization, Expected Arrival Count, and Participation</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#countdown_complete_reset_phase">B.25.4.&nbsp;A Barrier's Phase: Arrival, Countdown, Completion, and Reset</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#warp_specialization">B.25.5.&nbsp;Spatial Partitioning (also known as Warp Specialization)</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#early_exit">B.25.6.&nbsp;Early Exit (Dropping out of Participation)</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#memory_barrier_primitives_interface">B.25.7.&nbsp;Memory Barrier Primitives Interface</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#memory_barrier_primitives_datatypes">B.25.7.1.&nbsp;Data Types</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#memory_barrier_primitives_api">B.25.7.2.&nbsp;Memory Barrier Primitives API</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#memcpy_async">B.26.&nbsp;Asynchronous Data Copies</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#memcpy-async-api">B.26.1.&nbsp;memcpy_async API</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#copy-and-compute-pattern">B.26.2.&nbsp;Copy and Compute Pattern - Staging Data Through Shared Memory</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#without-memcpy_async">B.26.2.1.&nbsp;Without memcpy_async</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#with-memcpy_async">B.26.2.2.&nbsp;With memcpy_async</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#with-memcpy_async-pipeline-pattern">B.26.2.3.&nbsp;Multi-Stage Asynchronous Copy Pattern</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#performance-guidance-memcpy_async">B.26.3.&nbsp;Performance Guidance for memcpy_async</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#aligned_size_t">B.26.3.1.&nbsp;Alignment</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#trivially-copyable">B.26.3.2.&nbsp;Trivially copyable</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#warp-entanglement-commit">B.26.3.3.&nbsp;Warp Entanglement - Commit</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#warp-entanglement-wait">B.26.3.4.&nbsp;Warp Entanglement - Wait</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#warp-entanglement-arrive-on">B.26.3.5.&nbsp;Warp Entanglement - Arrive-On</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#keep-commit-arrive-on-ops-converged">B.26.3.6.&nbsp;Keep Commit and Arrive-On Operations Converged</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#pipeline-interface">B.26.4.&nbsp;Pipeline Interface</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#pipeline-primitives-interface">B.26.5.&nbsp;Pipeline Primitives Interface</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#memcpy_async-primitive">B.26.5.1.&nbsp;memcpy_async Primitive</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#commit-primitive">B.26.5.2.&nbsp;Commit Primitive</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#wait-primitive">B.26.5.3.&nbsp;Wait Primitive</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#arrive-primitive">B.26.5.4.&nbsp;Arrive On Barrier Primitive</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#profiler-counter-function">B.27.&nbsp;Profiler Counter Function</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#assertion">B.28.&nbsp;Assertion</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#trap-function">B.29.&nbsp;Trap function</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#breakpoint-function">B.30.&nbsp;Breakpoint Function</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#formatted-output">B.31.&nbsp;Formatted Output</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#format-specifiers">B.31.1.&nbsp;Format Specifiers</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#limitations">B.31.2.&nbsp;Limitations</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#associated-host-side-api">B.31.3.&nbsp;Associated Host-Side API</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#examples">B.31.4.&nbsp;Examples</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#dynamic-global-memory-allocation-and-operations">B.32.&nbsp;Dynamic Global Memory Allocation and Operations</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#heap-memory-allocation">B.32.1.&nbsp;Heap Memory Allocation</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#interoperability-host-memory-api">B.32.2.&nbsp;Interoperability with Host Memory API</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#examples-per-thread">B.32.3.&nbsp;Examples</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#per-thread-allocation">B.32.3.1.&nbsp;Per Thread Allocation</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#per-thread-block-allocation">B.32.3.2.&nbsp;Per Thread Block Allocation</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#allocation-persisting-kernel-launches">B.32.3.3.&nbsp;Allocation Persisting Between Kernel Launches</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#execution-configuration">B.33.&nbsp;Execution Configuration</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#launch-bounds">B.34.&nbsp;Launch Bounds</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#pragma-unroll">B.35.&nbsp;#pragma unroll</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#simd-video">B.36.&nbsp;SIMD Video Instructions</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#cooperative-groups">C.&nbsp;Cooperative Groups</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#introduction-cg">C.1.&nbsp;Introduction</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#whats-new-cg">C.2.&nbsp;What's New in CUDA 11.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#concept-cg">C.3.&nbsp;Programming Model Concept</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#composition-cg">C.3.1.&nbsp;Composition Example</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#group-types-cg">C.4.&nbsp;Group Types</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#group-types-implicit-cg">C.4.1.&nbsp;Implicit Groups</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#thread-block-group-cg">C.4.1.1.&nbsp;Thread Block Group</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#grid-group-cg">C.4.1.2.&nbsp;Grid Group</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#multi-grid-group-cg">C.4.1.3.&nbsp;Multi Grid Group</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#group-types-explicit-cg">C.4.2.&nbsp;Explicit Groups</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#thread-block-tile-group-cg">C.4.2.1.&nbsp;Thread Block Tile</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#ws-code-pattern-cg">C.4.2.1.1.&nbsp;Warp-Synchronous Code Pattern</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#thb-tiles-bigger-than-32-cg">C.4.2.1.2.&nbsp;Thread Block Tile of size larger than 32</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#coalesced-group-cg">C.4.2.2.&nbsp;Coalesced Groups</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#discovery-pattern-cg">C.4.2.2.1.&nbsp;Discovery Pattern</a></div>
                                       </li>
                                    </ul>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#partitions-cg">C.5.&nbsp;Group Partitioning</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#partitions-cg-tiled">C.5.1.&nbsp;tiled_partition</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#partitions-cg-labeled">C.5.2.&nbsp;labeled_partition</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#partitions-cg-binary">C.5.3.&nbsp;binary_partition</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#collectives-cg">C.6.&nbsp;Group Collectives</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#sync-collectives-cg">C.6.1.&nbsp;Synchronization</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#collectives-cg-sync">C.6.1.1.&nbsp;sync</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#data-transfer-collectives-cg">C.6.2.&nbsp;Data Transfer</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#collectives-cg-memcpy-async">C.6.2.1.&nbsp;memcpy_async</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#collectives-cg-wait">C.6.2.2.&nbsp;wait</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#reduce-collectives-cg">C.6.3.&nbsp;Data manipulation</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#collectives-cg-reduce">C.6.3.1.&nbsp;reduce</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#collectives-cg-reduce-operators">C.6.3.2.&nbsp;Reduce Operators</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#grid-synchronization-cg">C.7.&nbsp;Grid Synchronization</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#multi-device-synchronization-cg">C.8.&nbsp;Multi-Device Synchronization</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#cuda-dynamic-parallelism">D.&nbsp;CUDA Dynamic Parallelism</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#introduction-cuda-dynamic-parallelism">D.1.&nbsp;Introduction</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#overview">D.1.1.&nbsp;Overview</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#glossary">D.1.2.&nbsp;Glossary</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#execution-environment-and-memory-model">D.2.&nbsp;Execution Environment and Memory Model</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#execution-environment">D.2.1.&nbsp;Execution Environment</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#parent-and-child-grids">D.2.1.1.&nbsp;Parent and Child Grids</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#scope-of-cuda-primitives">D.2.1.2.&nbsp;Scope of CUDA Primitives</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#synchronization">D.2.1.3.&nbsp;Synchronization</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#streams-and-events">D.2.1.4.&nbsp;Streams and Events</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#ordering-and-concurrency">D.2.1.5.&nbsp;Ordering and Concurrency</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#device-management">D.2.1.6.&nbsp;Device Management</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#memory-model">D.2.2.&nbsp;Memory Model</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#coherence-and-consistency">D.2.2.1.&nbsp;Coherence and Consistency</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#global-memory">D.2.2.1.1.&nbsp;Global Memory</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#zero-copy-memory">D.2.2.1.2.&nbsp;Zero Copy Memory</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#constant-memory">D.2.2.1.3.&nbsp;Constant Memory</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#shared-and-local-memory">D.2.2.1.4.&nbsp;Shared and Local Memory</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#local-memory">D.2.2.1.5.&nbsp;Local Memory</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#texture-memory-cdp">D.2.2.1.6.&nbsp;Texture Memory</a></div>
                                       </li>
                                    </ul>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#programming-interface-cdp">D.3.&nbsp;Programming Interface</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#cuda-c-cplusplus">D.3.1.&nbsp;CUDA C++ Reference</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#device-side-kernel-launch">D.3.1.1.&nbsp;Device-Side Kernel Launch</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#launches-are-asynchronous">D.3.1.1.1.&nbsp;Launches are Asynchronous</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#launch-environment-configuration">D.3.1.1.2.&nbsp;Launch Environment Configuration</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#streams-cdp">D.3.1.2.&nbsp;Streams</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#implicit-null-stream">D.3.1.2.1.&nbsp;The Implicit (NULL) Stream</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#events-cdp">D.3.1.3.&nbsp;Events</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#synchronization-programming-interface">D.3.1.4.&nbsp;Synchronization</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#block-wide-synchronization">D.3.1.4.1.&nbsp;Block Wide Synchronization</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#device-management-programming">D.3.1.5.&nbsp;Device Management</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#memory-declarations">D.3.1.6.&nbsp;Memory Declarations</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#device-and-constant-memory">D.3.1.6.1.&nbsp;Device and Constant Memory</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#textures-and-surfaces">D.3.1.6.2.&nbsp;Textures &amp; Surfaces</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#shared-memory-variable-declarations">D.3.1.6.3.&nbsp;Shared Memory Variable Declarations</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#symbol-addresses">D.3.1.6.4.&nbsp;Symbol Addresses</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#api-errors-and-launch-failures">D.3.1.7.&nbsp;API Errors and Launch Failures</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#launch-setup-apis">D.3.1.7.1.&nbsp;Launch Setup APIs</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#api-reference">D.3.1.8.&nbsp;API Reference</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#device-side-launch-from-ptx">D.3.2.&nbsp;Device-side Launch from PTX</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#kernel-launch-apis">D.3.2.1.&nbsp;Kernel Launch APIs</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#cudalaunchdevice">D.3.2.1.1.&nbsp;cudaLaunchDevice</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#cudagetparameterbuffer">D.3.2.1.2.&nbsp;cudaGetParameterBuffer</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#parameter-buffer-layout">D.3.2.2.&nbsp;Parameter Buffer Layout</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#toolkit-support-for-dynamic-parallelism">D.3.3.&nbsp;Toolkit Support for Dynamic Parallelism</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#including-device-runtime-api-in-cuda-code">D.3.3.1.&nbsp;Including Device Runtime API in CUDA Code</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#compiling-and-linking">D.3.3.2.&nbsp;Compiling and Linking</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#programming-guidelines">D.4.&nbsp;Programming Guidelines</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#basics">D.4.1.&nbsp;Basics</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#performance">D.4.2.&nbsp;Performance</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#synchronization-performance">D.4.2.1.&nbsp;Synchronization</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#dynamic-parallelism-enabled-kernel-overhead">D.4.2.2.&nbsp;Dynamic-parallelism-enabled Kernel Overhead</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#implementation-restrictions-and-limitations">D.4.3.&nbsp;Implementation Restrictions and Limitations</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#runtime">D.4.3.1.&nbsp;Runtime</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#memory-footprint">D.4.3.1.1.&nbsp;Memory Footprint</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#nesting-and-synchronization-depth">D.4.3.1.2.&nbsp;Nesting and Synchronization Depth</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#pending-kernel-launches">D.4.3.1.3.&nbsp;Pending Kernel Launches</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#configuration-options">D.4.3.1.4.&nbsp;Configuration Options</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#memory-allocation-and-lifetime">D.4.3.1.5.&nbsp;Memory Allocation and Lifetime</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#sm-id-and-warp-id">D.4.3.1.6.&nbsp;SM Id and Warp Id</a></div>
                                       </li>
                                       <li>
                                          <div class="section-link"><a href="#ecc-errors">D.4.3.1.7.&nbsp;ECC Errors</a></div>
                                       </li>
                                    </ul>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#virtual-memory-management">E.&nbsp;Virtual Memory Management</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#introduction-virtual-memory-management">E.1.&nbsp;Introduction</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#querying-vmm-support">E.2.&nbsp;Query for support</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#allocating-physical-memory">E.3.&nbsp;Allocating Physical Memory</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#shareable-physical-memory">E.3.1.&nbsp;Shareable Memory Allocations</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#physical-memory-type">E.3.2.&nbsp;Memory Type</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#physical-memory-type-compression">E.3.2.1.&nbsp;Compressible Memory</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#address-reservation-and-free">E.4.&nbsp;Reserving a Virtual Address Range</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#virtual-aliasing-support">E.5.&nbsp;Virtual Aliasing Support</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#mapping-memory">E.6.&nbsp;Mapping Memory</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#accessing-memory">E.7.&nbsp;Control Access Rights</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#stream-ordered-memory-allocator">F.&nbsp;Stream Ordered Memory Allocator</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-memory-allocator-intro">F.1.&nbsp;Introduction</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-querying-memory-support">F.2.&nbsp;Query for Support</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-api-fundamentals">F.3.&nbsp;API Fundamentals (cudaMalloc and cudaFreeAsync)</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-memory-pools">F.4.&nbsp;Memory Pools and the cudaMemPool_t</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-default-implicit">F.5.&nbsp;Default/Impicit Pools</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-explicit-pools">F.6.&nbsp;Explicit Pools</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-physical-page-caching-behavior">F.7.&nbsp;Physical Page Caching Behavior</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-resource-usage-statisitics">F.8.&nbsp;Resource Usage Statistics</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-memory-reuse-policies">F.9.&nbsp;Memory Reuse Policies</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-mempoolreusefollow">F.9.1.&nbsp;cudaMemPoolReuseFollowEventDependencies</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-mempoolreuseallowopportunistic">F.9.2.&nbsp;cudaMemPoolReuseAllowOpportunistic</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-mempoolreuseallowinternal">F.9.3.&nbsp;cudaMemPoolReuseAllowInternalDependencies</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-disabling-reuse-policies">F.9.4.&nbsp;Disabling Reuse Policies</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-deviceaccessibility">F.10.&nbsp;Device Accessibility for Multi-GPU Support</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-ipc-memory-pools">F.11.&nbsp;IPC Memory Pools</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-creating-and-sharing-ipc-memory-pools">F.11.1.&nbsp;Creating and Sharing IPC Memory Pools</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-set-access-importing-process">F.11.2.&nbsp;Set Access in the Importing Process</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-allocations-from-exported-pool">F.11.3.&nbsp;Creating and Sharing Allocations from an Exported Pool</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-export-pool-limitations">F.11.4.&nbsp;IPC Export Pool Limitations</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-import-pool-limitations">F.11.5.&nbsp;IPC Import Pool Limitations</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-synchronization-api-actions">F.12.&nbsp;Synchronization API Actions</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#stream-ordered-addendums">F.13.&nbsp;Addendums</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-cudamemcpyasync">F.13.1.&nbsp;cudaMemcpyAsync Current Context/Device Sensitivity</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-cupointergetattribute">F.13.2.&nbsp;cuPointerGetAttribute Query</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-cugraphaddmemsetnode">F.13.3.&nbsp;cuGraphAddMemsetNode</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#stream-ordered-pointer-attributes">F.13.4.&nbsp;Pointer Attributes</a></div>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#mathematical-functions-appendix">G.&nbsp;Mathematical Functions</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#standard-functions">G.1.&nbsp;Standard Functions</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#intrinsic-functions">G.2.&nbsp;Intrinsic Functions</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#c-cplusplus-language-support">H.&nbsp;C++ Language Support</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#cpp11-language-features">H.1.&nbsp;C++11 Language Features</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#cpp14-language-features">H.2.&nbsp;C++14 Language Features</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#cpp17-language-features">H.3.&nbsp;C++17 Language Features</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#restrictions">H.4.&nbsp;Restrictions</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#host-compiler-extensions">H.4.1.&nbsp;Host Compiler Extensions</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#preprocessor-symbols">H.4.2.&nbsp;Preprocessor Symbols</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#cuda-arch-macro">H.4.2.1.&nbsp;__CUDA_ARCH__</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#qualifiers">H.4.3.&nbsp;Qualifiers</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#device-memory-specifiers">H.4.3.1.&nbsp;Device Memory Space Specifiers</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#managed-specifier">H.4.3.2.&nbsp;__managed__ Memory Space Specifier</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#volatile-qualifier">H.4.3.3.&nbsp;Volatile Qualifier</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#pointers">H.4.4.&nbsp;Pointers</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#operators">H.4.5.&nbsp;Operators</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#assignment-operator">H.4.5.1.&nbsp;Assignment Operator</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#address-operator">H.4.5.2.&nbsp;Address Operator</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#rtti">H.4.6.&nbsp;Run Time Type Information (RTTI)</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#exception-handling">H.4.7.&nbsp;Exception Handling</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#standard-library">H.4.8.&nbsp;Standard Library</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#functions">H.4.9.&nbsp;Functions</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#external-linkage">H.4.9.1.&nbsp;External Linkage</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#compiler-generated-functions">H.4.9.2.&nbsp;Implicitly-declared and explicitly-defaulted functions</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#function-parameters">H.4.9.3.&nbsp;Function Parameters</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#static-variables-function">H.4.9.4.&nbsp;Static Variables within Function</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#function-pointers">H.4.9.5.&nbsp;Function Pointers</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#function-recursion">H.4.9.6.&nbsp;Function Recursion</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#friend-function">H.4.9.7.&nbsp;Friend Functions</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#operator-function">H.4.9.8.&nbsp;Operator Function</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#classes">H.4.10.&nbsp;Classes</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#data-members">H.4.10.1.&nbsp;Data Members</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#function-members">H.4.10.2.&nbsp;Function Members</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#virtual-functions">H.4.10.3.&nbsp;Virtual Functions</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#virtual-base-classes">H.4.10.4.&nbsp;Virtual Base Classes</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#anon-union">H.4.10.5.&nbsp;Anonymous Unions</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#windows-specific">H.4.10.6.&nbsp;Windows-Specific</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#templates">H.4.11.&nbsp;Templates</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#trigraph-digraph">H.4.12.&nbsp;Trigraphs and Digraphs </a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#const-variables">H.4.13.&nbsp;Const-qualified variables </a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#long-double">H.4.14.&nbsp;Long Double </a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#deprecation-annotation">H.4.15.&nbsp;Deprecation Annotation </a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#noreturn-annotation">H.4.16.&nbsp;Noreturn Annotation </a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#likely-unlikely-attribute">H.4.17.&nbsp;[[likely]] / [[unlikely]] Standard Attributes </a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#cpp11">H.4.18.&nbsp;C++11 Features</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#lambda-expressions">H.4.18.1.&nbsp;Lambda Expressions</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#initializer-list">H.4.18.2.&nbsp;std::initializer_list</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#rvalue-references">H.4.18.3.&nbsp;Rvalue references</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#constexpr-functions">H.4.18.4.&nbsp;Constexpr functions and function templates</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#constexpr-variables">H.4.18.5.&nbsp;Constexpr variables</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#inline-namespaces">H.4.18.6.&nbsp;Inline namespaces</a></div>
                                    <ul>
                                       <li>
                                          <div class="section-link"><a href="#inline-unnamed-namespaces">H.4.18.6.1.&nbsp;Inline unnamed namespaces</a></div>
                                       </li>
                                    </ul>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#thread-local">H.4.18.7.&nbsp;thread_local</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#cpp11-global">H.4.18.8.&nbsp;__global__ functions and function templates</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#cpp11-device-variable">H.4.18.9.&nbsp;__managed__ and __shared__ variables</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#cpp11-defaulted-function">H.4.18.10.&nbsp;Defaulted functions</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#cpp14">H.4.19.&nbsp;C++14 Features</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#return-type-deduction">H.4.19.1.&nbsp;Functions with deduced return type</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#variable-templates">H.4.19.2.&nbsp;Variable templates</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#cpp17">H.4.20.&nbsp;C++17 Features</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#inline-variable">H.4.20.1.&nbsp;Inline Variable</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#structured-binding">H.4.20.2.&nbsp;Structured Binding</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#polymorphic-function-wrappers">H.5.&nbsp;Polymorphic Function Wrappers</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#extended-lambda">H.6.&nbsp;Extended Lambdas</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#extended-lambda-traits">H.6.1.&nbsp;Extended Lambda Type Traits</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#extended-lambda-restrictions">H.6.2.&nbsp;Extended Lambda Restrictions</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#host-device-lambda-notes">H.6.3.&nbsp;Notes on __host__ __device__  lambdas</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#star-this-capture">H.6.4.&nbsp;*this Capture By Value</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#extended-lambda-notes">H.6.5.&nbsp;Additional Notes</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#code-samples">H.7.&nbsp;Code Samples</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#data-aggregation-class">H.7.1.&nbsp;Data Aggregation Class</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#derived-class">H.7.2.&nbsp;Derived Class</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#class-template">H.7.3.&nbsp;Class Template</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#function-template">H.7.4.&nbsp;Function Template</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#functor-class">H.7.5.&nbsp;Functor Class</a></div>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#texture-fetching">I.&nbsp;Texture Fetching</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#nearest-point-sampling">I.1.&nbsp;Nearest-Point Sampling</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#linear-filtering">I.2.&nbsp;Linear Filtering</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#table-lookup">I.3.&nbsp;Table Lookup</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#compute-capabilities">J.&nbsp;Compute Capabilities</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#features-and-technical-specifications">J.1.&nbsp;Features and Technical Specifications</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#floating-point-standard">J.2.&nbsp;Floating-Point Standard</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-capability-3-0">J.3.&nbsp;Compute Capability 3.x</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#architecture-3-0">J.3.1.&nbsp;Architecture</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#global-memory-3-0">J.3.2.&nbsp;Global Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory-3-0">J.3.3.&nbsp;Shared Memory</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-capability-5-x">J.4.&nbsp;Compute Capability 5.x</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#architecture-5-x">J.4.1.&nbsp;Architecture</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#global-memory-5-x">J.4.2.&nbsp;Global Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory-5-x">J.4.3.&nbsp;Shared Memory</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-capability-6-x">J.5.&nbsp;Compute Capability 6.x</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#architecture-6-x">J.5.1.&nbsp;Architecture</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#global-memory-6-x">J.5.2.&nbsp;Global Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory-6-x">J.5.3.&nbsp;Shared Memory</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-capability-7-x">J.6.&nbsp;Compute Capability 7.x</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#architecture-7-x">J.6.1.&nbsp;Architecture</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#independent-thread-scheduling-7-x">J.6.2.&nbsp;Independent Thread Scheduling</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#global-memory-7-x">J.6.3.&nbsp;Global Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory-7-x">J.6.4.&nbsp;Shared Memory</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#compute-capability-8-x">J.7.&nbsp;Compute Capability 8.x</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#architecture-8-x">J.7.1.&nbsp;Architecture</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#global-memory-8-x">J.7.2.&nbsp;Global Memory</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory-8-x">J.7.3.&nbsp;Shared Memory</a></div>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#driver-api">K.&nbsp;Driver API</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#context">K.1.&nbsp;Context</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#module">K.2.&nbsp;Module</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#kernel-execution">K.3.&nbsp;Kernel Execution</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#interoperability-between-runtime-and-driver-apis">K.4.&nbsp;Interoperability between Runtime and Driver APIs</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#driver-entry-point-access">K.5.&nbsp;Driver Entry Point Access</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#introduction-driver-entry-point-access">K.5.1.&nbsp;Introduction</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#driver-function-typedefs">K.5.2.&nbsp;Driver Function Typedefs</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#driver-function-retrieval">K.5.3.&nbsp;Driver Function Retrieval</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#using-the-driver-API">K.5.3.1.&nbsp;Using the driver API</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#using-the-runtime-API">K.5.3.2.&nbsp;Using the runtime API</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#retrieve-per-thread-default-stream-versions">K.5.3.3.&nbsp;Retrieve per-thread default stream versions</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#access-new-CUDA-features">K.5.3.4.&nbsp;Access new CUDA features</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#env-vars">L.&nbsp;CUDA Environment Variables</a></div>
               </li>
               <li>
                  <div class="section-link"><a href="#um-unified-memory-programming-hd">M.&nbsp;Unified Memory Programming</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#um-introduction">M.1.&nbsp;Unified Memory Introduction</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#um-requirements">M.1.1.&nbsp;System Requirements</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-simplifying">M.1.2.&nbsp;Simplifying GPU Programming</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-data-migration">M.1.3.&nbsp;Data Migration and Coherency</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-oversubscription">M.1.4.&nbsp;GPU Memory Oversubscription</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-multi-gpu">M.1.5.&nbsp;Multi-GPU</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-system-allocator">M.1.6.&nbsp;System Allocator</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-hw-coherency">M.1.7.&nbsp;Hardware Coherency</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-access-counters">M.1.8.&nbsp;Access Counters</a></div>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#um-programming-model-hd">M.2.&nbsp;Programming Model</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#um-opt-in">M.2.1.&nbsp;Managed Memory Opt In</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#um-explicit-allocation">M.2.1.1.&nbsp;Explicit Allocation Using cudaMallocManaged()</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-global-scope">M.2.1.2.&nbsp;Global-Scope Managed Variables Using __managed__</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-coherency-hd">M.2.2.&nbsp;Coherency and Concurrency</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#um-gpu-exclusive">M.2.2.1.&nbsp;GPU Exclusive Access To Managed Memory</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-explicit-synchronization">M.2.2.2.&nbsp;Explicit Synchronization and Logical GPU Activity</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-managing-data">M.2.2.3.&nbsp;Managing Data Visibility and Concurrent CPU + GPU Access with Streams</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-stream-association">M.2.2.4.&nbsp;Stream Association Examples</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-stream-attach">M.2.2.5.&nbsp;Stream Attach With Multithreaded Host Programs</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-advanced-modular">M.2.2.6.&nbsp;Advanced Topic: Modular Programs and Data Access Constraints</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-memcpy-memset">M.2.2.7.&nbsp;Memcpy()/Memset() Behavior With Managed Memory </a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-language-integration">M.2.3.&nbsp;Language Integration</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#um-host-program-errors">M.2.3.1.&nbsp;Host Program Errors with __managed__ Variables</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-querying-um-hd">M.2.4.&nbsp;Querying Unified Memory Support</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#um-device-properties">M.2.4.1.&nbsp;Device Properties</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-pointer-attributes">M.2.4.2.&nbsp;Pointer Attributes</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-advanced-topics-hd">M.2.5.&nbsp;Advanced Topics</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#um-managed-memory">M.2.5.1.&nbsp;Managed Memory with Multi-GPU Programs on pre-6.x Architectures</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#um-fork-managed-memory">M.2.5.2.&nbsp;Using fork() with Managed Memory</a></div>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li>
                        <div class="section-link"><a href="#um-performance-tuning">M.3.&nbsp;Performance Tuning</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#um-tuning-prefetch">M.3.1.&nbsp;Data Prefetching</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-tuning-usage">M.3.2.&nbsp;Data Usage Hints</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#um-querying-usage">M.3.3.&nbsp;Querying Usage Attributes</a></div>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
            </ul>
         </nav>
         <div id="resize-nav"></div>
         <nav id="search-results">
            <h2>Search Results</h2>
            <ol></ol>
         </nav>
         
         <div id="contents-container">
            <div id="breadcrumbs-container">
               <div id="release-info">Programming Guide
                  (<a href="../pdf/CUDA_C_Programming_Guide.pdf">PDF</a>)
                  -
                   
                  
                  
                  v11.3.1
                  (<a href="https://developer.nvidia.com/cuda-toolkit-archive">older</a>)
                  -
                  Last updated May 20, 2021
                  -
                  <a href="mailto:CUDAIssues@nvidia.com?subject=CUDA Toolkit Documentation Feedback: Programming Guide">Send Feedback</a></div>
            </div>
            <article id="contents">
               <div class="topic nested0" id="abstract"><a name="abstract" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#abstract" name="abstract" shape="rect">CUDA C++ Programming Guide</a></h2>
                  <div class="body conbody">
                     <p class="shortdesc">The programming guide to the CUDA model and interface.</p>
                  </div>
               </div>
               <div class="topic concept nested0" id="changes-from-previous-version"><a name="changes-from-previous-version" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#changes-from-previous-version" name="changes-from-previous-version" shape="rect">Changes from Version 11.2</a></h2>
                  <div class="body conbody">
                     <ul class="ul">
                        <li class="li">Added <a class="xref" href="index.html#driver-entry-point-access" shape="rect">Driver Entry Point Access</a>.
                        </li>
                        <li class="li">Added <a class="xref" href="index.html#virtual-aliasing-support" shape="rect">Virtual Aliasing Support</a>.
                        </li>
                        <li class="li">Added <a class="xref" href="index.html#stream-ordered-memory-allocator" shape="rect">Stream Ordered Memory Allocator</a>.
                        </li>
                     </ul>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="introduction"><a name="introduction" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#introduction" name="introduction" shape="rect">1.&nbsp;Introduction</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="from-graphics-processing-to-general-purpose-parallel-computing"><a name="from-graphics-processing-to-general-purpose-parallel-computing" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#from-graphics-processing-to-general-purpose-parallel-computing" name="from-graphics-processing-to-general-purpose-parallel-computing" shape="rect">1.1.&nbsp;The Benefits of Using GPUs</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The Graphics Processing Unit (GPU)<a name="fnsrc_1" href="#fntarg_1" shape="rect"><sup>1</sup></a>
                           provides much higher instruction throughput and memory bandwidth than the CPU within a
                           similar price and power envelope. Many applications leverage these higher capabilities to 
                           run faster on the GPU than on the CPU (see
                           <a class="xref" href="http://www.nvidia.com/object/gpu-applications.html" target="_blank" shape="rect">GPU Applications</a>).
                           Other computing devices, like FPGAs, are also very energy efficient, but offer much less
                           programming flexibility than GPUs.
                           
                        </p>
                        <p class="p">
                           This difference in capabilities between the GPU and the CPU exists because they
                           are designed with different goals in mind. While the CPU is designed to excel at executing
                           a sequence of operations, called a <dfn class="term">thread</dfn>, as fast as possible and can execute
                           a few tens of these threads in parallel, the GPU is designed to excel at executing
                           thousands of them in parallel (amortizing the slower single-thread performance to achieve greater throughput).
                           
                        </p>
                        <p class="p">The GPU is specialized for highly parallel computations and therefore designed such that more 
                           transistors are devoted to data processing rather than data caching and flow control. 
                           The schematic <a class="xref" href="index.html#from-graphics-processing-to-general-purpose-parallel-computing__gpu-devotes-more-transistors-to-data-processing" shape="rect">Figure 1</a> 
                           shows an example distribution of chip resources for a CPU versus a GPU.
                           
                        </p>
                        <div class="fig fignone" id="from-graphics-processing-to-general-purpose-parallel-computing__gpu-devotes-more-transistors-to-data-processing"><a name="from-graphics-processing-to-general-purpose-parallel-computing__gpu-devotes-more-transistors-to-data-processing" shape="rect">
                              <!-- --></a><span class="figcap">Figure 1. The GPU Devotes More Transistors to Data Processing</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" width="600" src="graphics/gpu-devotes-more-transistors-to-data-processing.png" alt="The GPU Devotes More Transistors to Data Processing."></img></div><br clear="none"></br></div>
                        <p class="p">Devoting more transistors to data processing, e.g., floating-point computations, is beneficial for highly 
                           parallel computations; the GPU can hide memory access latencies with computation, instead of relying on 
                           large data caches and complex flow control to avoid long memory access latencies, both of which are 
                           expensive in terms of transistors.
                        </p>
                        <p class="p">In general, an application has a mix of parallel parts and sequential parts, so systems are 
                           designed with a mix of GPUs and CPUs in order to maximize overall performance. Applications 
                           with a high degree of parallelism can exploit this massively parallel nature of the GPU to 
                           achieve higher performance than on the CPU.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="cuda-general-purpose-parallel-computing-architecture"><a name="cuda-general-purpose-parallel-computing-architecture" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#cuda-general-purpose-parallel-computing-architecture" name="cuda-general-purpose-parallel-computing-architecture" shape="rect">1.2.&nbsp;CUDA<sup></sup>: A General-Purpose Parallel Computing
                           Platform and Programming Model</a></h3>
                     <div class="body conbody">
                        <p class="p">In November 2006, NVIDIA<sup></sup> introduced CUDA<sup></sup>, a general
                           purpose parallel computing platform and programming model that leverages
                           the parallel compute engine in NVIDIA GPUs to solve many complex
                           computational problems in a more efficient way than on a CPU.
                        </p>
                        <p class="p">CUDA comes with a software environment that allows developers to use C++
                           as a high-level programming language. As illustrated by <a class="xref" href="index.html#cuda-general-purpose-parallel-computing-architecture__cuda-is-designed-to-support-various-languages-and-application-programming-interfaces" title="CUDA is designed to support various languages and application programming interfaces." shape="rect">Figure 2</a>,
                           other languages, application programming interfaces, or directives-based
                           approaches are supported, such as FORTRAN, DirectCompute, OpenACC.
                        </p>
                        <div class="fig fignone" id="cuda-general-purpose-parallel-computing-architecture__cuda-is-designed-to-support-various-languages-and-application-programming-interfaces"><a name="cuda-general-purpose-parallel-computing-architecture__cuda-is-designed-to-support-various-languages-and-application-programming-interfaces" shape="rect">
                              <!-- --></a><span class="figcap">Figure 2. GPU Computing Applications</span>. <span class="desc figdesc">CUDA is designed to support various languages and application
                              programming interfaces.</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" width="800" src="graphics/gpu-computing-applications.png" alt="CUDA is designed to support       various languages and application programming interfaces."></img></div><br clear="none"></br></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="scalable-programming-model"><a name="scalable-programming-model" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#scalable-programming-model" name="scalable-programming-model" shape="rect">1.3.&nbsp;A Scalable Programming Model</a></h3>
                     <div class="body conbody">
                        <p class="p">The advent of multicore CPUs and manycore GPUs means that mainstream
                           processor chips are now parallel systems. The challenge is to develop
                           application software that transparently scales its parallelism to
                           leverage the increasing number of processor cores, much as 3D graphics
                           applications transparently scale their parallelism to manycore GPUs with
                           widely varying numbers of cores.
                        </p>
                        <p class="p">The CUDA parallel programming model is designed to overcome this
                           challenge while maintaining a low learning curve for programmers familiar
                           with standard programming languages such as C.
                        </p>
                        <p class="p">At its core are three key abstractions - a hierarchy of thread groups,
                           shared memories, and barrier synchronization - that are simply exposed to
                           the programmer as a minimal set of language extensions.
                        </p>
                        <p class="p">These abstractions provide fine-grained data parallelism and thread
                           parallelism, nested within coarse-grained data parallelism and task
                           parallelism. They guide the programmer to partition the problem into
                           coarse sub-problems that can be solved independently in parallel by
                           blocks of threads, and each sub-problem into finer pieces that can be
                           solved cooperatively in parallel by all threads within the block.
                        </p>
                        <p class="p">This decomposition preserves language expressivity by allowing threads
                           to cooperate when solving each sub-problem, and at the same time enables
                           automatic scalability. Indeed, each block of threads can be scheduled on
                           any of the available multiprocessors within a GPU, in any order,
                           concurrently or sequentially, so that a compiled CUDA program can execute
                           on any number of multiprocessors as illustrated by <a class="xref" href="index.html#scalable-programming-model__automatic-scalability" shape="rect">Figure 3</a>, and only
                           the runtime system needs to know the physical multiprocessor count.
                        </p>
                        <p class="p">This scalable programming model allows the GPU architecture to span a
                           wide market range by simply scaling the number of multiprocessors and
                           memory partitions: from the high-performance enthusiast GeForce GPUs and
                           professional Quadro and Tesla computing products to a variety of
                           inexpensive, mainstream GeForce GPUs (see <a class="xref" href="index.html#cuda-enabled-gpus" shape="rect">CUDA-Enabled GPUs</a> for a list of all CUDA-enabled GPUs).
                        </p>
                        <div class="fig fignone" id="scalable-programming-model__automatic-scalability"><a name="scalable-programming-model__automatic-scalability" shape="rect">
                              <!-- --></a><span class="figcap">Figure 3. Automatic Scalability</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/automatic-scalability.png" alt="Automatic Scalability."></img></div><br clear="none"></br><div class="note note"><span class="notetitle">Note:</span> A GPU is built around an array of Streaming
                              Multiprocessors (SMs) (see <a class="xref" href="index.html#hardware-implementation" shape="rect">Hardware Implementation</a> for
                              more details). A multithreaded program is partitioned into blocks of
                              threads that execute independently from each other, so that a GPU with
                              more multiprocessors will automatically execute the program in less
                              time than a GPU with fewer multiprocessors.
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="document-structure"><a name="document-structure" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#document-structure" name="document-structure" shape="rect">1.4.&nbsp;Document Structure</a></h3>
                     <div class="body conbody">
                        <div class="p">This document is organized into the following chapters:
                           
                           <ul class="ul">
                              <li class="li">
                                 Chapter <a class="xref" href="index.html#introduction" shape="rect">Introduction</a> is a general introduction to CUDA.
                                 
                              </li>
                              <li class="li">
                                 Chapter <a class="xref" href="index.html#programming-model" shape="rect">Programming Model</a> outlines the CUDA programming model.
                                 
                              </li>
                              <li class="li">
                                 Chapter <a class="xref" href="index.html#programming-interface" shape="rect">Programming Interface</a> describes the programming interface.
                                 
                              </li>
                              <li class="li">
                                 Chapter <a class="xref" href="index.html#hardware-implementation" shape="rect">Hardware Implementation</a> describes the hardware implementation.
                                 
                              </li>
                              <li class="li">
                                 Chapter <a class="xref" href="index.html#performance-guidelines" shape="rect">Performance Guidelines</a> gives some guidance on how to achieve maximum performance.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#cuda-enabled-gpus" shape="rect">CUDA-Enabled GPUs</a> lists all CUDA-enabled devices.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#c-language-extensions" shape="rect">C++ Language Extensions</a> is a detailed description of all extensions to the C++ language.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#cooperative-groups" shape="rect">Cooperative Groups</a> describes synchronization primitives for various groups of CUDA threads.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#cuda-dynamic-parallelism" shape="rect">CUDA Dynamic Parallelism</a> describes how to launch and synchronize one kernel from another.
                                 
                              </li>
                              <li class="li">Appendix <a class="xref" href="index.html#virtual-memory-management" shape="rect">Virtual Memory Management</a>
                                 describes how to manage the unified virtual address space.
                              </li>
                              <li class="li">Appendix <a class="xref" href="index.html#stream-ordered-memory-allocator" shape="rect">Stream Ordered Memory Allocator</a>
                                 describes how applications can order memory allocation and deallocation.
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#mathematical-functions-appendix" shape="rect">Mathematical Functions</a> lists the mathematical functions supported in CUDA.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#c-cplusplus-language-support" shape="rect">C++ Language Support</a> lists the C++ features supported in device code.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#texture-fetching" shape="rect">Texture Fetching</a> gives more details on texture fetching
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#compute-capabilities" shape="rect">Compute Capabilities</a> gives the technical specifications of various devices, as well as more architectural details.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#driver-api" shape="rect">Driver API</a> introduces the low-level driver API.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#env-vars" shape="rect">CUDA Environment Variables</a> lists all the CUDA environment variables.
                                 
                              </li>
                              <li class="li">
                                 Appendix <a class="xref" href="index.html#um-unified-memory-programming-hd" shape="rect">Unified Memory Programming</a> introduces the Unified Memory programming model.
                                 
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="programming-model"><a name="programming-model" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#programming-model" name="programming-model" shape="rect">2.&nbsp;Programming Model</a></h2>
                  <div class="body conbody">
                     <p class="p">This chapter introduces the main concepts behind the CUDA programming model by outlining how
                        they are exposed in C++. An extensive description of CUDA C++ is given in
                        <a class="xref" href="index.html#programming-interface" shape="rect">Programming Interface</a>.
                        
                     </p>
                     <p class="p">
                        Full code for the vector addition example used in this chapter and the next can be found in the
                        <a class="xref" href="http://docs.nvidia.com/cuda/cuda-samples/index.html#vector-addition" shape="rect"><samp class="ph codeph">vectorAdd</samp> CUDA sample</a>.
                        
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="kernels"><a name="kernels" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#kernels" name="kernels" shape="rect">2.1.&nbsp;Kernels</a></h3>
                     <div class="body conbody">
                        <p class="p">CUDA C++ extends C++ by allowing the programmer to define C++ functions,
                           called <dfn class="term">kernels</dfn>, that, when called, are executed N times in
                           parallel by N different <dfn class="term">CUDA threads</dfn>, as opposed to only
                           once like regular C++ functions.
                        </p>
                        <p class="p">A kernel is defined using the <samp class="ph codeph">__global__</samp> declaration
                           specifier and the number of CUDA threads that execute that kernel for a
                           given kernel call is specified using a new
                           <samp class="ph codeph">&lt;&lt;&lt;...&gt;&gt;&gt;</samp><dfn class="term">execution
                              configuration</dfn> syntax (see <a class="xref" href="index.html#c-language-extensions" shape="rect">C++ Language Extensions</a>). Each thread that executes the kernel
                           is given a unique <dfn class="term">thread ID</dfn> that is accessible within the
                           kernel through built-in variables.
                        </p>
                        <p class="p">As an illustration, the following sample code, using the built-in variable <samp class="ph codeph">threadIdx</samp>,
                           adds two vectors <dfn class="term">A</dfn> and <dfn class="term">B</dfn> of size <dfn class="term">N</dfn> and stores the
                           result into vector <dfn class="term">C</dfn>:
                        </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel definition</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> VecAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* B, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* C)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    C[i] = A[i] + B[i];
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    ...
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel invocation with N threads</span>
    VecAdd<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1, N<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(A, B, C);
    ...
}</pre><p class="p">Here, each of the <dfn class="term">N</dfn> threads that execute
                           <samp class="ph codeph">VecAdd()</samp> performs one pair-wise addition.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="thread-hierarchy"><a name="thread-hierarchy" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#thread-hierarchy" name="thread-hierarchy" shape="rect">2.2.&nbsp;Thread Hierarchy</a></h3>
                     <div class="body conbody">
                        <p class="p">For convenience, <samp class="ph codeph">threadIdx</samp> is a 3-component vector, so
                           that threads can be identified using a one-dimensional, two-dimensional,
                           or three-dimensional <dfn class="term">thread index</dfn>, forming a
                           one-dimensional, two-dimensional, or three-dimensional block of threads, called a <dfn class="term">thread block</dfn>. This
                           provides a natural way to invoke computation across the elements in a
                           domain such as a vector, matrix, or volume.
                        </p>
                        <p class="p">The index of a thread and its thread ID relate to each other in a
                           straightforward way: For a one-dimensional block, they are the same; for
                           a two-dimensional block of size <em class="ph i">(D<sub class="ph sub">x</sub>, D<sub class="ph sub">y</sub>)</em>,the
                           thread ID of a thread of index <em class="ph i">(x, y)</em> is <em class="ph i">(x + y
                              D<sub class="ph sub">x</sub>)</em>; for a three-dimensional block of size
                           <em class="ph i">(D<sub class="ph sub">x</sub>, D<sub class="ph sub">y</sub>, D<sub class="ph sub">z</sub>)</em>, the thread ID of a
                           thread of index <em class="ph i">(x, y, z)</em> is <em class="ph i">(x + y D<sub class="ph sub">x</sub> + z
                              D<sub class="ph sub">x</sub> D<sub class="ph sub">y</sub>)</em>.
                        </p>
                        <p class="p">As an example, the following code adds two matrices <em class="ph i">A</em> and
                           <em class="ph i">B</em> of size <em class="ph i">NxN</em> and stores the result into matrix
                           <em class="ph i">C</em>:
                        </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel definition</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> A[N][N], <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> B[N][N],
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> C[N][N])
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> j = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;
    C[i][j] = A[i][j] + B[i][j];
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    ...
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel invocation with one block of N * N * 1 threads</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> numBlocks = 1;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> threadsPerBlock(N, N);
    MatAdd<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>numBlocks, threadsPerBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(A, B, C);
    ...
}</pre><p class="p">There is a limit to the number of threads per block, since all threads
                           of a block are expected to reside on the same processor core and must
                           share the limited memory resources of that core. On current GPUs, a
                           thread block may contain up to 1024 threads.
                        </p>
                        <p class="p">However, a kernel can be executed by multiple equally-shaped thread
                           blocks, so that the total number of threads is equal to the number of
                           threads per block times the number of blocks.
                        </p>
                        <p class="p">Blocks are organized into a one-dimensional, two-dimensional, or
                           three-dimensional <dfn class="term">grid</dfn> of thread blocks as illustrated by
                           <a class="xref" href="index.html#thread-hierarchy__grid-of-thread-blocks" shape="rect">Figure 4</a>. The number of
                           thread blocks in a grid is usually dictated by the size of the data being
                           processed, which typically exceeds the number of processors in the system.
                        </p>
                        <div class="fig fignone" id="thread-hierarchy__grid-of-thread-blocks"><a name="thread-hierarchy__grid-of-thread-blocks" shape="rect">
                              <!-- --></a><span class="figcap">Figure 4. Grid of Thread Blocks</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/grid-of-thread-blocks.png" alt="Grid of Thread Blocks."></img></div><br clear="none"></br></div>
                        <p class="p">The number of threads per block and the number of blocks per grid
                           specified in the <samp class="ph codeph">&lt;&lt;&lt;...&gt;&gt;&gt;</samp> syntax can be of
                           type <samp class="ph codeph">int</samp> or <samp class="ph codeph">dim3</samp>. Two-dimensional
                           blocks or grids can be specified as in the example above.
                        </p>
                        <p class="p">Each block within the grid can be identified by a one-dimensional,
                           two-dimensional, or three-dimensional unique index accessible within the kernel
                           through the built-in <samp class="ph codeph">blockIdx</samp> variable. The dimension of
                           the thread block is accessible within the kernel through the built-in
                           <samp class="ph codeph">blockDim</samp> variable.
                        </p>
                        <p class="p">Extending the previous <samp class="ph codeph">MatAdd()</samp> example to handle
                           multiple blocks, the code becomes as follows.
                        </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel definition</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> A[N][N], <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> B[N][N],
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> C[N][N])
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> j = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (i &lt; N &amp;&amp; j &lt; N)
        C[i][j] = A[i][j] + B[i][j];
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    ...
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel invocation</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> threadsPerBlock(16, 16);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>numBlocks, threadsPerBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(A, B, C);
    ...
}</pre><p class="p">A thread block size of 16x16 (256 threads), although arbitrary in this
                           case, is a common choice. The grid is created with enough blocks to have
                           one thread per matrix element as before. For simplicity, this example
                           assumes that the number of threads per grid in each dimension is evenly
                           divisible by the number of threads per block in that dimension, although
                           that need not be the case.
                        </p>
                        <p class="p">Thread blocks are required to execute independently: It must be possible
                           to execute them in any order, in parallel or in series. This independence
                           requirement allows thread blocks to be scheduled in any order across any
                           number of cores as illustrated by <a class="xref" href="index.html#scalable-programming-model__automatic-scalability" shape="rect">Figure 3</a>, enabling programmers to
                           write code that scales with the number of cores.
                        </p>
                        <p class="p">Threads within a block can cooperate by sharing data through some
                           <dfn class="term">shared memory</dfn> and by synchronizing their execution to
                           coordinate memory accesses. More precisely, one can specify
                           synchronization points in the kernel by calling the <samp class="ph codeph">
                              __syncthreads()</samp>  intrinsic function; <samp class="ph codeph">
                              __syncthreads()</samp>  acts as a barrier at which all threads in the
                           block must wait before any is allowed to proceed. <a class="xref" href="index.html#shared-memory" shape="rect">Shared Memory</a> gives an example of
                           using shared memory. In addition to <samp class="ph codeph">__syncthreads()</samp>,
                           the <a class="xref" href="index.html#cooperative-groups" shape="rect">Cooperative Groups API</a> provides a rich set of thread-synchronization
                           primitives.
                        </p>
                        <p class="p">For efficient cooperation, the shared memory is expected to be a
                           low-latency memory near each processor core (much like an L1 cache) and
                           <samp class="ph codeph">__syncthreads()</samp> is expected to be lightweight.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="memory-hierarchy"><a name="memory-hierarchy" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#memory-hierarchy" name="memory-hierarchy" shape="rect">2.3.&nbsp;Memory Hierarchy</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           CUDA threads may access data from multiple memory spaces during their execution as illustrated by <a class="xref" href="index.html#memory-hierarchy__memory-hierarchy-figure" shape="rect">Figure 5</a>. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the
                           same lifetime as the block. All threads have access to the same global memory.  
                           
                        </p>
                        <p class="p">
                           There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The
                           global, constant, and texture memory spaces are optimized for different memory usages (see  <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>). Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>).  
                           
                        </p>
                        <p class="p">The global, constant, and texture memory spaces are persistent across kernel launches by the same application.  </p>
                        <div class="fig fignone" id="memory-hierarchy__memory-hierarchy-figure"><a name="memory-hierarchy__memory-hierarchy-figure" shape="rect">
                              <!-- --></a><span class="figcap">Figure 5. Memory Hierarchy</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/memory-hierarchy.png" alt="Memory Hierarchy."></img></div><br clear="none"></br></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="heterogeneous-programming"><a name="heterogeneous-programming" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#heterogeneous-programming" name="heterogeneous-programming" shape="rect">2.4.&nbsp;Heterogeneous Programming</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           As illustrated by <a class="xref" href="index.html#heterogeneous-programming__heterogeneous-programming" shape="rect">Figure 6</a>, the CUDA
                           programming model assumes that the CUDA threads execute on a physically separate
                           <dfn class="term">device</dfn> that operates as a coprocessor to the <dfn class="term">host</dfn> running the C++
                           program. This is the case, for example, when the kernels execute on a GPU and the rest of the
                           C++ program executes on a CPU.  
                           
                        </p>
                        <p class="p">
                           The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM,
                           referred to as <dfn class="term">host memory</dfn> and <dfn class="term">device memory</dfn>, respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls
                           to the CUDA runtime (described in <a class="xref" href="index.html#programming-interface" shape="rect">Programming Interface</a>). This includes device memory allocation and deallocation as well as data transfer between host and device memory.  
                           
                        </p>
                        <p class="p">
                           Unified Memory provides <dfn class="term">managed memory</dfn> to bridge the host and device memory spaces.
                           Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory
                           image with a common address space. This capability enables oversubscription of device memory and
                           can greatly simplify the task of porting applications by eliminating the need to explicitly
                           mirror data on host and device. See
                           <a class="xref" href="index.html#um-unified-memory-programming-hd" shape="rect">Unified Memory Programming</a> for an
                           introduction to Unified Memory.
                           
                        </p>
                        <div class="fig fignone" id="heterogeneous-programming__heterogeneous-programming"><a name="heterogeneous-programming__heterogeneous-programming" shape="rect">
                              <!-- --></a><span class="figcap">Figure 6. Heterogeneous Programming</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/heterogeneous-programming.png" alt="Heterogeneous Programming."></img></div><br clear="none"></br><div class="note note"><span class="notetitle">Note:</span> Serial code executes on the host while parallel code executes on the device.
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-capability"><a name="compute-capability" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-capability" name="compute-capability" shape="rect">2.5.&nbsp;Compute Capability</a></h3>
                     <div class="body conbody">
                        <p class="p">The <dfn class="term">compute capability</dfn> of a device is represented by a
                           version number, also sometimes called its "SM version".  This version
                           number identifies the features supported by the GPU hardware and is
                           used by applications at runtime to determine which hardware features
                           and/or instructions are available on the present GPU.
                        </p>
                        <p class="p">The compute capability comprises a major revision number <dfn class="term">X</dfn> and a minor
                           revision number <dfn class="term">Y</dfn> and is denoted by <dfn class="term">X.Y</dfn>.
                        </p>
                        <p class="p">Devices with the same major revision number are
                           of the same core architecture. The major revision number is 8 for devices based
                           on the <dfn class="term">NVIDIA Ampere GPU</dfn> architecture, 7 for devices
                           based on the <dfn class="term">Volta</dfn> architecture, 6 for devices based on the
                           <dfn class="term">Pascal</dfn> architecture, 5 for
                           devices based on the <dfn class="term">Maxwell</dfn> architecture, 3 for devices
                           based on the <dfn class="term">Kepler</dfn> architecture, 2 for devices based on
                           the <dfn class="term">Fermi</dfn> architecture, and 1 for devices based on the
                           <dfn class="term">Tesla</dfn> architecture.
                        </p>
                        <p class="p">The minor revision number corresponds to an incremental improvement
                           to the core architecture, possibly including new features.
                        </p>
                        <p class="p"><dfn class="term">Turing</dfn> is the architecture for devices of
                           compute capability 7.5, and is an incremental update based on the
                           <dfn class="term">Volta</dfn> architecture.
                        </p>
                        <p class="p"><a class="xref" href="index.html#cuda-enabled-gpus" shape="rect">CUDA-Enabled GPUs</a> lists of all
                           CUDA-enabled devices along with their compute capability. <a class="xref" href="index.html#compute-capabilities" shape="rect">Compute Capabilities</a> gives the
                           technical specifications of each compute capability.
                        </p>
                        <div class="note note"><span class="notetitle">Note:</span> The compute capability version of a particular GPU should not be
                           confused with the CUDA version (e.g., CUDA 7.5, CUDA 8, CUDA 9),
                           which is the version of the CUDA <em class="ph i">software platform</em>.  The CUDA
                           platform is used by application developers to create applications that
                           run on many generations of GPU architectures, including future GPU
                           architectures yet to be invented.  While new versions of the CUDA
                           platform often add native support for a new GPU architecture by
                           supporting the compute capability version of that architecture, new
                           versions of the CUDA platform typically also include software features
                           that are independent of hardware generation.
                        </div>
                        <p class="p">The <dfn class="term">Tesla</dfn> and <dfn class="term">Fermi</dfn> architectures are no longer supported starting with CUDA 7.0 and CUDA 9.0, respectively.
                        </p>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="programming-interface"><a name="programming-interface" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#programming-interface" name="programming-interface" shape="rect">3.&nbsp;Programming Interface</a></h2>
                  <div class="body conbody">
                     <p class="p">CUDA C++ provides a simple path for users familiar with the C++ programming
                        language to easily write programs for execution by the device.
                     </p>
                     <p class="p">It consists of a minimal set of extensions to the C++ language and a
                        runtime library.
                     </p>
                     <p class="p">The core language extensions have been introduced in <a class="xref" href="index.html#programming-model" shape="rect">Programming Model</a>. They allow programmers to define a kernel
                        as a C++ function and use some new syntax to specify the grid and block
                        dimension each time the function is called. A complete description of all
                        extensions can be found in <a class="xref" href="index.html#c-language-extensions" shape="rect">C++ Language Extensions</a>. Any
                        source file that contains some of these extensions must be compiled with
                        <samp class="ph codeph">nvcc</samp> as outlined in <a class="xref" href="index.html#compilation-with-nvcc" shape="rect">Compilation with NVCC</a>.
                     </p>
                     <p class="p">The runtime is introduced in <a class="xref" href="index.html#cuda-c-runtime" shape="rect">CUDA Runtime</a>. It
                        provides C and C++ functions that execute on the host to allocate and deallocate
                        device memory, transfer data between host memory and device memory,
                        manage systems with multiple devices, etc. A complete description of the
                        runtime can be found in the CUDA reference manual.
                     </p>
                     <p class="p">The runtime is built on top of a lower-level C API, the CUDA driver API,
                        which is also accessible by the application. The driver API provides an
                        additional level of control by exposing lower-level concepts such as CUDA
                        contexts - the analogue of host processes for the device - and CUDA
                        modules - the analogue of dynamically loaded libraries for the device.
                        Most applications do not use the driver API as they do not need this
                        additional level of control and when using the runtime, context and
                        module management are implicit, resulting in more concise code. As the runtime
                        is interoperable with the driver API, most applications that need some driver API
                        features can default to use the runtime API and only use the driver API where needed.
                        The driver API is introduced in <a class="xref" href="index.html#driver-api" shape="rect">Driver API</a> and fully
                        described in the reference manual.
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compilation-with-nvcc"><a name="compilation-with-nvcc" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compilation-with-nvcc" name="compilation-with-nvcc" shape="rect">3.1.&nbsp;Compilation with NVCC</a></h3>
                     <div class="body conbody">
                        <p class="p"> Kernels can be written using the CUDA instruction set architecture, called
                           <dfn class="term">PTX</dfn>, which is described in the PTX reference manual. It is however
                           usually more effective to use a high-level programming language such as C++. In both
                           cases, kernels must be compiled into binary code by <samp class="ph codeph">nvcc</samp> to execute
                           on the device. 
                        </p>
                        <p class="p"><samp class="ph codeph">nvcc</samp> is a compiler driver that simplifies the process of compiling
                           <dfn class="term">C++</dfn> or <dfn class="term">PTX</dfn> code: It provides simple and familiar command
                           line options and executes them by invoking the collection of tools that implement
                           the different compilation stages. This section gives an overview of
                           <samp class="ph codeph">nvcc</samp> workflow and command options. A complete description can
                           be found in the <samp class="ph codeph">nvcc</samp> user manual. 
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="compilation-workflow"><a name="compilation-workflow" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#compilation-workflow" name="compilation-workflow" shape="rect">3.1.1.&nbsp;Compilation Workflow</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="offline-compilation"><a name="offline-compilation" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#offline-compilation" name="offline-compilation" shape="rect">3.1.1.1.&nbsp;Offline Compilation</a></h3>
                           <div class="body conbody">
                              <p class="p">Source files compiled with <samp class="ph codeph">nvcc</samp> can include a mix of host code
                                 (i.e., code that executes on the host) and device code (i.e., code that executes on
                                 the device). <samp class="ph codeph">nvcc</samp>'s basic workflow consists in separating device
                                 code from host code and then:
                              </p>
                              <ul class="ul">
                                 <li class="li"> compiling the device code into an assembly form (<dfn class="term">PTX</dfn> code) and/or
                                    binary form (<dfn class="term">cubin</dfn> object), 
                                 </li>
                                 <li class="li"> and modifying the host code by replacing the
                                    <samp class="ph codeph">&lt;&lt;&lt;...&gt;&gt;&gt;</samp> syntax introduced in
                                    <a class="xref" href="index.html#kernels" shape="rect">Kernels</a> (and described in more details
                                    in <a class="xref" href="index.html#execution-configuration" shape="rect">Execution Configuration</a>) by the
                                    necessary CUDA runtime function calls to load and launch each compiled kernel
                                    from the <dfn class="term">PTX</dfn> code and/or <dfn class="term">cubin</dfn> object. 
                                 </li>
                              </ul>
                              <p class="p">The modified host code is output either as C++ code that is left to be compiled using
                                 another tool or as object code directly by letting <samp class="ph codeph">nvcc</samp> invoke the
                                 host compiler during the last compilation stage.
                              </p>
                              <p class="p">Applications can then:</p>
                              <ul class="ul">
                                 <li class="li">Either link to the compiled host code (this is the most common case),</li>
                                 <li class="li"> Or ignore the modified host code (if any) and use the CUDA driver API (see
                                    <a class="xref" href="index.html#driver-api" shape="rect">Driver API</a>) to load and execute the
                                    <dfn class="term">PTX</dfn> code or <dfn class="term">cubin</dfn> object. 
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="just-in-time-compilation"><a name="just-in-time-compilation" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#just-in-time-compilation" name="just-in-time-compilation" shape="rect">3.1.1.2.&nbsp;Just-in-Time Compilation</a></h3>
                           <div class="body conbody">
                              <p class="p">Any <dfn class="term">PTX</dfn> code loaded by an application at runtime is compiled further to
                                 binary code by the device driver. This is called <dfn class="term">just-in-time
                                    compilation</dfn>. Just-in-time compilation increases application load time,
                                 but allows the application to benefit from any new compiler improvements coming with
                                 each new device driver. It is also the only way for applications to run on devices
                                 that did not exist at the time the application was compiled, as detailed in <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a>.
                              </p>
                              <p class="p">When the device driver just-in-time compiles some <dfn class="term">PTX</dfn> code for some
                                 application, it automatically caches a copy of the generated binary code in order to
                                 avoid repeating the compilation in subsequent invocations of the application. The
                                 cache - referred to as <dfn class="term">compute cache</dfn> - is automatically invalidated
                                 when the device driver is upgraded, so that applications can benefit from the
                                 improvements in the new just-in-time compiler built into the device driver.
                              </p>
                              <p class="p">Environment variables are available to control just-in-time compilation as described
                                 in <a class="xref" href="index.html#env-vars" shape="rect">CUDA Environment Variables</a></p>
                              <p class="p">As an alternative to using <samp class="ph codeph">nvcc</samp> to compile CUDA C++ device code, NVRTC can be used to
                                 compile CUDA C++ device code to PTX at runtime. NVRTC is a runtime compilation library for CUDA C++; more
                                 information can be found in the NVRTC User guide.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="binary-compatibility"><a name="binary-compatibility" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#binary-compatibility" name="binary-compatibility" shape="rect">3.1.2.&nbsp;Binary Compatibility</a></h3>
                        <div class="body conbody">
                           <p class="p">Binary code is architecture-specific. A <dfn class="term">cubin</dfn> object is generated using
                              the compiler option <samp class="ph codeph"><span class="keyword option">-code</span></samp> that specifies the
                              targeted architecture: For example, compiling with
                              <samp class="ph codeph"><span class="keyword option">-code=sm_35</span></samp> produces binary code for
                              devices of <a class="xref" href="index.html#compute-capability" shape="rect">compute
                                 capability</a> 3.5. Binary compatibility is guaranteed from one minor
                              revision to the next one, but not from one minor revision to the previous one or
                              across major revisions. In other words, a <dfn class="term">cubin</dfn> object generated for
                              compute capability <em class="ph i">X.y</em> will only execute on devices of compute capability
                              <em class="ph i">X.z</em> where <em class="ph i">zy</em>. 
                           </p>
                           <div class="p">
                              <div class="note note"><span class="notetitle">Note:</span> Binary compatibility is supported only for the desktop. It is not supported
                                 					for Tegra. Also, the binary compatibility between desktop and Tegra is not
                                 					supported.
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="ptx-compatibility"><a name="ptx-compatibility" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#ptx-compatibility" name="ptx-compatibility" shape="rect">3.1.3.&nbsp;PTX Compatibility</a></h3>
                        <div class="body conbody">
                           <p class="p"> Some <dfn class="term">PTX</dfn> instructions are only supported on devices of higher compute
                              capabilities. For example, <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a> are only supported on
                              devices of compute capability 3.0 and above. The
                              <samp class="ph codeph"><span class="keyword option">-arch</span></samp> compiler option specifies the compute
                              capability that is assumed when compiling C++ to <dfn class="term">PTX</dfn> code. So, code that
                              contains warp shuffle, for example, must be compiled with
                              <samp class="ph codeph"><span class="keyword option">-arch=compute_30</span></samp> (or higher). 
                           </p>
                           <p class="p"><dfn class="term">PTX</dfn> code produced for some specific compute capability can always be
                              compiled to binary code of greater or equal compute capability. Note that a binary
                              compiled from an earlier PTX version may not make use of some hardware features. For
                              example, a binary targeting devices of compute capability 7.0 (Volta) compiled from
                              PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core
                              instructions, since these were not available on Pascal. As a result, the final
                              binary may perform worse than would be possible if the binary were generated using
                              the latest version of PTX. 
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="application-compatibility"><a name="application-compatibility" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#application-compatibility" name="application-compatibility" shape="rect">3.1.4.&nbsp;Application Compatibility</a></h3>
                        <div class="body conbody">
                           <p class="p">To execute code on devices of specific compute capability, an application must load
                              binary or <dfn class="term">PTX</dfn> code that is compatible with this compute capability as
                              described in <a class="xref" href="index.html#binary-compatibility" shape="rect">Binary Compatibility</a> and <a class="xref" href="index.html#ptx-compatibility" shape="rect">PTX Compatibility</a>. In particular, to be able to execute code on future
                              architectures with higher compute capability (for which no binary code can be
                              generated yet), an application must load <dfn class="term">PTX</dfn> code that will be
                              just-in-time compiled for these devices (see <a class="xref" href="index.html#just-in-time-compilation" shape="rect">Just-in-Time Compilation</a>).
                           </p>
                           <p class="p">Which <dfn class="term">PTX</dfn> and binary code gets embedded in a CUDA C++ application is
                              controlled by the <samp class="ph codeph">-arch</samp> and <samp class="ph codeph"><span class="keyword option">-code</span></samp>
                              compiler options or the <samp class="ph codeph"><span class="keyword option">-gencode</span></samp> compiler option
                              as detailed in the <samp class="ph codeph">nvcc</samp> user manual. For example,
                           </p><pre xml:space="preserve">nvcc x.cu
        -gencode arch=compute_50,code=sm_50
        -gencode arch=compute_60,code=sm_60
        -gencode arch=compute_70,code=\'compute_70,sm_70\'</pre><p class="p">embeds binary code compatible with compute capability 5.0 and 6.0 (first and second
                              <samp class="ph codeph"><span class="keyword option">-gencode</span></samp> options) and <dfn class="term">PTX</dfn> and
                              binary code compatible with compute capability 7.0 (third
                              <samp class="ph codeph"><span class="keyword option">-gencode</span></samp> option).
                           </p>
                           <p class="p">Host code is generated to automatically select at runtime the most appropriate code
                              to load and execute, which, in the above example, will be:
                           </p>
                           <ul class="ul">
                              <li class="li">5.0 binary code for devices with compute capability 5.0 and 5.2,</li>
                              <li class="li">6.0 binary code for devices with compute capability 6.0 and 6.1,</li>
                              <li class="li">7.0 binary code for devices with compute capability 7.0 and 7.5, </li>
                              <li class="li"><dfn class="term">PTX</dfn> code which is compiled to binary code at runtime for devices
                                 with compute capability 8.0 and 8.6. 
                              </li>
                           </ul>
                           <p class="p"><samp class="ph codeph">x.cu</samp> can have an optimized code path that uses warp shuffle
                              operations, for example, which are only supported in devices of compute capability
                              3.0 and higher. The <samp class="ph codeph">__CUDA_ARCH__</samp> macro can be used to
                              differentiate various code paths based on compute capability. It is only defined for
                              device code. When compiling with <samp class="ph codeph"><span class="keyword option">-arch=compute_35</span></samp>
                              for example, <samp class="ph codeph">__CUDA_ARCH__</samp> is equal to <samp class="ph codeph">350</samp>.
                           </p>
                           <p class="p">Applications using the driver API must compile code to separate files and explicitly
                              load and execute the most appropriate file at runtime.
                           </p>
                           <p class="p">The Volta architecture introduces <dfn class="term">Independent Thread Scheduling</dfn> which
                              changes the way threads are scheduled on the GPU. For code relying on specific
                              behavior of <a class="xref" href="index.html#simt-architecture" shape="rect">SIMT
                                 scheduling</a> in previous architecures, Independent Thread Scheduling may
                              alter the set of participating threads, leading to incorrect results. To aid
                              migration while implementing the corrective actions detailed in <a class="xref" href="index.html#independent-thread-scheduling-7-x" shape="rect">Independent Thread Scheduling</a>, Volta
                              developers can opt-in to Pascal's thread scheduling with the compiler option
                              combination <samp class="ph codeph"><span class="keyword option">-arch=compute_60 -code=sm_70</span></samp>.
                           </p>
                           <p class="p">The <samp class="ph codeph">nvcc</samp> user manual lists various shorthands for the
                              <samp class="ph codeph"><span class="keyword option">-arch</span></samp>,
                              <samp class="ph codeph"><span class="keyword option">-code</span></samp>, and
                              <samp class="ph codeph"><span class="keyword option">-gencode</span></samp> compiler options. For example,
                              <samp class="ph codeph"><span class="keyword option">-arch=sm_70</span></samp> is a shorthand for
                              <samp class="ph codeph"><span class="keyword option">-arch=compute_70 -code=compute_70,sm_70</span></samp> (which is the same as
                              <samp class="ph codeph"><span class="keyword option">-gencode arch=compute_70, code=\'compute_70,sm_70\'</span></samp>).
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="c-cplusplus-compatibility"><a name="c-cplusplus-compatibility" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#c-cplusplus-compatibility" name="c-cplusplus-compatibility" shape="rect">3.1.5.&nbsp;C++ Compatibility</a></h3>
                        <div class="body conbody">
                           <p class="p">The front end of the compiler processes CUDA source files according to C++ syntax
                              rules. Full C++ is supported for the host code. However, only a subset of C++ is
                              fully supported for the device code as described in <a class="xref" href="index.html#c-cplusplus-language-support" shape="rect">C++ Language Support</a>. 
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="sixtyfour-bit-compatibility"><a name="sixtyfour-bit-compatibility" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#sixtyfour-bit-compatibility" name="sixtyfour-bit-compatibility" shape="rect">3.1.6.&nbsp;64-Bit Compatibility</a></h3>
                        <div class="body conbody">
                           <p class="p">The 64-bit version of <samp class="ph codeph">nvcc</samp> compiles device code in 64-bit mode
                              (i.e., pointers are 64-bit). Device code compiled in 64-bit mode is only supported
                              with host code compiled in 64-bit mode. 
                           </p>
                           <p class="p">Similarly, the 32-bit version of <samp class="ph codeph">nvcc</samp> compiles device code in 32-bit
                              mode and device code compiled in 32-bit mode is only supported with host code
                              compiled in 32-bit mode. 
                           </p>
                           <p class="p"> The 32-bit version of <samp class="ph codeph">nvcc</samp> can compile device code in 64-bit mode
                              also using the <samp class="ph codeph"><span class="keyword option">-m64</span></samp> compiler option. 
                           </p>
                           <p class="p"> The 64-bit version of <samp class="ph codeph">nvcc</samp> can compile device code in 32-bit mode
                              also using the <samp class="ph codeph"><span class="keyword option">-m32</span></samp> compiler option. 
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="cuda-c-runtime"><a name="cuda-c-runtime" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#cuda-c-runtime" name="cuda-c-runtime" shape="rect">3.2.&nbsp;CUDA Runtime</a></h3>
                     <div class="body conbody">
                        <p class="p">The runtime is implemented in the <samp class="ph codeph">cudart</samp> library, which is linked to
                           the application, either statically via <samp class="ph codeph">cudart.lib</samp> or
                           <samp class="ph codeph">libcudart.a</samp>, or dynamically via <samp class="ph codeph">cudart.dll</samp> or
                           <samp class="ph codeph">libcudart.so</samp>. Applications that require
                           <samp class="ph codeph">cudart.dll</samp> and/or <samp class="ph codeph">cudart.so</samp> for dynamic
                           linking typically include them as part of the application installation package. It
                           is only safe to pass the address of CUDA runtime symbols between components that
                           link to the same instance of the CUDA runtime.
                        </p>
                        <p class="p">All its entry points are prefixed with <samp class="ph codeph">cuda</samp>.
                        </p>
                        <p class="p">As mentioned in <a class="xref" href="index.html#heterogeneous-programming" shape="rect">Heterogeneous Programming</a>, the
                           CUDA programming model assumes a system composed of a host and a device, each with
                           their own separate memory. <a class="xref" href="index.html#device-memory" shape="rect">Device Memory</a> gives an overview of the
                           runtime functions used to manage device memory.
                        </p>
                        <p class="p"><a class="xref" href="index.html#shared-memory" shape="rect">Shared Memory</a> illustrates the use of shared memory, introduced in
                           <a class="xref" href="index.html#thread-hierarchy" shape="rect">Thread Hierarchy</a>, to maximize
                           performance.
                        </p>
                        <p class="p"><a class="xref" href="index.html#page-locked-host-memory" shape="rect">Page-Locked Host Memory</a> introduces page-locked host memory that is
                           required to overlap kernel execution with data transfers between host and device
                           memory.
                        </p>
                        <p class="p"><a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a> describes the concepts and API used
                           to enable asynchronous concurrent execution at various levels in the system.
                        </p>
                        <p class="p"><a class="xref" href="index.html#multi-device-system" shape="rect">Multi-Device System</a> shows how the programming model extends to a
                           system with multiple devices attached to the same host.
                        </p>
                        <p class="p"><a class="xref" href="index.html#error-checking" shape="rect">Error Checking</a> describes how to properly check the errors generated
                           by the runtime.
                        </p>
                        <p class="p"><a class="xref" href="index.html#call-stack" shape="rect">Call Stack</a> mentions the runtime functions used to manage the CUDA C++
                           call stack.
                        </p>
                        <p class="p"><a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a> presents the texture and surface memory
                           spaces that provide another way to access device memory; they also expose a subset
                           of the GPU texturing hardware.
                        </p>
                        <p class="p"><a class="xref" href="index.html#graphics-interoperability" shape="rect">Graphics Interoperability</a> introduces the various functions the
                           runtime provides to interoperate with the two main graphics APIs, OpenGL and
                           Direct3D.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="initialization"><a name="initialization" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#initialization" name="initialization" shape="rect">3.2.1.&nbsp;Initialization</a></h3>
                        <div class="body conbody">
                           <p class="p">There is no explicit initialization function for the runtime; it
                              initializes the first time a runtime function is called (more
                              specifically any function other than functions from the error handling
                              and version management sections of the reference manual). One needs to keep
                              this in mind when timing runtime function calls and when interpreting the
                              error code from the first call into the runtime.
                           </p>
                           <p class="p">The runtime creates a CUDA context for each device in the system
                              (see <a class="xref" href="index.html#context" shape="rect">Context</a> for more details on CUDA contexts).
                              This context is the <dfn class="term">primary context</dfn> for this
                              device and is initialized at the first runtime function which requires an
                              active context on this device. It is shared among all the host threads of the application.
                              As part of this context creation, the device code is just-in-time compiled if necessary (see <a class="xref" href="index.html#just-in-time-compilation" shape="rect">Just-in-Time Compilation</a>) and loaded into device memory.
                              This all happens transparently. If needed, e.g. for driver API interoperability, the 
                              primary context of a device can be accessed from the driver API as described in <a class="xref" href="index.html#interoperability-between-runtime-and-driver-apis" shape="rect">Interoperability between Runtime and Driver APIs</a>.
                           </p>
                           <p class="p">When a host thread calls <samp class="ph codeph">cudaDeviceReset()</samp>, this
                              destroys the primary context of the device the host thread currently
                              operates on (i.e., the current device as defined in <a class="xref" href="index.html#device-selection" shape="rect">Device Selection</a>). The next runtime function call made by
                              any host thread that has this device as current will create a new primary
                              context for this device.
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span> 
                              The CUDA interfaces use global state that is initialized during host program
                              initiation and destroyed during host program termination. The CUDA runtime and
                              driver cannot detect if this state is invalid, so using any of these interfaces
                              (implicitly or explicity) during program initiation or termination after main)
                              will result in undefined behavior.
                              
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="device-memory"><a name="device-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#device-memory" name="device-memory" shape="rect">3.2.2.&nbsp;Device Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">As mentioned in <a class="xref" href="index.html#heterogeneous-programming" shape="rect">Heterogeneous Programming</a>, the CUDA
                              programming model assumes a system composed of a host and a device, each
                              with their own separate memory.  Kernels operate out of device memory, so
                              the runtime provides functions to allocate, deallocate, and copy device
                              memory, as well as transfer data between host memory and device memory.
                           </p>
                           <p class="p">Device memory can be allocated either as <dfn class="term">linear memory</dfn> or
                              as <dfn class="term">CUDA arrays</dfn>.
                           </p>
                           <p class="p">CUDA arrays are opaque memory layouts optimized for texture fetching.
                              They are described in <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>.
                           </p>
                           <p class="p">Linear memory is allocated in a single unified address space, which means that separately allocated
                              entities can reference one another via pointers, for example, in a binary tree or linked list. The
                              size of the address space depends on the host system (CPU) and the compute capability of the used GPU:
                           </p>
                           <div class="tablenoborder"><a name="device-memory__linear-memory-address-space" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="device-memory__linear-memory-address-space" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 1. Linear Memory Address Space</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" colspan="1" align="left" valign="top" id="d225e1475" rowspan="1">&nbsp;</th>
                                       <th class="entry" align="center" valign="top" width="19.047619047619047%" id="d225e1477" rowspan="1" colspan="1">x86_64 (AMD64)</th>
                                       <th class="entry" align="center" valign="top" width="19.047619047619047%" id="d225e1480" rowspan="1" colspan="1">POWER (ppc64le)</th>
                                       <th class="entry" align="center" valign="top" width="19.047619047619047%" id="d225e1483" rowspan="1" colspan="1">ARM64</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" align="left" valign="top" width="42.857142857142854%" headers="d225e1475" rowspan="1" colspan="1">up to compute capability 5.3 (Maxwell)</td>
                                       <td class="entry" align="center" valign="top" width="19.047619047619047%" headers="d225e1477" rowspan="1" colspan="1">40bit</td>
                                       <td class="entry" align="center" valign="top" width="19.047619047619047%" headers="d225e1480" rowspan="1" colspan="1">40bit</td>
                                       <td class="entry" align="center" valign="top" width="19.047619047619047%" headers="d225e1483" rowspan="1" colspan="1">40bit</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" align="left" valign="top" width="42.857142857142854%" headers="d225e1475" rowspan="1" colspan="1">compute capability 6.0 (Pascal) or newer</td>
                                       <td class="entry" align="center" valign="top" width="19.047619047619047%" headers="d225e1477" rowspan="1" colspan="1">up to 47bit</td>
                                       <td class="entry" align="center" valign="top" width="19.047619047619047%" headers="d225e1480" rowspan="1" colspan="1">up to 49bit</td>
                                       <td class="entry" align="center" valign="top" width="19.047619047619047%" headers="d225e1483" rowspan="1" colspan="1">up to 48bit</td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                           <div class="note note"><span class="notetitle">Note:</span> On devices of compute capability 5.3 (Maxwell) and earlier, the CUDA driver creates an uncommitted 40bit
                              virtual address reservation to ensure that memory allocations (pointers) fall into the supported range.
                              This reservation appears as reserved virtual memory, but does not occupy any physical memory until the
                              program actually allocates memory.
                           </div>
                           <p class="p">Linear memory is typically allocated using <samp class="ph codeph">cudaMalloc()</samp>
                              and freed using <samp class="ph codeph">cudaFree()</samp> and data transfer between
                              host memory and device memory are typically done using
                              <samp class="ph codeph">cudaMemcpy()</samp>. In the vector addition code sample of
                              <a class="xref" href="index.html#kernels" shape="rect">Kernels</a>, the vectors need to be copied from
                              host memory to device memory:
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> VecAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* B, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* C, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (i &lt; N)
        C[i] = A[i] + B[i];
}
            
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N = ...;
    size_t size = N * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate input vectors h_A and h_B in host memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* h_A = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)malloc(size);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* h_B = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)malloc(size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize input vectors</span>
    ...

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate vectors in device memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* d_A;
    cudaMalloc(&amp;d_A, size);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* d_B;
    cudaMalloc(&amp;d_B, size);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* d_C;
    cudaMalloc(&amp;d_C, size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy vectors from host memory to device memory</span>
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> threadsPerBlock = 256;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>blocksPerGrid, threadsPerBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(d_A, d_B, d_C, N);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy result from device memory to host memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// h_C contains the result in host memory</span>
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
            
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free host memory</span>
    ...
}</pre><p class="p">Linear memory can also be allocated through <samp class="ph codeph">cudaMallocPitch()</samp> and
                              <samp class="ph codeph">cudaMalloc3D()</samp>. These functions are recommended for allocations
                              of 2D or 3D arrays as it makes sure that the allocation is appropriately padded to
                              meet the alignment requirements described in <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>, therefore ensuring
                              best performance when accessing the row addresses or performing copies between 2D
                              arrays and other regions of device memory (using the <samp class="ph codeph">cudaMemcpy2D()</samp>
                              and <samp class="ph codeph">cudaMemcpy3D()</samp> functions). The returned pitch (or stride) must
                              be used to access array elements. The following code sample allocates a
                              <samp class="ph codeph">width</samp> x <samp class="ph codeph">height</samp> 2D array of floating-point
                              values and shows how to loop over the array elements in device code:
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width = 64, height = 64;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* devPtr;
size_t pitch;
cudaMallocPitch(&amp;devPtr, &amp;pitch,
                width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>), height);
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 512<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(devPtr, pitch, width, height);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* devPtr,
                         size_t pitch, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> r = 0; r &lt; height; ++r) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* row = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>*)devPtr + r * pitch);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> c = 0; c &lt; width; ++c) {
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> element = row[c];
        }
    }
}</pre><p class="p">The following code sample allocates a <samp class="ph codeph">width</samp> x
                              <samp class="ph codeph">height</samp> x <samp class="ph codeph">depth</samp> 3D array of floating-point
                              values and shows how to loop over the array elements in device code:
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width = 64, height = 64, depth = 64;
cudaExtent extent = make_cudaExtent(width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>),
                                    height, depth);
cudaPitchedPtr devPitchedPtr;
cudaMalloc3D(&amp;devPitchedPtr, extent);
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 512<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(devPitchedPtr, width, height, depth);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel(cudaPitchedPtr devPitchedPtr,
                         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> depth)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* devPtr = devPitchedPtr.ptr;
    size_t pitch = devPitchedPtr.pitch;
    size_t slicePitch = pitch * height;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z = 0; z &lt; depth; ++z) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* slice = devPtr + z * slicePitch;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = 0; y &lt; height; ++y) {
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* row = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)(slice + y * pitch);
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = 0; x &lt; width; ++x) {
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> element = row[x];
            }
        }
    }
}</pre><div class="p">
                              <div class="note note"><span class="notetitle">Note:</span> To avoid allocating too much memory and thus impacting system-wide performance,
                                 request the allocation parameters from the user based on the problem size. If
                                 the allocation fails, you can fallback to other slower memory types
                                 (<samp class="ph codeph">cudaMallocHost()</samp>, <samp class="ph codeph">cudaHostRegister()</samp>, etc.), or return an error telling the
                                 user how much memory was needed that was denied. If your application cannot
                                 request the allocation parameters for some reason, we recommend using
                                 <samp class="ph codeph">cudaMallocManaged()</samp> for platforms that support it.
                              </div>
                              The reference manual
                              lists all the various functions used to copy memory between linear memory allocated
                              with <samp class="ph codeph">cudaMalloc()</samp>, linear memory allocated with
                              <samp class="ph codeph">cudaMallocPitch()</samp> or <samp class="ph codeph">cudaMalloc3D()</samp>, CUDA
                              arrays, and memory allocated for variables declared in global or constant memory
                              space.
                           </div>
                           <p class="p">The following code sample illustrates various ways of accessing global variables via
                              the runtime API:
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__constant__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> constData[256];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> data[256];
cudaMemcpyToSymbol(constData, data, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(data));
cudaMemcpyFromSymbol(data, constData, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(data));

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> devData;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> value = 3.14f;
cudaMemcpyToSymbol(devData, &amp;value, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>));

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* devPointer;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* ptr;
cudaMalloc(&amp;ptr, 256 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>));
cudaMemcpyToSymbol(devPointer, &amp;ptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(ptr));</pre><p class="p"><samp class="ph codeph">cudaGetSymbolAddress()</samp> is used to retrieve the address pointing to
                              the memory allocated for a variable declared in global memory space. The size of the
                              allocated memory is obtained through <samp class="ph codeph">cudaGetSymbolSize()</samp>.
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="L2_access_intro"><a name="L2_access_intro" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#L2_access_intro" name="L2_access_intro" shape="rect">3.2.3.&nbsp;Device Memory L2 Access Management</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be <dfn class="term">persisting</dfn>. 
                                 On the other hand, if the data is only accessed once, such data accesses can be considered to be <dfn class="term">streaming</dfn>.
                                 
                              </p>
                              <p class="p">Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence
                                 persistence of data in the L2 cache, potentially providing higher bandwidth and lower latency accesses to global memory.
                              </p>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_set_aside"><a name="L2_set_aside" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_set_aside" name="L2_set_aside" shape="rect">3.2.3.1.&nbsp;L2 cache Set-Aside for Persisting Accesses</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">A portion of the L2 cache can be set aside to be used for persisting data accesses to global memory.
                                    Persisting accesses have prioritized use of this set-aside portion of L2 cache, whereas normal or streaming, 
                                    accesses to global memory can only utilize this portion of L2 when it is unused by persisting accesses.
                                 </p>
                                 <p class="p">The L2 cache set-aside size for persisting accesses may be adjusted, within limits:</p><pre xml:space="preserve">
cudaGetDeviceProperties(&amp;prop, device_id);                
size_t size = min(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* set-aside 3/4 of L2 cache for persisting accesses or the max allowed*/</span> </pre><p class="p">When the GPU is configured in Multi-Instance GPU (MIG) mode, the L2 cache set-aside functionality is disabled.</p>
                                 <p class="p">When using the Multi-Process Service (MPS), the L2 cache set-aside size cannot be changed by <samp class="ph codeph">cudaDeviceSetLimit</samp>. 
                                    Instead, the set-aside size can only be specified at start up of MPS server through the environment variable 
                                    <samp class="ph codeph">CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_access_policy"><a name="L2_access_policy" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_access_policy" name="L2_access_policy" shape="rect">3.2.3.2.&nbsp;L2 Policy for Persisting Accesses</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">An access policy window specifies a contiguous region of global memory and 
                                    a persistence property in the L2 cache for accesses within that region.
                                 </p>
                                 <p class="p">The code example below shows how to set an L2 persisting access window using a CUDA Stream.
                                    
                                 </p>
                                 <p class="p"><strong class="ph b">CUDA Stream Example</strong></p><pre xml:space="preserve">
cudaStreamAttrValue stream_attribute;                                         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Stream level attributes data structure</span>
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*&gt;(ptr); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Global Memory data pointer</span>
stream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Number of bytes for persistence access.</span>
                                                                              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)</span>
stream_attribute.accessPolicyWindow.hitRatio  = 0.6;                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Hint for cache hit ratio</span>
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Type of access property on cache hit</span>
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Type of access property on cache miss.</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//Set the attributes to a CUDA stream of type cudaStream_t</span>
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &amp;stream_attribute);    </pre><p class="p">When a kernel subsequently executes in CUDA <samp class="ph codeph">stream</samp>, memory accesses within the global memory extent 
                                    <samp class="ph codeph">[ptr..ptr+num_bytes)</samp> are more likely to persist in the L2 cache than accesses to other global memory locations.
                                 </p>
                                 <p class="p">L2 persistence can also be set for a CUDA Graph Kernel Node as shown in the example below:</p>
                                 <p class="p"><strong class="ph b">CUDA GraphKernelNode Example</strong></p><pre xml:space="preserve">
cudaKernelNodeAttrValue node_attribute;                                     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel level attributes data structure</span>
node_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*&gt;(ptr); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Global Memory data pointer</span>
node_attribute.accessPolicyWindow.num_bytes = num_bytes;                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Number of bytes for persistence access.</span>
                                                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)</span>
node_attribute.accessPolicyWindow.hitRatio  = 0.6;                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Hint for cache hit ratio</span>
node_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Type of access property on cache hit</span>
node_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Type of access property on cache miss.</span>
                                    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t</span>
cudaGraphKernelNodeSetAttribute(node, cudaKernelNodeAttributeAccessPolicyWindow, &amp;node_attribute); </pre><p class="p">The <samp class="ph codeph">hitRatio</samp> parameter can be used to specify the fraction of accesses that receive the <samp class="ph codeph">hitProp</samp> property. 
                                    In both of the examples above, 60% of the memory accesses in the global memory region <samp class="ph codeph">[ptr..ptr+num_bytes)</samp> have the persisting property and 
                                    40% of the memory accesses have the streaming property. Which specific memory accesses are classified as persisting (the <samp class="ph codeph">hitProp</samp>) 
                                    is random with a probability of approximately <samp class="ph codeph">hitRatio</samp>; the probability distribution depends upon the hardware 
                                    architecture and the memory extent.
                                 </p>
                                 <p class="p">For example, if the L2 set-aside cache size is 16KB and the <samp class="ph codeph">num_bytes</samp> in the <samp class="ph codeph">accessPolicyWindow</samp> is 32KB:
                                 </p>
                                 <ul class="ul">
                                    <li class="li">With a <samp class="ph codeph">hitRatio</samp> of 0.5, the hardware will select, at random, 16KB of the 32KB window to be 
                                       designated as persisting and cached in the set-aside L2 cache area.
                                    </li>
                                    <li class="li">With a <samp class="ph codeph">hitRatio</samp> of 1.0, the hardware will attempt to cache the whole 32KB window in the 
                                       set-aside L2 cache area. Since the set-aside area is smaller than the window, cache lines will be evicted 
                                       to keep the most recently used 16KB of the 32KB data in the set-aside portion of the L2 cache.
                                    </li>
                                 </ul>
                                 <p class="p">The <samp class="ph codeph">hitRatio</samp> can therefore be used to avoid thrashing of cache lines and overall reduce the amount of data moved into and out of the L2
                                    cache.
                                 </p>
                                 <p class="p">A <samp class="ph codeph">hitRatio</samp> value below 1.0 can be used to manually control the amount of data different <samp class="ph codeph">accessPolicyWindow</samp>s 
                                    from concurrent CUDA streams can cache in L2. For example, let the L2 set-aside cache size be 16KB; two concurrent kernels
                                    in two different CUDA streams, 
                                    each with a 16KB <samp class="ph codeph">accessPolicyWindow</samp>, and both with <samp class="ph codeph">hitRatio</samp> value 1.0, might evict each others' cache lines when 
                                    competing for the shared L2 resource. However, if both <samp class="ph codeph">accessPolicyWindows</samp> have a hitRatio value of 0.5, they will be less likely to 
                                    evict their own or each others' persisting cache lines.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_access_prop"><a name="L2_access_prop" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_access_prop" name="L2_access_prop" shape="rect">3.2.3.3.&nbsp;L2 Access Properties</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">Three types of access properties are defined for different global memory data accesses:</p>
                                 <ol class="ol">
                                    <li class="li"><samp class="ph codeph">cudaAccessPropertyStreaming</samp>: Memory accesses that occur with the streaming property are less likely 
                                       to persist in the L2 cache because these accesses are preferentially evicted.
                                    </li>
                                    <li class="li"><samp class="ph codeph">cudaAccessPropertyPersisting</samp>: Memory accesses that occur with the persisting property are more likely 
                                       to persist in the L2 cache because these accesses are preferentially retained in the set-aside portion of L2 cache.
                                    </li>
                                    <li class="li"><samp class="ph codeph">cudaAccessPropertyNormal</samp>: This access property forcibly resets previously applied persisting access property to a normal status.
                                       Memory accesses with the persisting property from previous CUDA kernels may be retained in L2 cache long after their intended
                                       use.
                                       This persistence-after-use reduces the amount of L2 cache available to subsequent kernels that do not use the persisting property.
                                       Resetting an access property window with the <samp class="ph codeph">cudaAccessPropertyNormal</samp> property removes the persisting 
                                       (preferential retention) status of the prior access, as if the prior access had been without an access property.
                                    </li>
                                 </ol>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_simple_example"><a name="L2_simple_example" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_simple_example" name="L2_simple_example" shape="rect">3.2.3.4.&nbsp;L2 Persistence Example</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">The following example shows how to set-aside L2 cache for persistent accesses, use the set-aside L2 cache in CUDA kernels
                                    via CUDA Stream
                                    and then reset the L2 cache.
                                 </p><pre xml:space="preserve">

cudaStream_t stream;
cudaStreamCreate(&amp;stream);                                                                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create CUDA stream</span>

cudaDeviceProp prop;                                                                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// CUDA device properties variable</span>
cudaGetDeviceProperties( &amp;prop, device_id);                                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Query GPU properties</span>
size_t size = min( <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>(prop.l2CacheSize * 0.75) , prop.persistingL2CacheMaxSize );
cudaDeviceSetLimit( cudaLimitPersistingL2CacheSize, size);                                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// set-aside 3/4 of L2 cache for persisting accesses or the max allowed</span>

size_t window_size = min(prop.accessPolicyMaxWindowSize, num_bytes);                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Select minimum of user defined num_bytes and max window size.</span>

cudaStreamAttrValue stream_attribute;                                                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Stream level attributes data structure</span>
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*&gt;(data1);               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Global Memory data pointer</span>
stream_attribute.accessPolicyWindow.num_bytes = window_size;                                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Number of bytes for persistence access</span>
stream_attribute.accessPolicyWindow.hitRatio  = 0.6;                                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Hint for cache hit ratio</span>
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting;               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Persistence Property</span>
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Type of access property on cache miss</span>

cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &amp;stream_attribute);   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set the attributes to a CUDA Stream</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 10; i++) {
    cuda_kernelA<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid_size,block_size,0,stream<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data1);                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This data1 is used by a kernel multiple times</span>
}                                                                                           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// [data1 + num_bytes) benefits from L2 persistence</span>
cuda_kernelB<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid_size,block_size,0,stream<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data1);                                     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// A different kernel in the same stream can also benefit</span>
                                                                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// from the persistence of data1</span>

stream_attribute.accessPolicyWindow.num_bytes = 0;                                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Setting the window size to 0 disable it</span>
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &amp;stream_attribute);   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Overwrite the access policy attribute to a CUDA Stream</span>
cudaCtxResetPersistingL2Cache();                                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Remove any persistent lines in L2 </span>

cuda_kernelC<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid_size,block_size,0,stream<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data2);                                     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// data2 can now benefit from full L2 in normal mode</span>
            </pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_reset_to_normal"><a name="L2_reset_to_normal" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_reset_to_normal" name="L2_reset_to_normal" shape="rect">3.2.3.5.&nbsp;Reset L2 Access to Normal</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">A persisting L2 cache line from a previous CUDA kernel may persist in L2 long after it has been used. Hence, a reset to normal
                                    for L2 cache is important
                                    for streaming or normal memory accesses to utilize the L2 cache with normal priority. There are three ways a persisting access
                                    can be reset to normal status.
                                 </p>
                                 <ol class="ol">
                                    <li class="li">Reset a previous persisting memory region with the access property, <samp class="ph codeph">cudaAccessPropertyNormal</samp>.
                                    </li>
                                    <li class="li">Reset all persisting L2 cache lines to normal by calling <samp class="ph codeph">cudaCtxResetPersistingL2Cache()</samp>.
                                    </li>
                                    <li class="li"><strong class="ph b">Eventually</strong> untouched lines are automatically reset to normal. Reliance on automatic reset is strongly discouraged because of the 
                                       undetermined length of time required for automatic reset to occur.
                                    </li>
                                 </ol>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_managing_utilization"><a name="L2_managing_utilization" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_managing_utilization" name="L2_managing_utilization" shape="rect">3.2.3.6.&nbsp;Manage Utilization of L2 set-aside cache</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned
                                    to 
                                    their streams. However, the L2 set-aside cache portion is shared among all these concurrent CUDA kernels. As a result, the
                                    net utilization of 
                                    this set-aside cache portion is the sum of all the concurrent kernels' individual use. The benefits of designating memory
                                    accesses 
                                    as persisting diminish as the volume of persisting accesses exceeds the set-aside L2 cache capacity.
                                 </p>
                                 <p class="p">To manage utilization of the set-aside L2 cache portion, an application must consider the following:</p>
                                 <ul class="ul">
                                    <li class="li">Size of L2 set-aside cache.</li>
                                    <li class="li">CUDA kernels that may concurrently execute.</li>
                                    <li class="li">The access policy window for all the CUDA kernels that may concurrently execute.</li>
                                    <li class="li">When and how L2 reset is required to allow normal or streaming 
                                       accesses to utilize the previously set-aside L2 cache with equal priority.
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_cache_query"><a name="L2_cache_query" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_cache_query" name="L2_cache_query" shape="rect">3.2.3.7.&nbsp;Query L2 cache Properties</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">Properties related to L2 cache are a part of <samp class="ph codeph">cudaDeviceProp</samp> struct and can be queried using CUDA runtime API 
                                    <samp class="ph codeph">cudaGetDeviceProperties</samp></p>
                                 <p class="p">CUDA Device Properties include:</p>
                                 <ul class="ul">
                                    <li class="li"><samp class="ph codeph">l2CacheSize</samp>: The amount of available L2 cache on the GPU.
                                    </li>
                                    <li class="li"><samp class="ph codeph">persistingL2CacheMaxSize</samp>: The maximum amount of L2 cache that can be set-aside for persisting memory accesses.
                                    </li>
                                    <li class="li"><samp class="ph codeph">accessPolicyMaxWindowSize</samp>: The maximum size of the access policy window.
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="L2_cache_getset_size"><a name="L2_cache_getset_size" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#L2_cache_getset_size" name="L2_cache_getset_size" shape="rect">3.2.3.8.&nbsp;Control L2 Cache Set-Aside Size for Persisting Memory Access</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">The L2 set-aside cache size for persisting memory accesses is queried using CUDA runtime API <samp class="ph codeph">cudaDeviceGetLimit</samp> and set using
                                    CUDA runtime API <samp class="ph codeph">cudaDeviceSetLimit</samp> as a <samp class="ph codeph">cudaLimit</samp>.
                                    The maximum value for setting this limit is <samp class="ph codeph">cudaDeviceProp::persistingL2CacheMaxSize</samp>.
                                 </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaLimit {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* other fields not shown */</span>
    cudaLimitPersistingL2CacheSize
};           </pre></div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory"><a name="shared-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory" name="shared-memory" shape="rect">3.2.4.&nbsp;Shared Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">As detailed in <a class="xref" href="index.html#variable-memory-space-specifiers" shape="rect">Variable Memory Space Specifiers</a> shared memory
                              is allocated using the <samp class="ph codeph">__shared__</samp> memory space specifier.
                           </p>
                           <p class="p">Shared memory is expected to be much faster than global memory as
                              mentioned in <a class="xref" href="index.html#thread-hierarchy" shape="rect">Thread Hierarchy</a> and detailed in <a class="xref" href="index.html#shared-memory" shape="rect">Shared Memory</a>. It can be used as scratchpad memory (or software managed cache)
                              to minimize global memory accesses from a CUDA block as illustrated by the following matrix multiplication 
                              example.
                           </p>
                           <p class="p">The following code sample is a straightforward implementation of matrix
                              multiplication that does not take advantage of shared memory. Each thread reads one
                              row of <em class="ph i">A</em> and one column of <em class="ph i">B</em> and computes the corresponding element of
                              <em class="ph i">C</em> as illustrated in <a class="xref" href="index.html#shared-memory__matrix-multiplication-no-shared-memory" shape="rect">Figure 7</a>. <em class="ph i">A</em> is
                              therefore read <em class="ph i">B.width</em> times from global memory and <em class="ph i">B</em> is read
                              <em class="ph i">A.height</em> times.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrices are stored in row-major order:</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// M(row, col) = *(M.elements + row * M.width + col)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* elements;
} Matrix;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread block size</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define BLOCK_SIZE 16</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Forward declaration of the matrix multiplication kernel</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatMulKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix, Matrix);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrix multiplication - Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrix dimensions are assumed to be multiples of BLOCK_SIZE</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatMul(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix B, Matrix C)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Load A and B to device memory</span>
    Matrix d_A;
    d_A.width = A.width; d_A.height = A.height;
    size_t size = A.width * A.height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    cudaMalloc(&amp;d_A.elements, size);
    cudaMemcpy(d_A.elements, A.elements, size,
               cudaMemcpyHostToDevice);
    Matrix d_B;
    d_B.width = B.width; d_B.height = B.height;
    size = B.width * B.height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    cudaMalloc(&amp;d_B.elements, size);
    cudaMemcpy(d_B.elements, B.elements, size,
               cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate C in device memory</span>
    Matrix d_C;
    d_C.width = C.width; d_C.height = C.height;
    size = C.width * C.height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    cudaMalloc(&amp;d_C.elements, size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
    MatMulKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(d_A, d_B, d_C);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Read C from device memory</span>
    cudaMemcpy(C.elements, d_C.elements, size,
               cudaMemcpyDeviceToHost);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFree(d_A.elements);
    cudaFree(d_B.elements);
    cudaFree(d_C.elements);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrix multiplication kernel called by MatMul()</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatMulKernel(Matrix A, Matrix B, Matrix C)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread computes one element of C</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// by accumulating results into Cvalue</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> Cvalue = 0;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> row = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> col = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> e = 0; e &lt; A.width; ++e)
        Cvalue += A.elements[row * A.width + e]
                * B.elements[e * B.width + col];
    C.elements[row * C.width + col] = Cvalue;
}</pre><div class="fig fignone" id="shared-memory__matrix-multiplication-no-shared-memory"><a name="shared-memory__matrix-multiplication-no-shared-memory" shape="rect">
                                 <!-- --></a><span class="figcap">Figure 7. Matrix Multiplication without Shared Memory</span><br clear="none"></br><img class="image" src="graphics/matrix-multiplication-without-shared-memory.png" alt="Matrix Multiplication without Shared Memory."></img><br clear="none"></br></div>
                           <p class="p">The following code sample is an implementation of matrix multiplication that does
                              take advantage of shared memory. In this implementation, each thread block is
                              responsible for computing one square sub-matrix <em class="ph i">C<sub class="ph sub">sub</sub></em> of <em class="ph i">C</em>
                              and each thread within the block is responsible for computing one element of
                              <em class="ph i">C<sub class="ph sub">sub</sub></em>. As illustrated in <a class="xref" href="index.html#shared-memory__matrix-multiplication-shared-memory" shape="rect">Figure 8</a>,
                              <em class="ph i">C<sub class="ph sub">sub</sub></em> is equal to the product of two rectangular matrices: the
                              sub-matrix of <em class="ph i">A</em> of dimension (<em class="ph i">A.width, block_size</em>) that has the same
                              row indices as <em class="ph i">C<sub class="ph sub">sub</sub></em>, and the sub-matrix of <em class="ph i">B</em> of dimension
                              (<em class="ph i">block_size, A.width </em>)that has the same column indices as
                              <em class="ph i">C<sub class="ph sub">sub</sub></em>. In order to fit into the device's resources, these
                              two rectangular matrices are divided into as many square matrices of dimension
                              <em class="ph i">block_size</em> as necessary and <em class="ph i">C<sub class="ph sub">sub</sub></em> is computed as the sum
                              of the products of these square matrices. Each of these products is performed by
                              first loading the two corresponding square matrices from global memory to shared
                              memory with one thread loading one element of each matrix, and then by having each
                              thread compute one element of the product. Each thread accumulates the result of
                              each of these products into a register and once done writes the result to global
                              memory.
                           </p>
                           <p class="p">By blocking the computation this way, we take advantage of fast shared memory and
                              save a lot of global memory bandwidth since <em class="ph i">A</em> is only read (<em class="ph i">B.width /
                                 block_size</em>) times from global memory and <em class="ph i">B</em> is read (<em class="ph i">A.height /
                                 block_size</em>) times.
                           </p>
                           <p class="p">The <dfn class="term">Matrix</dfn> type from the previous code sample is augmented with a
                              <dfn class="term">stride</dfn> field, so that sub-matrices can be efficiently represented
                              with the same type. <a class="xref" href="index.html#device-function-specifier" shape="rect"><samp class="ph codeph">__device__</samp></a> functions are used to get and set
                              elements and build any sub-matrix from a matrix.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrices are stored in row-major order:</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// M(row, col) = *(M.elements + row * M.stride + col)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> stride; 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* elements;
} Matrix;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get a matrix element</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> GetElement(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> row, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> col)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> A.elements[row * A.stride + col];
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set a matrix element</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> SetElement(Matrix A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> row, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> col,
                           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> value)
{
    A.elements[row * A.stride + col] = value;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// located col sub-matrices to the right and row sub-matrices down</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// from the upper-left corner of A</span>
 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Matrix GetSubMatrix(Matrix A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> row, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> col) 
{
    Matrix Asub;
    Asub.width    = BLOCK_SIZE;
    Asub.height   = BLOCK_SIZE;
    Asub.stride   = A.stride;
    Asub.elements = &amp;A.elements[A.stride * BLOCK_SIZE * row
                                         + BLOCK_SIZE * col];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> Asub;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread block size</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define BLOCK_SIZE 16</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Forward declaration of the matrix multiplication kernel</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatMulKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix, Matrix);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrix multiplication - Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrix dimensions are assumed to be multiples of BLOCK_SIZE</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatMul(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> Matrix B, Matrix C)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Load A and B to device memory</span>
    Matrix d_A;
    d_A.width = d_A.stride = A.width; d_A.height = A.height;
    size_t size = A.width * A.height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    cudaMalloc(&amp;d_A.elements, size);
    cudaMemcpy(d_A.elements, A.elements, size,
               cudaMemcpyHostToDevice);
    Matrix d_B;
    d_B.width = d_B.stride = B.width; d_B.height = B.height;
    size = B.width * B.height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);</pre><p class="p"></p><pre xml:space="preserve">    cudaMalloc(&amp;d_B.elements, size);
    cudaMemcpy(d_B.elements, B.elements, size,
    cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate C in device memory</span>
    Matrix d_C;
    d_C.width = d_C.stride = C.width; d_C.height = C.height;
    size = C.width * C.height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    cudaMalloc(&amp;d_C.elements, size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
    MatMulKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(d_A, d_B, d_C);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Read C from device memory</span>
    cudaMemcpy(C.elements, d_C.elements, size,
               cudaMemcpyDeviceToHost);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFree(d_A.elements);
    cudaFree(d_B.elements);
    cudaFree(d_C.elements);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Matrix multiplication kernel called by MatMul()</span>
 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MatMulKernel(Matrix A, Matrix B, Matrix C)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Block row and column</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> blockRow = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> blockCol = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread block computes one sub-matrix Csub of C</span>
    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread computes one element of Csub</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// by accumulating results into Cvalue</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> Cvalue = 0;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread row and column within Csub</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> row = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> col = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Loop over all the sub-matrices of A and B that are</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// required to compute Csub</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Multiply each pair of sub-matrices together</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// and accumulate the results</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> m = 0; m &lt; (A.width / BLOCK_SIZE); ++m) {

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get sub-matrix Asub of A</span>
        Matrix Asub = GetSubMatrix(A, blockRow, m);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get sub-matrix Bsub of B</span>
        Matrix Bsub = GetSubMatrix(B, m, blockCol);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Shared memory used to store Asub and Bsub respectively</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> As[BLOCK_SIZE][BLOCK_SIZE];
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> Bs[BLOCK_SIZE][BLOCK_SIZE];

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Load Asub and Bsub from device memory to shared memory</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread loads one element of each sub-matrix</span>
        As[row][col] = GetElement(Asub, row, col);
        Bs[row][col] = GetElement(Bsub, row, col);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Synchronize to make sure the sub-matrices are loaded</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// before starting the computation</span>
        __syncthreads();</pre><p class="p"></p><pre xml:space="preserve">        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Multiply Asub and Bsub together</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> e = 0; e &lt; BLOCK_SIZE; ++e)
            Cvalue += As[row][e] * Bs[e][col];

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Synchronize to make sure that the preceding</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// computation is done before loading two new</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// sub-matrices of A and B in the next iteration</span>
        __syncthreads();
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write Csub to device memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread writes one element</span>
    SetElement(Csub, row, col, Cvalue);
}</pre><div class="fig fignone" id="shared-memory__matrix-multiplication-shared-memory"><a name="shared-memory__matrix-multiplication-shared-memory" shape="rect">
                                 <!-- --></a><span class="figcap">Figure 8. Matrix Multiplication with Shared Memory</span><br clear="none"></br><img class="image" src="graphics/matrix-multiplication-with-shared-memory.png" alt="Matrix Multiplication with Shared Memory."></img><br clear="none"></br></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="page-locked-host-memory"><a name="page-locked-host-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#page-locked-host-memory" name="page-locked-host-memory" shape="rect">3.2.5.&nbsp;Page-Locked Host Memory</a></h3>
                        <div class="body conbody">
                           <p class="p"> The runtime provides functions to allow the use of <dfn class="term">page-locked</dfn> (also
                              known as <dfn class="term">pinned</dfn>) host memory (as opposed to regular pageable host
                              memory allocated by <samp class="ph codeph">malloc()</samp>): 
                           </p>
                           <ul class="ul">
                              <li class="li"><samp class="ph codeph">cudaHostAlloc()</samp> and <samp class="ph codeph">cudaFreeHost()</samp> allocate
                                 and free page-locked host memory; 
                              </li>
                              <li class="li"><samp class="ph codeph">cudaHostRegister()</samp> page-locks a range of memory allocated by
                                 <samp class="ph codeph">malloc()</samp> (see reference manual for limitations). 
                              </li>
                           </ul>
                           <p class="p">Using page-locked host memory has several benefits:</p>
                           <ul class="ul">
                              <li class="li">Copies between page-locked host memory and device memory can be performed
                                 concurrently with kernel execution for some devices as mentioned in <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>. 
                              </li>
                              <li class="li">On some devices, page-locked host memory can be mapped into the address space of
                                 the device, eliminating the need to copy it to or from device memory as detailed
                                 in <a class="xref" href="index.html#mapped-memory" shape="rect">Mapped Memory</a>. 
                              </li>
                              <li class="li">On systems with a front-side bus, bandwidth between host memory and device
                                 memory is higher if host memory is allocated as page-locked and even higher if
                                 in addition it is allocated as write-combining as described in <a class="xref" href="index.html#write-combining-memory" shape="rect">Write-Combining Memory</a>. 
                              </li>
                           </ul>
                           <p class="p">Page-locked host memory is a scarce resource however, so allocations in page-locked
                              memory will start failing long before allocations in pageable memory. In addition,
                              by reducing the amount of physical memory available to the operating system for
                              paging, consuming too much page-locked memory reduces overall system
                              performance.
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span>  Page-locked host memory is not cached on non I/O coherent Tegra devices. Also,
                              cudaHostRegister() is not supported on non I/O coherent Tegra devices. 
                           </div>
                           <p class="p">The simple zero-copy CUDA sample comes with a detailed document on the page-locked
                              memory APIs.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="portable-memory"><a name="portable-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#portable-memory" name="portable-memory" shape="rect">3.2.5.1.&nbsp;Portable Memory</a></h3>
                           <div class="body conbody">
                              <p class="p"> A block of page-locked memory can be used in conjunction with any device in the
                                 system (see <a class="xref" href="index.html#multi-device-system" shape="rect">Multi-Device System</a> for more details on multi-device
                                 systems), but by default, the benefits of using page-locked memory described above
                                 are only available in conjunction with the device that was current when the block
                                 was allocated (and with all devices sharing the same unified address space, if any,
                                 as described in <a class="xref" href="index.html#unified-virtual-address-space" shape="rect">Unified Virtual Address Space</a>). To make these
                                 advantages available to all devices, the block needs to be allocated by passing the
                                 flag <samp class="ph codeph">cudaHostAllocPortable</samp> to <samp class="ph codeph">cudaHostAlloc()</samp> or
                                 page-locked by passing the flag <samp class="ph codeph">cudaHostRegisterPortable</samp> to
                                 <samp class="ph codeph">cudaHostRegister()</samp>. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="write-combining-memory"><a name="write-combining-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#write-combining-memory" name="write-combining-memory" shape="rect">3.2.5.2.&nbsp;Write-Combining Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">By default page-locked host memory is allocated as cacheable. It can optionally be
                                 allocated as <dfn class="term">write-combining</dfn> instead by passing flag
                                 <samp class="ph codeph">cudaHostAllocWriteCombined</samp> to <samp class="ph codeph">cudaHostAlloc()</samp>.
                                 Write-combining memory frees up the host's L1 and L2 cache resources, making more
                                 cache available to the rest of the application. In addition, write-combining memory
                                 is not snooped during transfers across the PCI Express bus, which can improve
                                 transfer performance by up to 40%.
                              </p>
                              <p class="p">Reading from write-combining memory from the host is prohibitively slow, so
                                 write-combining memory should in general be used for memory that the host only
                                 writes to.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="mapped-memory"><a name="mapped-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#mapped-memory" name="mapped-memory" shape="rect">3.2.5.3.&nbsp;Mapped Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">A block of page-locked host memory can also be mapped into the address space of the
                                 device by passing flag <samp class="ph codeph">cudaHostAllocMapped</samp> to
                                 <samp class="ph codeph">cudaHostAlloc()</samp> or by passing flag
                                 <samp class="ph codeph">cudaHostRegisterMapped</samp> to <samp class="ph codeph">cudaHostRegister()</samp>.
                                 Such a block has therefore in general two addresses: one in host memory that is
                                 returned by <samp class="ph codeph">cudaHostAlloc()</samp> or <samp class="ph codeph">malloc()</samp>, and one
                                 in device memory that can be retrieved using
                                 <samp class="ph codeph">cudaHostGetDevicePointer()</samp> and then used to access the block
                                 from within a kernel. The only exception is for pointers allocated with
                                 <samp class="ph codeph">cudaHostAlloc()</samp> and when a unified address space is used for
                                 the host and the device as mentioned in <a class="xref" href="index.html#unified-virtual-address-space" shape="rect">Unified Virtual Address Space</a>. 
                              </p>
                              <p class="p">Accessing host memory directly from within a kernel does not provide the same bandwidth as device
                                 memory, but does have some advantages:
                              </p>
                              <ul class="ul">
                                 <li class="li">There is no need to allocate a block in device memory and copy data between this
                                    block and the block in host memory; data transfers are implicitly performed as
                                    needed by the kernel;
                                 </li>
                                 <li class="li"> There is no need to use streams (see <a class="xref" href="index.html#concurrent-data-transfers" shape="rect">Concurrent Data Transfers</a>)
                                    to overlap data transfers with kernel execution; the kernel-originated data
                                    transfers automatically overlap with kernel execution. 
                                 </li>
                              </ul>
                              <p class="p">Since mapped page-locked memory is shared between host and device however, the
                                 application must synchronize memory accesses using streams or events (see <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>) to avoid any potential
                                 read-after-write, write-after-read, or write-after-write hazards. 
                              </p>
                              <p class="p">To be able to retrieve the device pointer to any mapped page-locked memory,
                                 page-locked memory mapping must be enabled by calling
                                 <samp class="ph codeph">cudaSetDeviceFlags()</samp> with the
                                 <samp class="ph codeph">cudaDeviceMapHost</samp> flag before any other CUDA call is performed.
                                 Otherwise, <samp class="ph codeph">cudaHostGetDevicePointer()</samp> will return an error. 
                              </p>
                              <p class="p"><samp class="ph codeph">cudaHostGetDevicePointer()</samp> also returns an error if the device does
                                 not support mapped page-locked host memory. Applications may query this capability
                                 by checking the <samp class="ph codeph">canMapHostMemory</samp> device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>), which is equal to 1 for devices that support
                                 mapped page-locked host memory. 
                              </p>
                              <p class="p"> Note that atomic functions (see <a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>) operating on mapped
                                 page-locked memory are not atomic from the point of view of the host or other
                                 devices. 
                              </p>
                              <p class="p"> Also note that CUDA runtime requires that 1-byte, 2-byte, 4-byte, and 8-byte
                                 naturally aligned loads and stores to host memory initiated from the device are
                                 preserved as single accesses from the point of view of the host and other devices.
                                 On some platforms, atomics to memory may be broken by the hardware into separate
                                 load and store operations. These component load and store operations have the same
                                 requirements on preservation of naturally aligned accesses. As an example, the CUDA
                                 runtime does not support a PCI Express bus topology where a PCI Express bridge
                                 splits 8-byte naturally aligned writes into two 4-byte writes between the device and
                                 the host. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="asynchronous-concurrent-execution"><a name="asynchronous-concurrent-execution" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#asynchronous-concurrent-execution" name="asynchronous-concurrent-execution" shape="rect">3.2.6.&nbsp;Asynchronous Concurrent Execution</a></h3>
                        <div class="body conbody">
                           <p class="p">CUDA exposes the following operations as independent tasks that can operate
                              concurrently with one another:
                           </p>
                           <ul class="ul">
                              <li class="li">Computation on the host;</li>
                              <li class="li">Computation on the device;</li>
                              <li class="li">Memory transfers from the host to the device;</li>
                              <li class="li">Memory transfers from the device to the host;</li>
                              <li class="li">Memory transfers within the memory of a given device;</li>
                              <li class="li">Memory transfers among devices.</li>
                           </ul>
                           <p class="p">The level of concurrency achieved between these operations will depend on the feature
                              set and compute capability of the device as described below.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="concurrent-execution-host-device"><a name="concurrent-execution-host-device" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#concurrent-execution-host-device" name="concurrent-execution-host-device" shape="rect">3.2.6.1.&nbsp;Concurrent Execution between Host and Device</a></h3>
                           <div class="body conbody">
                              <p class="p">Concurrent host execution is facilitated through asynchronous library functions that
                                 return control to the host thread before the device completes the requested task.
                                 Using asynchronous calls, many device operations can be queued up together to be
                                 executed by the CUDA driver when appropriate device resources are available. This
                                 relieves the host thread of much of the responsibility to manage the device, leaving
                                 it free for other tasks. The following device operations are asynchronous with
                                 respect to the host:
                              </p>
                              <ul class="ul">
                                 <li class="li">Kernel launches;</li>
                                 <li class="li">Memory copies within a single device's memory;</li>
                                 <li class="li">Memory copies from host to device of a memory block of 64 KB or less;</li>
                                 <li class="li">Memory copies performed by functions that are suffixed with
                                    <samp class="ph codeph">Async</samp>;
                                 </li>
                                 <li class="li">Memory set function calls.</li>
                              </ul>
                              <p class="p">Programmers can globally disable asynchronicity of kernel launches for all CUDA
                                 applications running on a system by setting the
                                 <samp class="ph codeph">CUDA_LAUNCH_BLOCKING</samp> environment variable to 1. This feature is
                                 provided for debugging purposes only and should not be used as a way to make
                                 production software run reliably. 
                              </p>
                              <p class="p"> Kernel launches are synchronous if hardware counters are collected via a profiler
                                 (Nsight, Visual Profiler) unless concurrent kernel profiling is enabled.
                                 <samp class="ph codeph">Async</samp> memory copies will also be synchronous if they involve
                                 host memory that is not page-locked. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="concurrent-kernel-execution"><a name="concurrent-kernel-execution" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#concurrent-kernel-execution" name="concurrent-kernel-execution" shape="rect">3.2.6.2.&nbsp;Concurrent Kernel Execution</a></h3>
                           <div class="body conbody">
                              <p class="p">Some devices of compute capability 2.x and higher can execute multiple kernels
                                 concurrently. Applications may query this capability by checking the
                                 <samp class="ph codeph">concurrentKernels</samp> device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>), which is equal to 1 for devices that support it. 
                              </p>
                              <p class="p"> The maximum number of kernel launches that a device can execute concurrently depends
                                 on its compute capability and is listed in <a class="xref" href="index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">Table 15</a>. 
                              </p>
                              <p class="p">A kernel from one CUDA context cannot execute concurrently with a kernel from another
                                 CUDA context.
                              </p>
                              <p class="p">Kernels that use many textures or a large amount of local memory are less likely to
                                 execute concurrently with other kernels.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="overlap-of-data-transfer-and-kernel-execution"><a name="overlap-of-data-transfer-and-kernel-execution" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#overlap-of-data-transfer-and-kernel-execution" name="overlap-of-data-transfer-and-kernel-execution" shape="rect">3.2.6.3.&nbsp;Overlap of Data Transfer and Kernel Execution</a></h3>
                           <div class="body conbody">
                              <p class="p">Some devices can perform an asynchronous memory copy to or from the GPU concurrently
                                 with kernel execution. Applications may query this capability by checking the
                                 <samp class="ph codeph">asyncEngineCount</samp> device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>), which is greater than zero for devices that
                                 support it. If host memory is involved in the copy, it must be page-locked. 
                              </p>
                              <p class="p">It is also possible to perform an intra-device copy simultaneously with kernel
                                 execution (on devices that support the <samp class="ph codeph">concurrentKernels</samp> device
                                 property) and/or with copies to or from the device (for devices that support the
                                 <samp class="ph codeph">asyncEngineCount</samp> property). Intra-device copies are initiated
                                 using the standard memory copy functions with destination and source addresses
                                 residing on the same device.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="concurrent-data-transfers"><a name="concurrent-data-transfers" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#concurrent-data-transfers" name="concurrent-data-transfers" shape="rect">3.2.6.4.&nbsp;Concurrent Data Transfers</a></h3>
                           <div class="body conbody">
                              <p class="p">Some devices of compute capability 2.x and higher can overlap copies to and from the
                                 device. Applications may query this capability by checking the
                                 <samp class="ph codeph">asyncEngineCount</samp> device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>), which is equal to 2 for devices that support it.
                                 In order to be overlapped, any host memory involved in the transfers must be
                                 page-locked. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="streams"><a name="streams" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#streams" name="streams" shape="rect">3.2.6.5.&nbsp;Streams</a></h3>
                           <div class="body conbody">
                              <p class="p"> Applications manage the concurrent operations described above through
                                 <dfn class="term">streams</dfn>. A stream is a sequence of commands (possibly issued by
                                 different host threads) that execute in order. Different streams, on the other hand,
                                 may execute their commands out of order with respect to one another or concurrently;
                                 this behavior is not guaranteed and should therefore not be relied upon for
                                 correctness (e.g., inter-kernel communication is undefined). The commands issued on a
                                 stream may execute when all the dependencies of the command are met. The dependencies
                                 could be previously launched commands on same stream or dependencies from other streams.
                                 The successful completion of synchronize call guarantees that all the commands launched
                                 are completed.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="creation-and-destruction-streams"><a name="creation-and-destruction-streams" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#creation-and-destruction-streams" name="creation-and-destruction-streams" shape="rect">3.2.6.5.1.&nbsp;Creation and Destruction</a></h3>
                              <div class="body conbody">
                                 <p class="p">A stream is defined by creating a stream object and specifying it as the stream
                                    parameter to a sequence of kernel launches and host <samp class="ph codeph">&lt;-&gt;</samp>
                                    device memory copies. The following code sample creates two streams and allocates an
                                    array <samp class="ph codeph">hostPtr</samp> of <samp class="ph codeph">float</samp> in page-locked memory. 
                                 </p><pre xml:space="preserve">cudaStream_t stream[2];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i)
    cudaStreamCreate(&amp;stream[i]);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* hostPtr;
cudaMallocHost(&amp;hostPtr, 2 * size);</pre><p class="p">Each of these streams is defined by the following code sample as a sequence of one
                                    memory copy from host to device, one kernel launch, and one memory copy from device
                                    to host:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i) {
    cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 512, 0, stream[i]<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>
          (outputDevPtr + i * size, inputDevPtr + i * size, size);
    cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);
}</pre><p class="p"> Each stream copies its portion of input array <samp class="ph codeph">hostPtr</samp> to array
                                    <samp class="ph codeph">inputDevPtr</samp> in device memory, processes
                                    <samp class="ph codeph">inputDevPtr</samp> on the device by calling
                                    <samp class="ph codeph">MyKernel()</samp>, and copies the result <samp class="ph codeph">outputDevPtr</samp>
                                    back to the same portion of <samp class="ph codeph">hostPtr</samp>. <a class="xref" href="index.html#overlapping-behavior" shape="rect">Overlapping Behavior</a> describes how the streams overlap in this example
                                    depending on the capability of the device. Note that <samp class="ph codeph">hostPtr</samp> must
                                    point to page-locked host memory for any overlap to occur. 
                                 </p>
                                 <p class="p"> Streams are released by calling <samp class="ph codeph">cudaStreamDestroy()</samp>. 
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i)
    cudaStreamDestroy(stream[i]);</pre><p class="p"> In case the device is still doing work in the stream when
                                    <samp class="ph codeph">cudaStreamDestroy()</samp> is called, the function will return
                                    immediately and the resources associated with the stream will be released
                                    automatically once the device has completed all work in the stream. 
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="default-stream"><a name="default-stream" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#default-stream" name="default-stream" shape="rect">3.2.6.5.2.&nbsp;Default Stream</a></h3>
                              <div class="body conbody">
                                 <p class="p">Kernel launches and host <samp class="ph codeph">&lt;-&gt;</samp> device memory copies that
                                    do not specify any stream parameter, or equivalently that set the stream parameter
                                    to zero, are issued to the default stream. They are therefore executed in order.
                                 </p>
                                 <div class="p"> For code that is compiled using the <samp class="ph codeph">--default-stream per-thread</samp>
                                    compilation flag (or that defines the
                                    <samp class="ph codeph">CUDA_API_PER_THREAD_DEFAULT_STREAM</samp> macro before including CUDA
                                    headers (<samp class="ph codeph">cuda.h</samp> and <samp class="ph codeph">cuda_runtime.h</samp>)), the default
                                    stream is a regular stream and each host thread has its own default stream.
                                    
                                    <div class="note note"><span class="notetitle">Note:</span><samp class="ph codeph">#define CUDA_API_PER_THREAD_DEFAULT_STREAM 1</samp> cannot be
                                       used to enable this behavior when the code is compiled by
                                       <samp class="ph codeph">nvcc</samp> as <samp class="ph codeph">nvcc</samp> implicitly includes
                                       <samp class="ph codeph">cuda_runtime.h</samp> at the top of the translation unit. In
                                       this case the <samp class="ph codeph">--default-stream per-thread</samp> compilation
                                       flag needs to be used or the
                                       <samp class="ph codeph">CUDA_API_PER_THREAD_DEFAULT_STREAM</samp> macro needs to be
                                       defined with the <samp class="ph codeph">-DCUDA_API_PER_THREAD_DEFAULT_STREAM=1</samp>
                                       compiler flag.
                                    </div>
                                 </div>
                                 <p class="p">For code that is compiled using the <samp class="ph codeph">--default-stream legacy</samp>
                                    compilation flag, the default stream is a special stream called the <dfn class="term">NULL
                                       stream</dfn> and each device has a single NULL stream used for all host
                                    threads. The NULL stream is special as it causes implicit synchronization as
                                    described in <a class="xref" href="index.html#implicit-synchronization" shape="rect">Implicit Synchronization</a>. 
                                 </p>
                                 <p class="p">For code that is compiled without specifying a <samp class="ph codeph">--default-stream</samp>
                                    compilation flag, <samp class="ph codeph">--default-stream legacy</samp> is assumed as the
                                    default. 
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="explicit-synchronization"><a name="explicit-synchronization" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#explicit-synchronization" name="explicit-synchronization" shape="rect">3.2.6.5.3.&nbsp;Explicit Synchronization</a></h3>
                              <div class="body conbody">
                                 <p class="p">There are various ways to explicitly synchronize streams with each other.</p>
                                 <p class="p"><samp class="ph codeph">cudaDeviceSynchronize()</samp> waits until all preceding commands in all
                                    streams of all host threads have completed.
                                 </p>
                                 <p class="p"><samp class="ph codeph">cudaStreamSynchronize()</samp>takes a stream as a parameter and waits until
                                    all preceding commands in the given stream have completed. It can be used to
                                    synchronize the host with a specific stream, allowing other streams to continue
                                    executing on the device.
                                 </p>
                                 <p class="p"><samp class="ph codeph">cudaStreamWaitEvent()</samp>takes a stream and an event as parameters (see
                                    <a class="xref" href="index.html#events" shape="rect">Events</a> for a description of events)and makes all the commands
                                    added to the given stream after the call to
                                    <samp class="ph codeph">cudaStreamWaitEvent()</samp>delay their execution until the given event
                                    has completed.
                                 </p>
                                 <p class="p"><samp class="ph codeph">cudaStreamQuery()</samp>provides applications with a way to know if all
                                    preceding commands in a stream have completed.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="implicit-synchronization"><a name="implicit-synchronization" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#implicit-synchronization" name="implicit-synchronization" shape="rect">3.2.6.5.4.&nbsp;Implicit Synchronization</a></h3>
                              <div class="body conbody">
                                 <p class="p">Two commands from different streams cannot run concurrently if any one of the
                                    following operations is issued in-between them by the host thread: 
                                 </p>
                                 <ul class="ul">
                                    <li class="li">a page-locked host memory allocation,</li>
                                    <li class="li">a device memory allocation,</li>
                                    <li class="li">a device memory set,</li>
                                    <li class="li">a memory copy between two addresses to the same device memory,</li>
                                    <li class="li">any CUDA command to the NULL stream,</li>
                                    <li class="li"> a switch between the L1/shared memory configurations described in <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a> and <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a>. 
                                    </li>
                                 </ul>
                                 <p class="p">For devices that support concurrent kernel execution and are of compute capability
                                    3.0 or lower, any operation that requires a dependency check to see if a streamed
                                    kernel launch is complete: 
                                 </p>
                                 <ul class="ul">
                                    <li class="li">Can start executing only when all thread blocks of all prior kernel launches
                                       from any stream in the CUDA context have started executing; 
                                    </li>
                                    <li class="li">Blocks all later kernel launches from any stream in the CUDA context until the
                                       kernel launch being checked is complete. 
                                    </li>
                                 </ul>
                                 <p class="p">Operations that require a dependency check include any other commands within the same
                                    stream as the launch being checked and any call to
                                    <samp class="ph codeph">cudaStreamQuery()</samp> on that stream. Therefore, applications
                                    should follow these guidelines to improve their potential for concurrent kernel
                                    execution: 
                                 </p>
                                 <ul class="ul">
                                    <li class="li">All independent operations should be issued before dependent operations,</li>
                                    <li class="li">Synchronization of any kind should be delayed as long as possible.</li>
                                 </ul>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="overlapping-behavior"><a name="overlapping-behavior" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#overlapping-behavior" name="overlapping-behavior" shape="rect">3.2.6.5.5.&nbsp;Overlapping Behavior</a></h3>
                              <div class="body conbody">
                                 <p class="p">The amount of execution overlap between two streams depends on the order in which the
                                    commands are issued to each stream and whether or not the device supports overlap of
                                    data transfer and kernel execution (see <a class="xref" href="index.html#overlap-of-data-transfer-and-kernel-execution" shape="rect">Overlap of Data Transfer and Kernel Execution</a>), concurrent kernel
                                    execution (see <a class="xref" href="index.html#concurrent-kernel-execution" shape="rect">Concurrent Kernel Execution</a>), and/or concurrent data
                                    transfers (see <a class="xref" href="index.html#concurrent-data-transfers" shape="rect">Concurrent Data Transfers</a>). 
                                 </p>
                                 <p class="p">For example, on devices that do not support concurrent data transfers, the two
                                    streams of the code sample of <a class="xref" href="index.html#creation-and-destruction-streams" shape="rect">Creation and Destruction</a> do
                                    not overlap at all because the memory copy from host to device is issued to
                                    stream[1] after the memory copy from device to host is issued to stream[0], so it
                                    can only start once the memory copy from device to host issued to stream[0] has
                                    completed. If the code is rewritten the following way (and assuming the device
                                    supports overlap of data transfer and kernel execution) 
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i)
    cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i)
    MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 512, 0, stream[i]<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>
          (outputDevPtr + i * size, inputDevPtr + i * size, size);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i)
    cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);</pre><p class="p">then the memory copy from host to device issued to stream[1] overlaps with the kernel
                                    launch issued to stream[0].
                                 </p>
                                 <p class="p">On devices that do support concurrent data transfers, the two streams of the code
                                    sample of <a class="xref" href="index.html#creation-and-destruction-streams" shape="rect">Creation and Destruction</a> do overlap: The memory
                                    copy from host to device issued to stream[1] overlaps with the memory copy from
                                    device to host issued to stream[0] and even with the kernel launch issued to
                                    stream[0] (assuming the device supports overlap of data transfer and kernel
                                    execution). However, for devices of compute capability 3.0 or lower, the kernel
                                    executions cannot possibly overlap because the second kernel launch is issued to
                                    stream[1] after the memory copy from device to host is issued to stream[0], so it is
                                    blocked until the first kernel launch issued to stream[0] is complete as per <a class="xref" href="index.html#implicit-synchronization" shape="rect">Implicit Synchronization</a>. If the code is rewritten as above, the
                                    kernel executions overlap (assuming the device supports concurrent kernel execution)
                                    since the second kernel launch is issued to stream[1] before the memory copy from
                                    device to host is issued to stream[0]. In that case however, the memory copy from
                                    device to host issued to stream[0] only overlaps with the last thread blocks of the
                                    kernel launch issued to stream[1] as per <a class="xref" href="index.html#implicit-synchronization" shape="rect">Implicit Synchronization</a>,
                                    which can represent only a small portion of the total execution time of the kernel.
                                    
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="stream-callbacks"><a name="stream-callbacks" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#stream-callbacks" name="stream-callbacks" shape="rect">3.2.6.5.6.&nbsp;Host Functions (Callbacks)</a></h3>
                              <div class="body conbody">
                                 <p class="p">
                                    The runtime provides a way to insert a CPU function call at any point into a stream via 
                                    <samp class="ph codeph">cudaLaunchHostFunc()</samp>. The provided function is executed on the host
                                    once all commands issued to the stream before the callback have completed.
                                    
                                 </p>
                                 <p class="p">
                                    The following code sample adds the host function
                                    <samp class="ph codeph">MyCallback</samp> to each of two streams
                                    after issuing a host-to-device memory copy, a kernel launch and a
                                    device-to-host memory copy into each stream. The function will
                                    begin execution on the host after each of the device-to-host memory
                                    copies completes.
                                    
                                 </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> CUDART_CB MyCallback(cudaStream_t stream, cudaError_t status, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *data){
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Inside callback %d\n"</span>, (size_t)data);
}
...
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t i = 0; i &lt; 2; ++i) {
    cudaMemcpyAsync(devPtrIn[i], hostPtr[i], size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 512, 0, stream[i]<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(devPtrOut[i], devPtrIn[i], size);
    cudaMemcpyAsync(hostPtr[i], devPtrOut[i], size, cudaMemcpyDeviceToHost, stream[i]);
    cudaLaunchHostFunc(stream[i], MyCallback, (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)i);
}
        </pre><p class="p">
                                    The commands that are issued in a stream after a host function do not start executing before the
                                    function has completed.
                                    
                                 </p>
                                 <p class="p">
                                    A host function enqueued into a stream must not make CUDA API calls (directly or indirectly),
                                    as it might end up waiting on itself if it makes such a call leading to a deadlock.
                                    
                                 </p>
                                 <p class="p"></p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="stream-priorities"><a name="stream-priorities" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#stream-priorities" name="stream-priorities" shape="rect">3.2.6.5.7.&nbsp;Stream Priorities</a></h3>
                              <div class="body conbody">
                                 <p class="p"> The relative priorities of streams can be specified at creation using 
                                    <samp class="ph codeph">cudaStreamCreateWithPriority()</samp>. The range of allowable priorities, ordered as
                                    [ highest priority, lowest priority ] can be obtained using the 
                                    <samp class="ph codeph">cudaDeviceGetStreamPriorityRange()</samp> function. At runtime, pending work in
                                    higher-priority streams takes preference over pending work in low-priority streams.
                                    	
                                 </p>
                                 <p class="p"> The following code sample obtains the allowable range of priorities for the current
                                    device, and creates streams with the highest and lowest available priorities. 
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// get the range of stream priorities for this device</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> priority_high, priority_low;
cudaDeviceGetStreamPriorityRange(&amp;priority_low, &amp;priority_high);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// create streams with highest and lowest available priorities</span>
cudaStream_t st_high, st_low;
cudaStreamCreateWithPriority(&amp;st_high, cudaStreamNonBlocking, priority_high);
cudaStreamCreateWithPriority(&amp;st_low, cudaStreamNonBlocking, priority_low);</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="cuda-graphs"><a name="cuda-graphs" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#cuda-graphs" name="cuda-graphs" shape="rect">3.2.6.6.&nbsp;CUDA Graphs</a></h3>
                           <div class="body conbody">
                              <p class="p">CUDA Graphs present a new model for work submission in CUDA. A graph is a series of
                                 operations, such as kernel launches, connected by dependencies, which is defined
                                 separately from its execution. This allows a graph to be defined once and then
                                 launched repeatedly. Separating out the definition of a graph from its execution
                                 enables a number of optimizations: first, CPU launch costs are reduced compared to
                                 streams, because much of the setup is done in advance; second,  presenting the whole
                                 workflow to CUDA enables optimizations which might not be possible with the
                                 piecewise work submission mechanism of streams. 
                              </p>
                              <p class="p">To see the optimizations possible with graphs, consider what happens in a stream:
                                 when you place a kernel into a stream, the host driver performs a sequence of
                                 operations in preparation for the execution of the kernel on the GPU. These
                                 operations, necessary for setting up and launching the kernel, are an overhead cost
                                 which must be paid for each kernel that is issued. For a GPU kernel with a short
                                 execution time, this overhead cost can be a significant fraction of the overall
                                 end-to-end execution time. 
                              </p>
                              <div class="p">Work submission using graphs is separated into three distinct stages: definition,
                                 instantiation, and execution. <a name="cuda-graphs__ul_m2w_wnr_2fb" shape="rect">
                                    <!-- --></a><ul class="ul" id="cuda-graphs__ul_m2w_wnr_2fb">
                                    <li class="li">During the definition phase, a program creates a description of the
                                       operations in the graph along with the dependencies between them. 
                                    </li>
                                    <li class="li">Instantiation takes a snapshot of the graph template, validates it, and
                                       performs much of the setup and initialization of work with the aim of
                                       minimizing what needs to be done at launch. The resulting instance is known
                                       as an <em class="ph i">executable graph.</em></li>
                                    <li class="li">An executable graph may be launched into a stream, similar to any other CUDA
                                       work. It may be launched any number of times without repeating the
                                       instantiation. 
                                    </li>
                                 </ul>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="graph-structure"><a name="graph-structure" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#graph-structure" name="graph-structure" shape="rect">3.2.6.6.1.&nbsp;Graph Structure</a></h3>
                              <div class="body conbody">
                                 <p class="p">An operation forms a node in a graph. The dependencies between the operations are the
                                    edges. These dependencies constrain the execution sequence of the operations. 
                                 </p>
                                 <p class="p">An operation may be scheduled at any time once the nodes on which it depends are
                                    complete. Scheduling is left up to the CUDA system. 
                                 </p>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="node-types"><a name="node-types" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#node-types" name="node-types" shape="rect">3.2.6.6.1.1.&nbsp;Node Types</a></h3>
                                 <div class="body conbody">
                                    <p class="p">A graph node can be one of: </p>
                                    <div class="p"><a name="node-types__ul_dld_g5j_2fb" shape="rect">
                                          <!-- --></a><ul class="ul" id="node-types__ul_dld_g5j_2fb">
                                          <li class="li">kernel  </li>
                                          <li class="li">CPU function call </li>
                                          <li class="li">memory copy </li>
                                          <li class="li">memset </li>
                                          <li class="li">empty node </li>
                                          <li class="li">waiting on an <a class="xref" href="index.html#events" shape="rect">event</a></li>
                                          <li class="li">recording an <a class="xref" href="index.html#events" shape="rect">event</a></li>
                                          <li class="li">signalling an <a class="xref" href="index.html#external-resource-interoperability" shape="rect">external semaphore</a></li>
                                          <li class="li">waiting on an <a class="xref" href="index.html#external-resource-interoperability" shape="rect">external semaphore</a></li>
                                          <li class="li">child graph: To execute a separate nested graph. See <a class="xref" href="index.html#node-types__fig-child-graph" shape="rect">Figure 9</a>.
                                          </li>
                                       </ul>
                                    </div>
                                    <div class="fig fignone" id="node-types__fig-child-graph"><a name="node-types__fig-child-graph" shape="rect">
                                          <!-- --></a><span class="figcap">Figure 9. Child Graph Example</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" width="200" src="../common/graphics/child-graph.png" alt="Child Graph Example"></img></div><br clear="none"></br></div>
                                 </div>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="creating-a-graph-using-api"><a name="creating-a-graph-using-api" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#creating-a-graph-using-api" name="creating-a-graph-using-api" shape="rect">3.2.6.6.2.&nbsp;Creating a Graph Using Graph APIs</a></h3>
                              <div class="body conbody">
                                 <p class="p">Graphs can be created via two mechanisms: explicit API and stream capture. The
                                    following is an example of creating and executing the below graph. 
                                 </p>
                                 <div class="fig fignone" id="creating-a-graph-using-api__fig-creating-using-graph-apis"><a name="creating-a-graph-using-api__fig-creating-using-graph-apis" shape="rect">
                                       <!-- --></a><span class="figcap">Figure 10. Creating a Graph Using Graph APIs Example</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" width="200" src="../common/graphics/create-a-graph.png" alt="Child Graph Example"></img></div><br clear="none"></br></div><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create the graph - it starts out empty</span>
cudaGraphCreate(&amp;graph, 0);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// For the purpose of this example, we'll create</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the nodes separately from the dependencies to</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// demonstrate that it can be done in two stages.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note that dependencies can also be specified </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// at node creation. </span>
cudaGraphAddKernelNode(&amp;a, graph, NULL, 0, &amp;nodeParams);
cudaGraphAddKernelNode(&amp;b, graph, NULL, 0, &amp;nodeParams);
cudaGraphAddKernelNode(&amp;c, graph, NULL, 0, &amp;nodeParams);
cudaGraphAddKernelNode(&amp;d, graph, NULL, 0, &amp;nodeParams);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Now set up dependencies on each node</span>
cudaGraphAddDependencies(graph, &amp;a, &amp;b, 1);     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// A-&gt;B</span>
cudaGraphAddDependencies(graph, &amp;a, &amp;c, 1);     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// A-&gt;C</span>
cudaGraphAddDependencies(graph, &amp;b, &amp;d, 1);     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// B-&gt;D</span>
cudaGraphAddDependencies(graph, &amp;c, &amp;d, 1);     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// C-&gt;D</span></pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="creating-a-graph-using-stream-capture"><a name="creating-a-graph-using-stream-capture" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#creating-a-graph-using-stream-capture" name="creating-a-graph-using-stream-capture" shape="rect">3.2.6.6.3.&nbsp;Creating a Graph Using Stream Capture</a></h3>
                              <div class="body conbody">
                                 <p class="p">Stream capture provides a mechanism to create a graph from existing stream-based
                                    APIs. A section of code which launches work into streams, including existing code,
                                    can be bracketed with calls to <samp class="ph codeph">cudaStreamBeginCapture()</samp> and
                                    <samp class="ph codeph">cudaStreamEndCapture()</samp>. See below.
                                 </p><pre xml:space="preserve">cudaGraph_t graph;

cudaStreamBeginCapture(stream);

kernel_A<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);
kernel_B<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);
libraryCall(stream);
kernel_C<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);

cudaStreamEndCapture(stream, &amp;graph);</pre><p class="p">A call to <samp class="ph codeph">cudaStreamBeginCapture()</samp> places a stream in capture mode.
                                    When a stream is being captured, work launched into the stream is not enqueued for
                                    execution. It is instead appended to an internal graph that is progressively being
                                    built up. This graph is then returned by calling
                                    <samp class="ph codeph">cudaStreamEndCapture()</samp>, which also ends capture mode for the
                                    stream. A graph which is actively being constructed by stream capture is referred to
                                    as a <em class="ph i">capture graph.</em></p>
                                 <p class="p">Stream capture can be used on any CUDA stream except
                                    <samp class="ph codeph">cudaStreamLegacy</samp> (the NULL stream). Note that it <em class="ph i">can</em>
                                    be used on <samp class="ph codeph">cudaStreamPerThread</samp>. If a program is using the legacy
                                    stream, it may be possible to redefine stream 0 to be the per-thread stream with no
                                    functional change. See <a class="xref" href="index.html#default-stream" shape="rect">Default Stream</a>. 
                                 </p>
                                 <p class="p">Whether a stream is being captured can be queried with
                                    <samp class="ph codeph">cudaStreamIsCapturing()</samp>. 
                                 </p>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="cross-stream-dependencies"><a name="cross-stream-dependencies" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#cross-stream-dependencies" name="cross-stream-dependencies" shape="rect">3.2.6.6.3.1.&nbsp;Cross-stream Dependencies and Events</a></h3>
                                 <div class="body conbody">
                                    <p class="p">Stream capture can handle cross-stream dependencies expressed with
                                       <samp class="ph codeph">cudaEventRecord()</samp> and <samp class="ph codeph">cudaStreamWaitEvent()</samp>,
                                       provided the event being waited upon was recorded into the same capture graph. 
                                    </p>
                                    <p class="p">When an event is recorded in a stream that is in capture mode, it results in a
                                       <em class="ph i">captured event.</em> A captured event represents a set of nodes in a capture
                                       graph. 
                                    </p>
                                    <p class="p">When a captured event is waited on by a stream, it places the stream in capture mode
                                       if it is not already, and the next item in the stream will have additional
                                       dependencies on the nodes in the captured event. The two streams are then being captured
                                       to the same capture graph. 
                                    </p>
                                    <p class="p">When cross-stream dependencies are present in stream capture,
                                       <samp class="ph codeph">cudaStreamEndCapture()</samp> must still be called in the same stream
                                       where <samp class="ph codeph">cudaStreamBeginCapture()</samp> was called; this is the <em class="ph i">origin
                                          stream</em>. Any other streams which are being captured to the same capture
                                       graph, due to event-based dependencies, must also be joined back to the origin
                                       stream. This is illustrated below. All streams being captured to the same capture
                                       graph are taken out of capture mode upon <samp class="ph codeph">cudaStreamEndCapture()</samp>.
                                       Failure to rejoin to the origin stream will result in failure of the overall capture
                                       operation. 
                                    </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// stream1 is the origin stream</span>
cudaStreamBeginCapture(stream1);

kernel_A<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Fork into stream2</span>
cudaEventRecord(event1, stream1);
cudaStreamWaitEvent(stream2, event1);

kernel_B<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);
kernel_C<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream2 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Join stream2 back to origin stream (stream1)</span>
cudaEventRecord(event2, stream2);
cudaStreamWaitEvent(stream1, event2);

kernel_D<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> ..., stream1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// End capture in the origin stream</span>
cudaStreamEndCapture(stream1, &amp;graph);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// stream1 and stream2 no longer in capture mode    </span></pre><div class="p">Graph returned by the above code is shown in <a class="xref" href="index.html#creating-a-graph-using-api__fig-creating-using-graph-apis" shape="rect">Figure 10</a>.
                                       
                                       
                                       <div class="note note"><span class="notetitle">Note:</span> When a stream is taken out of capture mode, the next non-captured
                                          item in the stream (if any) will still have a dependency on the most recent
                                          prior non-captured item, despite intermediate items having been removed.
                                          
                                       </div>
                                    </div>
                                 </div>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="prohibited-unhandled-operations"><a name="prohibited-unhandled-operations" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#prohibited-unhandled-operations" name="prohibited-unhandled-operations" shape="rect">3.2.6.6.3.2.&nbsp;Prohibited and Unhandled Operations </a></h3>
                                 <div class="body conbody">
                                    <p class="p">It is invalid to synchronize or query the execution status of a stream which is being
                                       captured or a captured event, because they do not represent items scheduled for
                                       execution. It is also invalid to query the execution status of or synchronize a
                                       broader handle which encompasses an active stream capture, such as a device or
                                       context handle when any associated stream is in capture mode. 
                                    </p>
                                    <p class="p">When any stream in the same context is being captured, and it was not created with
                                       <samp class="ph codeph">cudaStreamNonBlocking</samp>, any attempted use of the legacy stream
                                       is invalid. This is because the legacy stream handle at all times encompasses these
                                       other streams; enqueueing to the legacy stream would create a dependency on the
                                       streams being captured, and querying it or synchronizing it would query or
                                       synchronize the streams being captured. 
                                    </p>
                                    <p class="p">It is therefore also invalid to call synchronous APIs in this case. Synchronous APIs,
                                       such as <samp class="ph codeph">cudaMemcpy()</samp>, enqueue work to the legacy stream and
                                       synchronize it before returning. 
                                    </p>
                                    <div class="p">
                                       <div class="note note"><span class="notetitle">Note:</span> As a general rule, when a dependency relation would connect something that is
                                          captured with something that was not captured and instead enqueued for
                                          execution, CUDA prefers to return an error rather than ignore the dependency. An
                                          exception is made for placing a stream into or out of capture mode; this severs
                                          a dependency relation between items added to the stream immediately before and
                                          after the mode transition.
                                       </div>
                                    </div>
                                    <p class="p">It is invalid to merge two separate capture graphs by waiting on a captured event
                                       from a stream which is being captured and is associated with a different capture
                                       graph than the event. It is invalid to wait on a non-captured event from a  stream
                                       which is being captured. 
                                    </p>
                                    <p class="p">A small number of APIs that enqueue asynchronous operations into streams are not
                                       currently supported in graphs and will return an error if called with a stream which
                                       is being captured, such as <samp class="ph codeph">cudaStreamAttachMemAsync()</samp>. 
                                    </p>
                                 </div>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="invalidation"><a name="invalidation" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#invalidation" name="invalidation" shape="rect">3.2.6.6.3.3.&nbsp;Invalidation </a></h3>
                                 <div class="body conbody">
                                    <p class="p">When an invalid operation is attempted during stream capture, any associated capture
                                       graphs are <em class="ph i">invalidated</em>. When a capture graph is invalidated, further use of
                                       any streams which are being captured or captured events associated with the graph is invalid and
                                       will return an error, until stream capture is ended with
                                       <samp class="ph codeph">cudaStreamEndCapture()</samp>. This call will take the associated
                                       streams out of capture mode, but will also return an error value and a NULL graph. 
                                    </p>
                                 </div>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="updating-instantiated-graphs"><a name="updating-instantiated-graphs" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#updating-instantiated-graphs" name="updating-instantiated-graphs" shape="rect">3.2.6.6.4.&nbsp;Updating Instantiated Graphs</a></h3>
                              <div class="body conbody">
                                 <p class="p">Work submission using graphs is separated into three distinct
                                    stages: definition, instantiation, and execution. In situations
                                    where the workflow is not changing, the overhead of definition and
                                    instantiation can be amortized over many executions, and graphs
                                    provide a clear advantage over streams.
                                 </p>
                                 <p class="p">A graph is a snapshot of a workflow, including kernels,
                                    parameters, and dependencies, in order to replay it as rapidly and
                                    efficiently as possible. In situations where the workflow changes
                                    the graph becomes out of date
                                    and must be modified. Major changes to graph structure such as
                                    topology or types of nodes will require re-instantiation of the
                                    source graph because various topology-related optimization
                                    techniques must be re-applied.
                                 </p>
                                 <p class="p">The cost of repeated instantiation can reduce the overall
                                    performance benefit from graph execution, but it is common for only
                                    node parameters, such as kernel parameters and <samp class="ph codeph">cudaMemcpy</samp>
                                    addresses, to change while graph topology remains the same. For
                                    this case, CUDA provides a lightweight mechanism known as Graph
                                    Update, which allows certain node parameters to be modified
                                    in-place without having to rebuild the entire graph. This is much
                                    more efficient than re-instantiation. 
                                 </p>
                                 <p class="p">Updates will take effect the next time the graph is launched, so they
                                    will not impact previous graph launches, even if they are running
                                    at the time of the update. A graph may be updated and relaunched repeatedly, so
                                    multiple updates/launches can be queued on a stream.
                                 </p>
                                 <p class="p">CUDA provides two mechanisms for updating instantiated graphs,
                                    whole graph update and individual node update. Whole graph update
                                    allows the user to supply a topologically identical <samp class="ph codeph">cudaGraph_t</samp>
                                    object whose nodes contain updated parameters. Individual node
                                    update allows the user to explicitly update the parameters of
                                    individual nodes. Using an updated <samp class="ph codeph">cudaGraph_t</samp> is more convenient
                                    when a large number of nodes are being updated, or when the graph
                                    topology is unknown to the caller (i.e., The graph resulted from
                                    stream capture of a library call). Using individual node update is
                                    preferred when the number of changes is small and the user has the
                                    handles to the nodes requiring updates. Individual node update
                                    skips the topology checks and comparisons for unchanged nodes, so
                                    it can be more efficient in many cases. The following sections
                                    explain each approach in more detail.
                                 </p>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="graph-update-limitations"><a name="graph-update-limitations" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#graph-update-limitations" name="graph-update-limitations" shape="rect">3.2.6.6.4.1.&nbsp;Graph Update Limitations</a></h3>
                                 <div class="body conbody">
                                    <p class="p">Kernel nodes:</p>
                                    <ul class="ul">
                                       <li class="li">The owning context of the function cannot change.</li>
                                       <li class="li">A node whose function originally did not use CUDA dynamic parallelism cannot be updated to a function which uses CUDA dynamic
                                          parallelism.
                                       </li>
                                    </ul>
                                    <p class="p"><samp class="ph codeph">cudaMemset</samp> and <samp class="ph codeph">cudaMemcpy</samp> nodes:
                                    </p>
                                    <ul class="ul">
                                       <li class="li">The CUDA device(s) to which the operand(s) was allocated/mapped cannot change.</li>
                                       <li class="li">The source/destination memory must be allocated from the same context as the original source/destination memory.</li>
                                       <li class="li">Only 1D <samp class="ph codeph">cudaMemset</samp>/<samp class="ph codeph">cudaMemcpy</samp> nodes can be changed.
                                       </li>
                                    </ul>
                                    <p class="p">Additional memcpy node restrictions:</p>
                                    <ul class="ul">
                                       <li class="li">Changing either the source or destination memory type (i.e., <samp class="ph codeph">cudaPitchedPtr</samp>, <samp class="ph codeph">cudaArray_t</samp>, etc.), 
                                          or the type of transfer (i.e., <samp class="ph codeph">cudaMemcpyKind</samp>) is not supported.
                                       </li>
                                    </ul>
                                    <p class="p">External semaphore wait nodes and record nodes:</p>
                                    <ul class="ul">
                                       <li class="li">Changing the number of semaphores is not supported.</li>
                                    </ul>
                                    <p class="p">There are no restrictions on updates to host nodes, event record nodes, or event wait nodes.</p>
                                 </div>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="whole-graph-update"><a name="whole-graph-update" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#whole-graph-update" name="whole-graph-update" shape="rect">3.2.6.6.4.2.&nbsp;Whole Graph Update</a></h3>
                                 <div class="body conbody">
                                    <p class="p"><samp class="ph codeph">cudaGraphExecUpdate()</samp> allows an instantiated
                                       graph (the "original graph") to be updated with the parameters
                                       from a topologically identical graph (the "updating" graph). The
                                       topology of the updating graph must be identical to the original
                                       graph used to instantiate the <samp class="ph codeph">cudaGraphExec_t</samp>.
                                       In addition, the order in which nodes were added to, or removed
                                       from, the original graph must match the order in which the nodes
                                       were added to (or removed from) the updating graph. Therefore, when
                                       using stream capture, the nodes must be captured in the same
                                       order and when using the explicit graph node creation APIs, all
                                       nodes must be added and/or deleted in the same order.
                                    </p>
                                    <p class="p">The following example
                                       shows how the API could be used to update an instantiated
                                       graph:
                                    </p><pre xml:space="preserve">cudaGraphExec_t graphExec = NULL;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 10; i++) {
    cudaGraph_t graph;
    cudaGraphExecUpdateResult updateResult;
    cudaGraphNode_t errorNode;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// In this example we use stream capture to create the graph.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// You can also use the Graph API to produce a graph.</span>
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Call a user-defined, stream based workload, for example</span>
    do_cuda_work(stream);

    cudaStreamEndCapture(stream, &amp;graph);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// If we've already instantiated the graph, try to update it directly</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// and avoid the instantiation overhead</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (graphExec != NULL) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// If the graph fails to update, errorNode will be set to the</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// node causing the failure and updateResult will be set to a</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// reason code.</span>
        cudaGraphExecUpdate(graphExec, graph, &amp;errorNode, &amp;updateResult);
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Instantiate during the first iteration or whenever the update</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// fails for any reason</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (graphExec == NULL || updateResult != cudaGraphExecUpdateSuccess) {

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// If a previous update failed, destroy the cudaGraphExec_t</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// before re-instantiating it</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (graphExec != NULL) {
            cudaGraphExecDestroy(graphExec);
        }   
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Instantiate graphExec from graph. The error node and</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error message parameters are unused here.</span>
        cudaGraphInstantiate(&amp;graphExec, graph, NULL, NULL, 0);
    }   

    cudaGraphDestroy(graph);
    cudaGraphLaunch(graphExec, stream);
    cudaStreamSynchronize(stream);
}</pre><p class="p">A typical workflow is to create the initial <samp class="ph codeph">cudaGraph_t</samp> using either the
                                       stream capture or graph API. The <samp class="ph codeph">cudaGraph_t</samp> is then instantiated and
                                       launched as normal. After the initial launch, a new <samp class="ph codeph">cudaGraph_t</samp> is created using
                                       the same method as the initial graph and 
                                       <samp class="ph codeph">cudaGraphExecUpdate()</samp> is called. If the graph update is successful, indicated
                                       by the <samp class="ph codeph">updateResult</samp> parameter in the above example, the
                                       updated <samp class="ph codeph">cudaGraphExec_t</samp> is launched. If the update fails for any reason,
                                       the <samp class="ph codeph">cudaGraphExecDestroy()</samp> and <samp class="ph codeph">cudaGraphInstantiate()</samp> are called
                                       to destroy the original <samp class="ph codeph">cudaGraphExec_t</samp> and instantiate a new one.
                                    </p>
                                    <p class="p">It is also possible to update the <samp class="ph codeph">cudaGraph_t</samp> nodes
                                       directly (i.e., Using <samp class="ph codeph">cudaGraphKernelNodeSetParams()</samp>) and
                                       subsequently update the <samp class="ph codeph">cudaGraphExec_t</samp>, however it is more
                                       efficient to use the explicit node update APIs covered in the
                                       next section.
                                    </p>
                                    <p class="p">Please see the <a class="xref" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH" target="_blank" shape="rect">Graph API</a> for more information on usage and current limitations.
                                    </p>
                                 </div>
                              </div>
                              <div class="topic concept nested5" xml:lang="en-US" id="individual-node-update"><a name="individual-node-update" shape="rect">
                                    <!-- --></a><h3 class="title topictitle2"><a href="#individual-node-update" name="individual-node-update" shape="rect">3.2.6.6.4.3.&nbsp;Individual node update</a></h3>
                                 <div class="body conbody">
                                    <p class="p">Instantiated graph node parameters can be updated directly. This
                                       eliminates the overhead of instantiation as well as the overhead of
                                       creating a new <samp class="ph codeph">cudaGraph_t</samp>. If the number of nodes requiring update
                                       is small relative to the total number of nodes in the graph, it is
                                       better to update the nodes individually. The following methods are
                                       available for updating <samp class="ph codeph">cudaGraphExec_t</samp> nodes:
                                    </p>
                                    <ul class="ul">
                                       <li class="li"><samp class="ph codeph">cudaGraphExecKernelNodeSetParams()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecMemcpyNodeSetParams()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecMemsetNodeSetParams()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecHostNodeSetParams()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecChildGraphNodeSetParams()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecEventRecordNodeSetEvent()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecEventWaitNodeSetEvent()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecExternalSemaphoresSignalNodeSetParams()</samp></li>
                                       <li class="li"><samp class="ph codeph">cudaGraphExecExternalSemaphoresWaitNodeSetParams()</samp></li>
                                    </ul>
                                    <p class="p">Please see the <a class="xref" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH" target="_blank" shape="rect">Graph API</a> for more information on usage and current limitations.
                                    </p>
                                 </div>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="using-graph-apis"><a name="using-graph-apis" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#using-graph-apis" name="using-graph-apis" shape="rect">3.2.6.6.5.&nbsp;Using Graph APIs </a></h3>
                              <div class="body conbody">
                                 <p class="p"><em class="ph i"><samp class="ph codeph">cudaGraph_t</samp> objects are not thread-safe.</em> It is the
                                    responsibility of the user to ensure that multiple threads do not concurrently
                                    access the same <samp class="ph codeph">cudaGraph_t</samp>. 
                                 </p>
                                 <p class="p">A <samp class="ph codeph">cudaGraphExec_t</samp> cannot run concurrently with itself. A launch of a
                                    <samp class="ph codeph">cudaGraphExec_t</samp> will be ordered after previous launches of the
                                    same executable graph. 
                                 </p>
                                 <p class="p">Graph execution is done in streams for ordering with other asynchronous work.
                                    However, the stream is for ordering only; it does not constrain the internal
                                    parallelism of the graph, nor does it affect where graph nodes execute. 
                                 </p>
                                 <p class="p">See <a class="xref" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH" target="_blank" shape="rect">Graph API.</a></p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="events"><a name="events" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#events" name="events" shape="rect">3.2.6.7.&nbsp;Events</a></h3>
                           <div class="body conbody">
                              <p class="p">The runtime also provides a way to closely monitor the device's progress, as well as
                                 perform accurate timing, by letting the application asynchronously record
                                 <dfn class="term">events</dfn> at any point in the program, and query when these events are
                                 completed. An event has completed when all tasks - or optionally, all commands in a
                                 given stream - preceding the event have completed. Events in stream zero are
                                 completed after all preceding tasks and commands in all streams are completed.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="creation-and-destruction-events"><a name="creation-and-destruction-events" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#creation-and-destruction-events" name="creation-and-destruction-events" shape="rect">3.2.6.7.1.&nbsp;Creation and Destruction</a></h3>
                              <div class="body conbody">
                                 <p class="p">The following code sample creates two events:</p><pre xml:space="preserve">cudaEvent_t start, stop;
cudaEventCreate(&amp;start);
cudaEventCreate(&amp;stop);</pre><p class="p">They are destroyed this way:</p><pre xml:space="preserve">cudaEventDestroy(start);
cudaEventDestroy(stop);</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="elapsed-time"><a name="elapsed-time" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#elapsed-time" name="elapsed-time" shape="rect">3.2.6.7.2.&nbsp;Elapsed Time</a></h3>
                              <div class="body conbody">
                                 <p class="p"> The events created in <a class="xref" href="index.html#creation-and-destruction-events" shape="rect">Creation and Destruction</a> can be used to
                                    time the code sample of <a class="xref" href="index.html#creation-and-destruction-streams" shape="rect">Creation and Destruction</a> the
                                    following way: 
                                 </p><pre xml:space="preserve">cudaEventRecord(start, 0);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 2; ++i) {
    cudaMemcpyAsync(inputDev + i * size, inputHost + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 512, 0, stream[i]<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>
               (outputDev + i * size, inputDev + i * size, size);
    cudaMemcpyAsync(outputHost + i * size, outputDev + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);
}
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> elapsedTime;
cudaEventElapsedTime(&amp;elapsedTime, start, stop);</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="synchronous-calls"><a name="synchronous-calls" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#synchronous-calls" name="synchronous-calls" shape="rect">3.2.6.8.&nbsp;Synchronous Calls</a></h3>
                           <div class="body conbody">
                              <p class="p">When a synchronous function is called, control is not returned to the host thread
                                 before the device has completed the requested task. Whether the host thread will
                                 then yield, block, or spin can be specified by calling
                                 <samp class="ph codeph">cudaSetDeviceFlags()</samp>with some specific flags (see reference
                                 manual for details) before any other CUDA call is performed by the host thread. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="multi-device-system"><a name="multi-device-system" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#multi-device-system" name="multi-device-system" shape="rect">3.2.7.&nbsp;Multi-Device System</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="device-enumeration"><a name="device-enumeration" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#device-enumeration" name="device-enumeration" shape="rect">3.2.7.1.&nbsp;Device Enumeration</a></h3>
                           <div class="body conbody">
                              <p class="p">A host system can have multiple devices. The following code sample shows how to
                                 enumerate these devices, query their properties, and determine the number of
                                 CUDA-enabled devices.
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> deviceCount;
cudaGetDeviceCount(&amp;deviceCount);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> device;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (device = 0; device &lt; deviceCount; ++device) {
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&amp;deviceProp, device);
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Device %d has compute capability %d.%d.\n"</span>,
           device, deviceProp.major, deviceProp.minor);
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="device-selection"><a name="device-selection" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#device-selection" name="device-selection" shape="rect">3.2.7.2.&nbsp;Device Selection</a></h3>
                           <div class="body conbody">
                              <p class="p">A host thread can set the device it operates on at any time by calling
                                 <samp class="ph codeph">cudaSetDevice()</samp>. Device memory allocations and kernel launches
                                 are made on the currently set device; streams and events are created in association
                                 with the currently set device. If no call to <samp class="ph codeph">cudaSetDevice()</samp> is
                                 made, the current device is device 0. 
                              </p>
                              <p class="p">The following code sample illustrates how setting the current device affects memory
                                 allocation and kernel execution.
                              </p><pre xml:space="preserve">size_t size = 1024 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
cudaSetDevice(0);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 0 as current</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* p0;
cudaMalloc(&amp;p0, size);       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory on device 0</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1000, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(p0); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 0</span>
cudaSetDevice(1);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 1 as current</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* p1;
cudaMalloc(&amp;p1, size);       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory on device 1</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1000, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(p1); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 1</span></pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="stream-and-event-behavior"><a name="stream-and-event-behavior" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#stream-and-event-behavior" name="stream-and-event-behavior" shape="rect">3.2.7.3.&nbsp;Stream and Event Behavior</a></h3>
                           <div class="body conbody">
                              <p class="p">A kernel launch will fail if it is issued to a stream that is not associated to the
                                 current device as illustrated in the following code sample.
                              </p><pre xml:space="preserve">cudaSetDevice(0);               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 0 as current</span>
cudaStream_t s0;
cudaStreamCreate(&amp;s0);          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create stream s0 on device 0</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 64, 0, s0<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 0 in s0</span>
cudaSetDevice(1);               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 1 as current</span>
cudaStream_t s1;
cudaStreamCreate(&amp;s1);          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create stream s1 on device 1</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 64, 0, s1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 1 in s1</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This kernel launch will fail:</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>100, 64, 0, s0<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 1 in s0</span></pre><p class="p">A memory copy will succeed even if it is issued to a stream that is not associated to
                                 the current device.
                              </p>
                              <p class="p"><samp class="ph codeph">cudaEventRecord()</samp> will fail if the input event and input stream are
                                 associated to different devices. 
                              </p>
                              <p class="p"><samp class="ph codeph">cudaEventElapsedTime(</samp>) will fail if the two input events are
                                 associated to different devices. 
                              </p>
                              <p class="p"><samp class="ph codeph">cudaEventSynchronize()</samp> and <samp class="ph codeph">cudaEventQuery()</samp> will
                                 succeed even if the input event is associated to a device that is different from the
                                 current device. 
                              </p>
                              <p class="p"><samp class="ph codeph">cudaStreamWaitEvent()</samp> will succeed even if the input stream and
                                 input event are associated to different devices.
                                 <samp class="ph codeph">cudaStreamWaitEvent()</samp> can therefore be used to synchronize
                                 multiple devices with each other. 
                              </p>
                              <p class="p"> Each device has its own default stream (see <a class="xref" href="index.html#default-stream" shape="rect">Default Stream</a>), so
                                 commands issued to the default stream of a device may execute out of order or
                                 concurrently with respect to commands issued to the default stream of any other
                                 device. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="peer-to-peer-memory-access"><a name="peer-to-peer-memory-access" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#peer-to-peer-memory-access" name="peer-to-peer-memory-access" shape="rect">3.2.7.4.&nbsp;Peer-to-Peer Memory Access</a></h3>
                           <div class="body conbody">
                              <p class="p">Depending on the system properties, specifically the PCIe and/or NVLINK topology, devices are able
                                 to address each other's memory (i.e., a kernel executing on
                                 one device can dereference a pointer to the memory of the other device). This peer-to-peer
                                 memory access feature is supported between two devices if
                                 					<samp class="ph codeph">cudaDeviceCanAccessPeer()</samp> returns true for these two devices. 
                              </p>
                              <p class="p">Peer-to-peer memory access is only supported in 64-bit applications and must be enabled between
                                 two devices by calling <samp class="ph codeph">cudaDeviceEnablePeerAccess()</samp> as illustrated in the following 
                                 code sample. On non-NVSwitch enabled systems, each device can support a system-wide maximum of eight
                                 peer connections.
                                 
                              </p>
                              <p class="p">A unified address space is used for both devices (see <a class="xref" href="index.html#unified-virtual-address-space" shape="rect">Unified Virtual Address Space</a>), so the same pointer can be used to
                                 address memory from both devices as shown in the code sample below. 
                              </p><pre xml:space="preserve">cudaSetDevice(0);                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 0 as current</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* p0;
size_t size = 1024 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
cudaMalloc(&amp;p0, size);              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory on device 0</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1000, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(p0);        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 0</span>
cudaSetDevice(1);                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 1 as current</span>
cudaDeviceEnablePeerAccess(0, 0);   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Enable peer-to-peer access</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// with device 0</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 1</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This kernel launch can access memory on device 0 at address p0</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1000, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(p0);</pre></div>
                           <div class="topic concept nested4" xml:lang="en-US" id="iommu-on-linux"><a name="iommu-on-linux" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#iommu-on-linux" name="iommu-on-linux" shape="rect">3.2.7.4.1.&nbsp;IOMMU on Linux</a></h3>
                              <div class="body conbody">
                                 <p class="p">On Linux only, CUDA and the display driver does not support IOMMU-enabled bare-metal
                                    PCIe peer to peer memory copy. However, CUDA and the display driver does support
                                    IOMMU via VM pass through. As a consequence, users on Linux, when running on a
                                    native bare metal system, should disable the IOMMU. The IOMMU should be enabled and
                                    the VFIO driver be used as a PCIe pass through for virtual machines. 
                                 </p>
                                 <p class="p">On Windows the above limitation does not exist. </p>
                                 <p class="p"> See also <a class="xref" href="https://download.nvidia.com/XFree86/Linux-x86_64/396.51/README/dma_issues.html" target="_blank" shape="rect">Allocating DMA Buffers on 64-bit
                                       Platforms</a>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="peer-to-peer-memory-copy"><a name="peer-to-peer-memory-copy" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#peer-to-peer-memory-copy" name="peer-to-peer-memory-copy" shape="rect">3.2.7.5.&nbsp;Peer-to-Peer Memory Copy</a></h3>
                           <div class="body conbody">
                              <p class="p">Memory copies can be performed between the memories of two different devices. </p>
                              <p class="p"> When a unified address space is used for both devices (see <a class="xref" href="index.html#unified-virtual-address-space" shape="rect">Unified Virtual Address Space</a>), this is done using the regular memory
                                 copy functions mentioned in <a class="xref" href="index.html#device-memory" shape="rect">Device Memory</a>. 
                              </p>
                              <p class="p">Otherwise, this is done using <samp class="ph codeph">cudaMemcpyPeer()</samp>,
                                 					<samp class="ph codeph">cudaMemcpyPeerAsync()</samp>, <samp class="ph codeph">cudaMemcpy3DPeer()</samp>, or
                                 <samp class="ph codeph">cudaMemcpy3DPeerAsync()</samp> as illustrated in the following code
                                 sample. 
                              </p><pre xml:space="preserve">cudaSetDevice(0);                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 0 as current</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* p0;
size_t size = 1024 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
cudaMalloc(&amp;p0, size);              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory on device 0</span>
cudaSetDevice(1);                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 1 as current</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* p1;
cudaMalloc(&amp;p1, size);              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory on device 1</span>
cudaSetDevice(0);                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 0 as current</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1000, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(p0);        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 0</span>
cudaSetDevice(1);                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set device 1 as current</span>
cudaMemcpyPeer(p1, 1, p0, 0, size); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy p0 to p1</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1000, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(p1);        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch kernel on device 1</span></pre><p class="p">A copy (in the implicit <dfn class="term">NULL</dfn> stream) between the memories of two
                                 different devices: 
                              </p><a name="peer-to-peer-memory-copy__ul_uff_wxz_1gb" shape="rect">
                                 <!-- --></a><ul class="ul" id="peer-to-peer-memory-copy__ul_uff_wxz_1gb">
                                 <li class="li">does not start until all commands previously issued to either device have
                                    completed and 
                                 </li>
                                 <li class="li">runs to completion before any commands (see <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>) issued after the copy to either
                                    device can start. 
                                 </li>
                              </ul>
                              <p class="p">Consistent with the normal behavior of streams, an asynchronous copy between the
                                 memories of two devices may overlap with copies or kernels in another stream.
                              </p>
                              <p class="p">Note that if peer-to-peer access is enabled between two devices via
                                 <samp class="ph codeph">cudaDeviceEnablePeerAccess()</samp> as described in <a class="xref" href="index.html#peer-to-peer-memory-access" shape="rect">Peer-to-Peer Memory Access</a>, peer-to-peer memory copy between these two
                                 devices no longer needs to be staged through the host and is therefore faster. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="unified-virtual-address-space"><a name="unified-virtual-address-space" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#unified-virtual-address-space" name="unified-virtual-address-space" shape="rect">3.2.8.&nbsp;Unified Virtual Address Space</a></h3>
                        <div class="body conbody">
                           <p class="p"> When the application is run as a 64-bit process, a single address space is used for
                              the host and all the devices of compute capability 2.0 and higher. All host memory
                              allocations made via CUDA API calls and all device memory allocations on supported
                              devices are within this virtual address range. As a consequence:
                           </p>
                           <ul class="ul">
                              <li class="li">The location of any memory on the host allocated through CUDA, or on any of the
                                 devices which use the unified address space, can be determined from the value of
                                 the pointer using <samp class="ph codeph">cudaPointerGetAttributes()</samp>.
                              </li>
                              <li class="li">When copying to or from the memory of any device which uses the unified address
                                 space, the <samp class="ph codeph">cudaMemcpyKind</samp> parameter of
                                 <samp class="ph codeph">cudaMemcpy*()</samp> can be set to
                                 <samp class="ph codeph">cudaMemcpyDefault</samp> to determine locations from the pointers.
                                 This also works for host pointers not allocated through CUDA, as long as the
                                 current device uses unified addressing.
                              </li>
                              <li class="li">Allocations via <samp class="ph codeph">cudaHostAlloc()</samp> are automatically portable (see
                                 <a class="xref" href="index.html#portable-memory" shape="rect">Portable Memory</a>) across all the devices for which the
                                 unified address space is used, and pointers returned by
                                 <samp class="ph codeph">cudaHostAlloc()</samp> can be used directly from within kernels
                                 running on these devices (i.e., there is no need to obtain a device pointer via
                                 <samp class="ph codeph">cudaHostGetDevicePointer()</samp> as described in <a class="xref" href="index.html#mapped-memory" shape="rect">Mapped Memory</a>.
                              </li>
                           </ul>
                           <p class="p">Applications may query if the unified address space is used for a particular device
                              by checking that the <samp class="ph codeph">unifiedAddressing</samp> device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>) is equal to 1.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="interprocess-communication"><a name="interprocess-communication" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#interprocess-communication" name="interprocess-communication" shape="rect">3.2.9.&nbsp;Interprocess Communication</a></h3>
                        <div class="body conbody">
                           <p class="p">Any device memory pointer or event handle created by a host thread can be directly
                              referenced by any other thread within the same process. It is not valid outside this
                              process however, and therefore cannot be directly referenced by threads belonging to
                              a different process.
                           </p>
                           <p class="p">To share device memory pointers and events across processes, an application must use
                              the Inter Process Communication API, which is described in detail in the reference
                              manual. The IPC API is only supported for 64-bit processes on Linux and for devices
                              of compute capability 2.0 and higher. Note that the IPC API is not supported for
                              <samp class="ph codeph">cudaMallocManaged</samp> allocations.
                           </p>
                           <p class="p">Using this API, an application can get the IPC handle for a given device memory
                              pointer using <samp class="ph codeph">cudaIpcGetMemHandle()</samp>, pass it to another process
                              using standard IPC mechanisms (e.g., interprocess shared memory or files), and use
                              <samp class="ph codeph">cudaIpcOpenMemHandle()</samp> to retrieve a device pointer from the
                              IPC handle that is a valid pointer within this other process. Event handles can be
                              shared using similar entry points.
                           </p>
                           <p class="p">Note that allocations made by <samp class="ph codeph">cudaMalloc()</samp> may be sub-allocated from
                              a larger block of memory for performance reasons. In such case, CUDA IPC APIs will
                              share the entire underlying memory block which may cause other sub-allocations to be
                              shared, which can potentially lead to information disclosure between processes. To
                              prevent this behavior, it is recommended to only share allocations with a 2MiB
                              aligned size.
                           </p>
                           <p class="p">An example of using the IPC API is where a single primary process generates a batch
                              of input data, making the data available to multiple secondary processes without
                              requiring regeneration or copying.
                           </p>
                           <p class="p">Applications using CUDA IPC to communicate with each other should be compiled, linked, and run with the same CUDA
                              driver and runtime.
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span>  CUDA IPC calls are not supported on Tegra devices. 
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="error-checking"><a name="error-checking" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#error-checking" name="error-checking" shape="rect">3.2.10.&nbsp;Error Checking</a></h3>
                        <div class="body conbody">
                           <p class="p"> All runtime functions return an error code, but for an asynchronous function (see
                              <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>), this error code cannot
                              possibly report any of the asynchronous errors that could occur on the device since
                              the function returns before the device has completed the task; the error code only
                              reports errors that occur on the host prior to executing the task, typically related
                              to parameter validation; if an asynchronous error occurs, it will be reported by
                              some subsequent unrelated runtime function call. 
                           </p>
                           <p class="p">The only way to check for asynchronous errors just after some asynchronous function
                              call is therefore to synchronize just after the call by calling
                              <samp class="ph codeph">cudaDeviceSynchronize()</samp> (or by using any other synchronization
                              mechanisms described in <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>) and
                              checking the error code returned by <samp class="ph codeph">cudaDeviceSynchronize()</samp>. 
                           </p>
                           <p class="p">The runtime maintains an error variable for each host thread that is initialized to
                              <samp class="ph codeph">cudaSuccess</samp> and is overwritten by the error code every time an
                              error occurs (be it a parameter validation error or an asynchronous error).
                              <samp class="ph codeph">cudaPeekAtLastError()</samp> returns this variable.
                              <samp class="ph codeph">cudaGetLastError()</samp> returns this variable and resets it to
                              <samp class="ph codeph">cudaSuccess</samp>. 
                           </p>
                           <p class="p"> Kernel launches do not return any error code, so
                              <samp class="ph codeph">cudaPeekAtLastError()</samp> or <samp class="ph codeph">cudaGetLastError()</samp>
                              must be called just after the kernel launch to retrieve any pre-launch errors. To
                              ensure that any error returned by <samp class="ph codeph">cudaPeekAtLastError()</samp> or
                              <samp class="ph codeph">cudaGetLastError()</samp> does not originate from calls prior to the
                              kernel launch, one has to make sure that the runtime error variable is set to
                              cudaSuccess just before the kernel launch, for example, by calling
                              <samp class="ph codeph">cudaGetLastError()</samp> just before the kernel launch. Kernel
                              launches are asynchronous, so to check for asynchronous errors, the application must
                              synchronize in-between the kernel launch and the call to
                              <samp class="ph codeph">cudaPeekAtLastError()</samp> or <samp class="ph codeph">cudaGetLastError()</samp>. 
                           </p>
                           <p class="p"> Note that <samp class="ph codeph">cudaErrorNotReady</samp> that may be returned by
                              <samp class="ph codeph">cudaStreamQuery()</samp> and <samp class="ph codeph">cudaEventQuery()</samp> is not
                              considered an error and is therefore not reported by
                              <samp class="ph codeph">cudaPeekAtLastError()</samp> or <samp class="ph codeph">cudaGetLastError()</samp>.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="call-stack"><a name="call-stack" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#call-stack" name="call-stack" shape="rect">3.2.11.&nbsp;Call Stack</a></h3>
                        <div class="body conbody">
                           <p class="p">On devices of compute capability 2.x and higher, the size of the call stack can be
                              queried using<samp class="ph codeph"> cudaDeviceGetLimit()</samp> and set using
                              <samp class="ph codeph">cudaDeviceSetLimit()</samp>. 
                           </p>
                           <p class="p">When the call stack overflows, the kernel call fails with a stack overflow error if
                              the application is run via a CUDA debugger (cuda-gdb, Nsight) or an unspecified
                              launch error, otherwise.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="texture-and-surface-memory"><a name="texture-and-surface-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#texture-and-surface-memory" name="texture-and-surface-memory" shape="rect">3.2.12.&nbsp;Texture and Surface Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">CUDA supports a subset of the texturing hardware that the GPU uses for graphics to
                              access texture and surface memory. Reading data from texture or surface memory
                              instead of global memory can have several performance benefits as described in <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>.
                           </p>
                           <p class="p">There are two different APIs to access texture and surface memory:</p>
                           <ul class="ul">
                              <li class="li">The texture reference API that is supported on all devices,</li>
                              <li class="li">The texture object API that is only supported on devices of compute capability
                                 3.x and higher.
                              </li>
                           </ul>
                           <p class="p">The texture reference API has limitations that the texture object API does not have.
                              They are mentioned in <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="texture-memory"><a name="texture-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texture-memory" name="texture-memory" shape="rect">3.2.12.1.&nbsp;Texture Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">Texture memory is read from kernels using the device functions described in <a class="xref" href="index.html#texture-functions" shape="rect">Texture Functions</a>. The process of reading a
                                 texture calling one of these functions is called a <dfn class="term">texture fetch</dfn>. Each
                                 texture fetch specifies a parameter called a <dfn class="term">texture object</dfn> for the
                                 texture object API or a <dfn class="term">texture reference</dfn> for the texture reference
                                 API.
                              </p>
                              <p class="p">The texture object or the texture reference specifies:</p>
                              <ul class="ul">
                                 <li class="li">The <dfn class="term">texture</dfn>, which is the piece of texture memory that is fetched.
                                    Texture objects are created at runtime and the texture is specified when
                                    creating the texture object as described in <a class="xref" href="index.html#texture-object-api" shape="rect">Texture Object API</a>.
                                    Texture references are created at compile time and the texture is specified at
                                    runtime by bounding the texture reference to the texture through runtime
                                    functions as described in <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>; several
                                    distinct texture references might be bound to the same texture or to textures
                                    that overlap in memory. A texture can be any region of linear memory or a CUDA
                                    array (described in <a class="xref" href="index.html#cuda-arrays" shape="rect">CUDA Arrays</a>).
                                 </li>
                                 <li class="li">Its <dfn class="term">dimensionality</dfn> that specifies whether the texture is addressed
                                    as a one dimensional array using one texture coordinate, a two-dimensional array
                                    using two texture coordinates, or a three-dimensional array using three texture
                                    coordinates. Elements of the array are called <dfn class="term">texels</dfn>, short for
                                    <dfn class="term">texture elements</dfn>. The <dfn class="term">texture width</dfn>,
                                    <dfn class="term">height</dfn>, and <dfn class="term">depth</dfn> refer to the size of the array
                                    in each dimension. <a class="xref" href="index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">Table 15</a> lists the maximum texture width, height, and depth depending on the compute
                                    capability of the device.
                                 </li>
                                 <li class="li">The type of a texel, which is restricted to the basic integer and
                                    single-precision floating-point types and any of the 1-, 2-, and 4-component
                                    vector types defined in <a class="xref" href="index.html#built-in-vector-types" shape="rect">Built-in Vector Types</a>
                                    that are derived from the basic integer and single-precision floating-point
                                    types.
                                 </li>
                                 <li class="li">The <dfn class="term">read mode</dfn>, which is equal to
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> or
                                    <samp class="ph codeph">cudaReadModeElementType</samp>. If it is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> and the type of the texel is a
                                    16-bit or 8-bit integer type, the value returned by the texture fetch is
                                    actually returned as floating-point type and the full range of the integer type
                                    is mapped to [0.0, 1.0] for unsigned integer type and [-1.0, 1.0] for signed
                                    integer type; for example, an unsigned 8-bit texture element with the value 0xff
                                    reads as 1. If it is <samp class="ph codeph">cudaReadModeElementType</samp>, no conversion is
                                    performed.
                                 </li>
                                 <li class="li">Whether texture coordinates are normalized or not. By default, textures are
                                    referenced (by the functions of <a class="xref" href="index.html#texture-functions" shape="rect">Texture Functions</a>) using floating-point
                                    coordinates in the range [0, N-1] where N is the size of the texture in the
                                    dimension corresponding to the coordinate. For example, a texture that is 64x32
                                    in size will be referenced with coordinates in the range [0, 63] and [0, 31] for
                                    the x and y dimensions, respectively. Normalized texture coordinates cause the
                                    coordinates to be specified in the range [0.0, 1.0-1/N] instead of [0, N-1], so
                                    the same 64x32 texture would be addressed by normalized coordinates in the range
                                    [0, 1-1/N] in both the x and y dimensions. Normalized texture coordinates are a
                                    natural fit to some applications' requirements, if it is preferable for the
                                    texture coordinates to be independent of the texture size.
                                 </li>
                                 <li class="li">The <dfn class="term">addressing mode</dfn>. It is valid to call the device functions of
                                    Section B.8 with coordinates that are out of range. The addressing mode defines
                                    what happens in that case. The default addressing mode is to clamp the
                                    coordinates to the valid range: [0, N) for non-normalized coordinates and [0.0,
                                    1.0) for normalized coordinates. If the border mode is specified instead,
                                    texture fetches with out-of-range texture coordinates return zero. For
                                    normalized coordinates, the wrap mode and the mirror mode are also available.
                                    When using the wrap mode, each coordinate x is converted to <dfn class="term">frac(x)=x
                                       floor(x)</dfn> where <dfn class="term">floor(x)</dfn> is the largest integer not
                                    greater than <em class="ph i">x</em>. When using the mirror mode, each coordinate <em class="ph i">x</em> is
                                    converted to <em class="ph i">frac(x)</em> if <em class="ph i">floor(x)</em> is even and <em class="ph i">1-frac(x)</em> if
                                    <em class="ph i">floor(x)</em> is odd. The addressing mode is specified as an array of size
                                    three whose first, second, and third elements specify the addressing mode for
                                    the first, second, and third texture coordinates, respectively; the addressing
                                    mode are <samp class="ph codeph">cudaAddressModeBorder</samp>,
                                    <samp class="ph codeph">cudaAddressModeClamp</samp>, <samp class="ph codeph">cudaAddressModeWrap</samp>,
                                    and <samp class="ph codeph">cudaAddressModeMirror</samp>; <samp class="ph codeph">cudaAddressModeWrap</samp>
                                    and <samp class="ph codeph">cudaAddressModeMirror</samp> are only supported for normalized
                                    texture coordinates 
                                 </li>
                                 <li class="li">The <dfn class="term">filtering</dfn> mode which specifies how the value returned when
                                    fetching the texture is computed based on the input texture coordinates. Linear
                                    texture filtering may be done only for textures that are configured to return
                                    floating-point data. It performs low-precision interpolation between neighboring
                                    texels. When enabled, the texels surrounding a texture fetch location are read
                                    and the return value of the texture fetch is interpolated based on where the
                                    texture coordinates fell between the texels. Simple linear interpolation is
                                    performed for one-dimensional textures, bilinear interpolation for
                                    two-dimensional textures, and trilinear interpolation for three-dimensional
                                    textures. <a class="xref" href="index.html#texture-fetching" shape="rect">Texture Fetching</a> gives more
                                    details on texture fetching. The filtering mode is equal to
                                    <samp class="ph codeph">cudaFilterModePoint</samp> or
                                    <samp class="ph codeph">cudaFilterModeLinear</samp>. If it is
                                    <samp class="ph codeph">cudaFilterModePoint</samp>, the returned value is the texel whose
                                    texture coordinates are the closest to the input texture coordinates. If it is
                                    <samp class="ph codeph">cudaFilterModeLinear</samp>, the returned value is the linear
                                    interpolation of the two (for a one-dimensional texture), four (for a two
                                    dimensional texture), or eight (for a three dimensional texture) texels whose
                                    texture coordinates are the closest to the input texture coordinates.
                                    <samp class="ph codeph">cudaFilterModeLinear</samp> is only valid for returned values of
                                    floating-point type. 
                                 </li>
                              </ul>
                              <p class="p"><a class="xref" href="index.html#texture-object-api" shape="rect">Texture Object API</a> introduces the texture object API.
                              </p>
                              <p class="p"><a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a> introduces the texture reference API.
                              </p>
                              <p class="p"><a class="xref" href="index.html#sixteen-bit-floating-point-textures" shape="rect">16-Bit Floating-Point Textures</a> explains how to deal with 16-bit
                                 floating-point textures.
                              </p>
                              <p class="p">Textures can also be layered as described in <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                              </p>
                              <p class="p"><a class="xref" href="index.html#cubemap-textures" shape="rect">Cubemap Textures</a> and <a class="xref" href="index.html#cubemap-layered-textures" shape="rect">Cubemap Layered Textures</a>
                                 describe a special type of texture, the cubemap texture.
                              </p>
                              <p class="p"><a class="xref" href="index.html#texture-gather" shape="rect">Texture Gather</a> describes a special texture fetch, texture gather.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="texture-object-api"><a name="texture-object-api" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#texture-object-api" name="texture-object-api" shape="rect">3.2.12.1.1.&nbsp;Texture Object API</a></h3>
                              <div class="body conbody">
                                 <p class="p">A texture object is created using <samp class="ph codeph">cudaCreateTextureObject()</samp> from a
                                    resource description of type struct <samp class="ph codeph">cudaResourceDesc</samp>, which
                                    specifies the texture, and from a texture description defined as such:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaTextureDesc
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureAddressMode addressMode[3];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureFilterMode  filterMode;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode    readMode;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>                         sRGB;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>                         normalizedCoords;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>                maxAnisotropy;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureFilterMode  mipmapFilterMode;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>                       mipmapLevelBias;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>                       minMipmapLevelClamp;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>                       maxMipmapLevelClamp;
};</pre><ul class="ul">
                                    <li class="li"><samp class="ph codeph">addressMode</samp> specifies the addressing mode;
                                    </li>
                                    <li class="li"><samp class="ph codeph">filterMode</samp> specifies the filter mode;
                                    </li>
                                    <li class="li"><samp class="ph codeph">readMode</samp> specifies the read mode;
                                    </li>
                                    <li class="li"><samp class="ph codeph">normalizedCoords</samp> specifies whether texture coordinates are
                                       normalized or not;
                                    </li>
                                    <li class="li">See reference manual for <samp class="ph codeph">sRGB</samp>, <samp class="ph codeph">maxAnisotropy</samp>,
                                       <samp class="ph codeph">mipmapFilterMode</samp>, <samp class="ph codeph">mipmapLevelBias</samp>,
                                       <samp class="ph codeph">minMipmapLevelClamp</samp>, and
                                       <samp class="ph codeph">maxMipmapLevelClamp</samp>.
                                    </li>
                                 </ul>
                                 <p class="p">The following code sample applies some simple transformation kernel to a texture.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Simple transformation kernel</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> transformKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* output,
                                cudaTextureObject_t texObj,
                                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height,
                                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> theta) 
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate normalized texture coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> u = x / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> v = y / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)height;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Transform coordinates</span>
    u -= 0.5f;
    v -= 0.5f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tv = v * cosf(theta) + u * sinf(theta) + 0.5f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Read from texture and write to global memory</span>
    output[y * width + x] = tex2D&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt;(texObj, tu, tv);
}</pre><p class="p"></p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height = 1024;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width = 1024;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> angle = 0.5;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate and set some host data</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> *h_data = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> *)std::malloc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>) * width * height);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; height * width; ++i)
        h_data[i] = i;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate CUDA array in device memory</span>
    cudaChannelFormatDesc channelDesc =
        cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat);
    cudaArray_t cuArray;
    cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set pitch of the source (the width in memory in bytes of the 2D array pointed</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// to by src, including padding), we dont have any padding</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> size_t spitch = width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy data located at address h_data in host memory to device memory</span>
    cudaMemcpy2DToArray(cuArray, 0, 0, h_data, spitch, width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>),
                        height, cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Specify texture</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaResourceDesc resDesc;
    memset(&amp;resDesc, 0, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(resDesc));
    resDesc.resType = cudaResourceTypeArray;
    resDesc.res.array.array = cuArray;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Specify texture object parameters</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaTextureDesc texDesc;
    memset(&amp;texDesc, 0, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(texDesc));
    texDesc.addressMode[0] = cudaAddressModeWrap;
    texDesc.addressMode[1] = cudaAddressModeWrap;
    texDesc.filterMode = cudaFilterModeLinear;
    texDesc.readMode = cudaReadModeElementType;
    texDesc.normalizedCoords = 1;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create texture object</span>
    cudaTextureObject_t texObj = 0;
    cudaCreateTextureObject(&amp;texObj, &amp;resDesc, &amp;texDesc, NULL);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate result of transformation in device memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> *output;
    cudaMalloc(&amp;output, width * height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> threadsperBlock(16, 16);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> numBlocks((width + threadsperBlock.x - 1) / threadsperBlock.x,
                    (height + threadsperBlock.y - 1) / threadsperBlock.y);
    transformKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>numBlocks, threadsperBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(output, texObj, width, height,
                                                    angle);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy data from device back to host</span>
    cudaMemcpy(h_data, output, width * height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>),
                cudaMemcpyDeviceToHost);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Destroy texture object</span>
    cudaDestroyTextureObject(texObj);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFreeArray(cuArray);
    cudaFree(output);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free host memory</span>
    free(h_data);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="texture-reference-api"><a name="texture-reference-api" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#texture-reference-api" name="texture-reference-api" shape="rect">3.2.12.1.2.&nbsp;[[DEPRECATED]] Texture Reference API</a></h3>
                              <div class="body conbody">
                                 <p class="p">Texture Reference API is deprecated.</p>
                                 <p class="p">Some of the attributes of a texture reference are immutable and must be known at
                                    compile time; they are specified when declaring the texture reference. A texture
                                    reference is declared at file scope as a variable of type
                                    <samp class="ph codeph">texture</samp>:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, Type, ReadMode&gt; texRef;</pre><p class="p">where:</p>
                                 <ul class="ul">
                                    <li class="li"><samp class="ph codeph">DataType</samp> specifies the type of the texel;
                                    </li>
                                    <li class="li"><samp class="ph codeph">Type</samp> specifies the type of the texture reference and is equal
                                       to <samp class="ph codeph">cudaTextureType1D</samp>, <samp class="ph codeph">cudaTextureType2D</samp>, or
                                       <samp class="ph codeph">cudaTextureType3D</samp>, for a one-dimensional, two-dimensional,
                                       or three-dimensional texture, respectively, or
                                       <samp class="ph codeph">cudaTextureType1DLayered</samp> or
                                       <samp class="ph codeph">cudaTextureType2DLayered</samp> for a one-dimensional or
                                       two-dimensional layered texture respectively; Type is an optional argument which
                                       defaults to <samp class="ph codeph">cudaTextureType1D</samp>;
                                    </li>
                                    <li class="li"><samp class="ph codeph">ReadMode</samp> specifies the read mode; it is an optional argument
                                       which defaults to <samp class="ph codeph">cudaReadModeElementType</samp>.
                                    </li>
                                 </ul>
                                 <p class="p">A texture reference can only be declared as a static global variable and cannot be
                                    passed as an argument to a function.
                                 </p>
                                 <p class="p">The other attributes of a texture reference are mutable and can be changed at runtime
                                    through the host runtime. As explained in the reference manual, the runtime API has
                                    a <em class="ph i">low-level</em> C-style interface and a <em class="ph i">high-level</em> C++-style interface.
                                    The <samp class="ph codeph">texture</samp> type is defined in the high-level API as a structure
                                    publicly derived from the <samp class="ph codeph">textureReference</samp> type defined in the
                                    low-level API as such:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>Reference {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>                          normalized;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureFilterMode   filterMode;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureAddressMode  addressMode[3];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaChannelFormatDesc channelDesc;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>                          sRGB;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>                 maxAnisotropy;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureFilterMode   mipmapFilterMode;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>                        mipmapLevelBias;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>                        minMipmapLevelClamp;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>                        maxMipmapLevelClamp;
}</pre><ul class="ul">
                                    <li class="li"><samp class="ph codeph">normalized</samp> specifies whether texture coordinates are normalized
                                       or not;
                                    </li>
                                    <li class="li"><samp class="ph codeph">filterMode</samp> specifies the filtering mode;
                                    </li>
                                    <li class="li"><samp class="ph codeph">addressMode</samp> specifies the addressing mode;
                                    </li>
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">channelDesc</samp> describes the format of the texel; it must match
                                          the <samp class="ph codeph">DataType</samp> argument of the texture reference declaration;
                                          <samp class="ph codeph">channelDesc</samp> is of the following type:
                                       </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaChannelFormatDesc {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, y, z, w;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaChannelFormatKind f;
};</pre><p class="p">where x, y, z, and w are equal to the number of bits of each component of the
                                          returned value and f is:
                                       </p>
                                       <ul class="ul">
                                          <li class="li"><samp class="ph codeph">cudaChannelFormatKindSigned</samp> if these components are of
                                             signed integer type,
                                          </li>
                                          <li class="li"><samp class="ph codeph">cudaChannelFormatKindUnsigned</samp> if they are of unsigned
                                             integer type,
                                          </li>
                                          <li class="li"><samp class="ph codeph">cudaChannelFormatKindFloat</samp> if they are of floating
                                             point type.
                                          </li>
                                       </ul>
                                    </li>
                                    <li class="li">See reference manual for <samp class="ph codeph">sRGB</samp>, <samp class="ph codeph">maxAnisotropy</samp>,
                                       <samp class="ph codeph">mipmapFilterMode</samp>, <samp class="ph codeph">mipmapLevelBias</samp>,
                                       <samp class="ph codeph">minMipmapLevelClamp</samp>, and
                                       <samp class="ph codeph">maxMipmapLevelClamp</samp>.
                                    </li>
                                 </ul>
                                 <p class="p"><samp class="ph codeph">normalized</samp>, <samp class="ph codeph">addressMode</samp>, and
                                    <samp class="ph codeph">filterMode</samp> may be directly modified in host code.
                                 </p>
                                 <p class="p">Before a kernel can use a texture reference to read from texture memory, the texture
                                    reference must be bound to a texture using <samp class="ph codeph">cudaBindTexture()</samp> or
                                    <samp class="ph codeph">cudaBindTexture2D()</samp> for linear memory, or
                                    <samp class="ph codeph">cudaBindTextureToArray()</samp> for CUDA arrays.
                                    <samp class="ph codeph">cudaUnbindTexture()</samp> is used to unbind a texture reference. Once
                                    a texture reference has been unbound, it can be safely rebound to another array,
                                    even if kernels that use the previously bound texture have not completed. It is
                                    recommended to allocate two-dimensional textures in linear memory using
                                    <samp class="ph codeph">cudaMallocPitch()</samp> and use the pitch returned by
                                    <samp class="ph codeph">cudaMallocPitch()</samp> as input parameter to
                                    <samp class="ph codeph">cudaBindTexture2D()</samp>.
                                 </p>
                                 <p class="p">The following code samples bind a 2D texture reference to linear memory pointed to by
                                    <samp class="ph codeph">devPtr</samp>:
                                 </p>
                                 <ul class="ul">
                                    <li class="li">
                                       <p class="p">Using the low-level API:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>, cudaTextureType2D,
        cudaReadModeElementType&gt; texRef;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>Reference* texRefPtr;
cudaGetTextureReference(&amp;texRefPtr, &amp;texRef);
cudaChannelFormatDesc channelDesc =
                             cudaCreateChannelDesc&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt;();
size_t offset;
cudaBindTexture2D(&amp;offset, texRefPtr, devPtr, &amp;channelDesc,
                  width, height, pitch);</pre></li>
                                    <li class="li">
                                       <p class="p">Using the high-level API:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>, cudaTextureType2D,
        cudaReadModeElementType&gt; texRef;
cudaChannelFormatDesc channelDesc =
                             cudaCreateChannelDesc&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt;();
size_t offset;
cudaBindTexture2D(&amp;offset, texRef, devPtr, channelDesc,
                  width, height, pitch);</pre></li>
                                 </ul>
                                 <p class="p">The following code samples bind a 2D texture reference to a CUDA array
                                    <samp class="ph codeph">cuArray</samp>:
                                 </p>
                                 <ul class="ul">
                                    <li class="li">
                                       <p class="p">Using the low-level API:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>, cudaTextureType2D,
        cudaReadModeElementType&gt; texRef;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>Reference* texRefPtr;
cudaGetTextureReference(&amp;texRefPtr, &amp;texRef);
cudaChannelFormatDesc channelDesc;
cudaGetChannelDesc(&amp;channelDesc, cuArray);
cudaBindTextureToArray(texRef, cuArray, &amp;channelDesc);</pre></li>
                                    <li class="li">
                                       <p class="p">Using the high-level API:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>, cudaTextureType2D,
        cudaReadModeElementType&gt; texRef;
cudaBindTextureToArray(texRef, cuArray);</pre></li>
                                 </ul>
                                 <p class="p">The format specified when binding a texture to a texture reference must match the
                                    parameters specified when declaring the texture reference; otherwise, the results of
                                    texture fetches are undefined.
                                 </p>
                                 <p class="p">There is a limit to the number of textures that can be bound to a kernel as specified
                                    in <a class="xref" href="index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">Table 15</a>.
                                 </p>
                                 <p class="p">The following code sample applies some simple transformation kernel to a texture.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 2D float texture</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Simple transformation kernel</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> transformKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* output,
                                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height,
                                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> theta) 
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate normalized texture coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> u = x / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> v = y / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)height;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Transform coordinates</span>
    u -= 0.5f;
    v -= 0.5f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tv = v * cosf(theta) + u * sinf(theta) + 0.5f;


    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Read from texture and write to global memory</span>
    output[y * width + x] = tex2D(texRef, tu, tv);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate CUDA array in device memory</span>
    cudaChannelFormatDesc channelDesc =
               cudaCreateChannelDesc(32, 0, 0, 0,
                                     cudaChannelFormatKindFloat);
    cudaArray* cuArray;
    cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy to device memory some data located at address h_data</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// in host memory </span>
    cudaMemcpyToArray(cuArray, 0, 0, h_data, size,
                      cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set texture reference parameters</span>
    texRef.addressMode[0] = cudaAddressModeWrap;
    texRef.addressMode[1] = cudaAddressModeWrap;
    texRef.filterMode     = cudaFilterModeLinear;
    texRef.normalized     = true;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Bind the array to the texture reference</span>
    cudaBindTextureToArray(texRef, cuArray, channelDesc);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate result of transformation in device memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* output;
    cudaMalloc(&amp;output, width * height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(16, 16);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid((width  + dimBlock.x - 1) / dimBlock.x,
                 (height + dimBlock.y - 1) / dimBlock.y);
    transformKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(output, width, height,
                                           angle);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFreeArray(cuArray);
    cudaFree(output);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="sixteen-bit-floating-point-textures"><a name="sixteen-bit-floating-point-textures" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#sixteen-bit-floating-point-textures" name="sixteen-bit-floating-point-textures" shape="rect">3.2.12.1.3.&nbsp;16-Bit Floating-Point Textures</a></h3>
                              <div class="body conbody">
                                 <p class="p"> The 16-bit floating-point or <dfn class="term">half</dfn> format supported by CUDA arrays is
                                    the same as the IEEE 754-2008 binary2 format. 
                                 </p>
                                 <p class="p">CUDA C++ does not support a matching data type, but provides intrinsic functions to
                                    convert to and from the 32-bit floating-point format via the <samp class="ph codeph">unsigned
                                       short</samp> type: <samp class="ph codeph">__float2half_rn(float)</samp> and
                                    <samp class="ph codeph">__half2float(unsigned short)</samp>. These functions are only
                                    supported in device code. Equivalent functions for the host code can be found in the
                                    OpenEXR library, for example. 
                                 </p>
                                 <p class="p">16-bit floating-point components are promoted to 32 bit float during texture fetching
                                    before any filtering is performed.
                                 </p>
                                 <p class="p"> A channel description for the 16-bit floating-point format can be created by calling
                                    one of the <samp class="ph codeph">cudaCreateChannelDescHalf*()</samp> functions. 
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="layered-textures"><a name="layered-textures" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#layered-textures" name="layered-textures" shape="rect">3.2.12.1.4.&nbsp;Layered Textures</a></h3>
                              <div class="body conbody">
                                 <p class="p"> A one-dimensional or two-dimensional layered texture (also known as <dfn class="term">texture
                                       array</dfn> in Direct3D and <dfn class="term">array texture</dfn> in OpenGL) is a texture
                                    made up of a sequence of layers, all of which are regular textures of same
                                    dimensionality, size, and data type. 
                                 </p>
                                 <p class="p">A one-dimensional layered texture is addressed using an integer index and a
                                    floating-point texture coordinate; the index denotes a layer within the sequence and
                                    the coordinate addresses a texel within that layer. A two-dimensional layered
                                    texture is addressed using an integer index and two floating-point texture
                                    coordinates; the index denotes a layer within the sequence and the coordinates
                                    address a texel within that layer.
                                 </p>
                                 <p class="p">A layered texture can only be a CUDA array by calling
                                    <samp class="ph codeph">cudaMalloc3DArray()</samp> with the <samp class="ph codeph">cudaArrayLayered</samp>
                                    flag (and a height of zero for one-dimensional layered texture). 
                                 </p>
                                 <p class="p">Layered textures are fetched using the device functions described in <a class="xref" href="index.html#tex1dlayered" shape="rect">tex1DLayered()</a>, <a class="xref" href="index.html#tex1dlayered-object" shape="rect">tex1DLayered()</a>, <a class="xref" href="index.html#tex2dlayered" shape="rect">tex2DLayered()</a>, and <a class="xref" href="index.html#tex2dlayered-object" shape="rect">tex2DLayered()</a>. Texture filtering (see
                                    <a class="xref" href="index.html#texture-fetching" shape="rect">Texture Fetching</a>) is done only within a
                                    layer, not across layers.
                                 </p>
                                 <p class="p">Layered textures are only supported on devices of compute capability 2.0 and
                                    higher.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="cubemap-textures"><a name="cubemap-textures" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#cubemap-textures" name="cubemap-textures" shape="rect">3.2.12.1.5.&nbsp;Cubemap Textures</a></h3>
                              <div class="body conbody">
                                 <p class="p"> A <dfn class="term">cubemap</dfn> texture is a special type of two-dimensional layered texture
                                    that has six layers representing the faces of a cube:
                                 </p>
                                 <ul class="ul">
                                    <li class="li">The width of a layer is equal to its height.</li>
                                    <li class="li"> The cubemap is addressed using three texture coordinates <em class="ph i">x</em>, <em class="ph i">y</em>,
                                       and <em class="ph i">z</em> that are interpreted as a direction vector emanating from the
                                       center of the cube and pointing to one face of the cube and a texel within the
                                       layer corresponding to that face. More specifically, the face is selected by the
                                       coordinate with largest magnitude <em class="ph i">m</em> and the corresponding layer is
                                       addressed using coordinates <em class="ph i">(s/m+1)/2</em> and <em class="ph i">(t/m+1)/2</em> where <em class="ph i">s</em>
                                       and <em class="ph i">t</em> are defined in <a class="xref" href="index.html#cubemap-textures__cubemap-fetch" shape="rect">Table 2</a>.
                                       
                                    </li>
                                 </ul>
                                 <div class="tablenoborder"><a name="cubemap-textures__cubemap-fetch" shape="rect">
                                       <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="cubemap-textures__cubemap-fetch" class="table" frame="border" border="1" rules="all">
                                       <caption><span class="tablecap">Table 2. Cubemap Fetch</span></caption>
                                       <thead class="thead" align="left">
                                          <tr class="row">
                                             <th class="entry" colspan="2" align="center" valign="top" id="d225e4776" rowspan="1">&nbsp;</th>
                                             <th class="entry" align="center" valign="top" width="15.384615384615385%" id="d225e4778" rowspan="1" colspan="1">face</th>
                                             <th class="entry" align="center" valign="top" width="7.6923076923076925%" id="d225e4781" rowspan="1" colspan="1">m</th>
                                             <th class="entry" align="center" valign="top" width="7.6923076923076925%" id="d225e4784" rowspan="1" colspan="1">s</th>
                                             <th class="entry" align="center" valign="top" width="7.6923076923076925%" id="d225e4787" rowspan="1" colspan="1">t</th>
                                          </tr>
                                       </thead>
                                       <tbody class="tbody">
                                          <tr class="row">
                                             <td class="entry" rowspan="2" align="center" valign="middle" width="46.15384615384615%" headers="d225e4776" colspan="1">|x| &gt; |y| and |x| &gt;
                                                |z|
                                             </td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4776" rowspan="1" colspan="1">x <u class="ph u">&gt;</u> 0
                                             </td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4778" rowspan="1" colspan="1">0</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4781" rowspan="1" colspan="1">x</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4784" rowspan="1" colspan="1">-z</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4787" rowspan="1" colspan="1">-y</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4776" rowspan="1" colspan="1">x &lt; 0</td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4778" rowspan="1" colspan="1">1</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4781" rowspan="1" colspan="1">-x</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4784" rowspan="1" colspan="1">z</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4787" rowspan="1" colspan="1">-y</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" rowspan="2" align="center" valign="middle" width="46.15384615384615%" headers="d225e4776" colspan="1">|y| &gt; |x| and |y| &gt;
                                                |z|
                                             </td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4776" rowspan="1" colspan="1">y <u class="ph u">&gt;</u> 0
                                             </td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4778" rowspan="1" colspan="1">2</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4781" rowspan="1" colspan="1">y</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4784" rowspan="1" colspan="1">x</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4787" rowspan="1" colspan="1">z</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4776" rowspan="1" colspan="1">y &lt; 0</td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4778" rowspan="1" colspan="1">3</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4781" rowspan="1" colspan="1">-y</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4784" rowspan="1" colspan="1">x</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4787" rowspan="1" colspan="1">-z</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" rowspan="2" align="center" valign="middle" width="46.15384615384615%" headers="d225e4776" colspan="1">|z| &gt; |x| and |z| &gt;
                                                |y|
                                             </td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4776" rowspan="1" colspan="1">z <u class="ph u">&gt;</u> 0
                                             </td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4778" rowspan="1" colspan="1">4</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4781" rowspan="1" colspan="1">z</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4784" rowspan="1" colspan="1">x</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4787" rowspan="1" colspan="1">-y</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4776" rowspan="1" colspan="1">z &lt; 0</td>
                                             <td class="entry" align="center" valign="top" width="15.384615384615385%" headers="d225e4778" rowspan="1" colspan="1">5</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4781" rowspan="1" colspan="1">-z</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4784" rowspan="1" colspan="1">-x</td>
                                             <td class="entry" align="center" valign="top" width="7.6923076923076925%" headers="d225e4787" rowspan="1" colspan="1">-y</td>
                                          </tr>
                                       </tbody>
                                    </table>
                                 </div>
                                 <p class="p">A cubemap texture can only be a CUDA array by calling
                                    <samp class="ph codeph">cudaMalloc3DArray()</samp> with the <samp class="ph codeph">cudaArrayCubemap</samp>
                                    flag. 
                                 </p>
                                 <p class="p"> Cubemap textures are fetched using the device function described in <a class="xref" href="index.html#texcubemap" shape="rect">texCubemap()</a> and <a class="xref" href="index.html#texcubemap-object" shape="rect">texCubemap()</a>. 
                                 </p>
                                 <p class="p">Cubemap textures are only supported on devices of compute capability 2.0 and
                                    higher.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="cubemap-layered-textures"><a name="cubemap-layered-textures" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#cubemap-layered-textures" name="cubemap-layered-textures" shape="rect">3.2.12.1.6.&nbsp;Cubemap Layered Textures</a></h3>
                              <div class="body conbody">
                                 <p class="p"> A <dfn class="term">cubemap layered</dfn> texture is a layered texture whose layers are
                                    cubemaps of same dimension. 
                                 </p>
                                 <p class="p">A cubemap layered texture is addressed using an integer index and three
                                    floating-point texture coordinates; the index denotes a cubemap within the sequence
                                    and the coordinates address a texel within that cubemap.
                                 </p>
                                 <p class="p"> A cubemap layered texture can only be a CUDA array by calling
                                    <samp class="ph codeph">cudaMalloc3DArray()</samp> with the <samp class="ph codeph">cudaArrayLayered</samp>
                                    and <samp class="ph codeph">cudaArrayCubemap</samp> flags. 
                                 </p>
                                 <p class="p"> Cubemap layered textures are fetched using the device function described in <a class="xref" href="index.html#texcubemaplayered" shape="rect">texCubemapLayered()</a> and <a class="xref" href="index.html#texcubemaplayered-object" shape="rect">texCubemapLayered()</a>. Texture filtering
                                    (see <a class="xref" href="index.html#texture-fetching" shape="rect">Texture Fetching</a>) is done only within a
                                    layer, not across layers. 
                                 </p>
                                 <p class="p">Cubemap layered textures are only supported on devices of compute capability 2.0 and
                                    higher.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="texture-gather"><a name="texture-gather" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#texture-gather" name="texture-gather" shape="rect">3.2.12.1.7.&nbsp;Texture Gather</a></h3>
                              <div class="body conbody">
                                 <p class="p">Texture gather is a special texture fetch that is available for two-dimensional
                                    textures only. It is performed by the <samp class="ph codeph">tex2Dgather()</samp> function, which
                                    has the same parameters as <samp class="ph codeph">tex2D()</samp>, plus an additional
                                    <samp class="ph codeph">comp</samp> parameter equal to 0, 1, 2, or 3 (see <a class="xref" href="index.html#tex2dgather" shape="rect">tex2Dgather()</a> and <a class="xref" href="index.html#tex2dgather-object" shape="rect">tex2Dgather()</a>). It returns four 32-bit
                                    numbers that correspond to the value of the component <samp class="ph codeph">comp</samp> of each
                                    of the four texels that would have been used for bilinear filtering during a regular
                                    texture fetch. For example, if these texels are of values (253, 20, 31, 255), (250,
                                    25, 29, 254), (249, 16, 37, 253), (251, 22, 30, 250), and <samp class="ph codeph">comp</samp> is
                                    2, <samp class="ph codeph">tex2Dgather()</samp> returns (31, 29, 37, 30).
                                 </p>
                                 <p class="p">Note that texture coordinates are computed with only 8 bits of fractional precision.
                                    <samp class="ph codeph">tex2Dgather()</samp> may therefore return unexpected results for cases
                                    where <samp class="ph codeph">tex2D()</samp> would use 1.0 for one of its weights ( or , see
                                    <a class="xref" href="index.html#linear-filtering" shape="rect">Linear Filtering</a>). For example, with an
                                    <em class="ph i">x</em> texture coordinate of 2.49805: <em class="ph i">x<sub class="ph sub">B</sub>=x-0.5=1.99805</em>,
                                    however the fractional part of <em class="ph i">x<sub class="ph sub">B</sub></em> is stored in an 8-bit
                                    fixed-point format. Since 0.99805 is closer to 256.f/256.f than it is to
                                    255.f/256.f, <em class="ph i">x<sub class="ph sub">B</sub></em> has the value 2. A <samp class="ph codeph">tex2Dgather()</samp>
                                    in this case would therefore return indices 2 and 3 in <em class="ph i">x</em>, instead of indices
                                    1 and 2.
                                 </p>
                                 <p class="p">Texture gather is only supported for CUDA arrays created with the
                                    <samp class="ph codeph">cudaArrayTextureGather</samp> flag and of width and height less than
                                    the maximum specified in <a class="xref" href="index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">Table 15</a> for texture gather, which is smaller than for regular texture fetch.
                                 </p>
                                 <p class="p">Texture gather is only supported on devices of compute capability 2.0 and higher.</p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="surface-memory"><a name="surface-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surface-memory" name="surface-memory" shape="rect">3.2.12.2.&nbsp;Surface Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">For devices of compute capability 2.0 and higher, a CUDA array (described in <a class="xref" href="index.html#cubemap-surfaces" shape="rect">Cubemap Surfaces</a>), created with the
                                 <samp class="ph codeph">cudaArraySurfaceLoadStore</samp> flag, can be read and written via a
                                 <dfn class="term">surface object</dfn> or <dfn class="term">surface reference</dfn> using the
                                 functions described in <a class="xref" href="index.html#surface-functions" shape="rect">Surface Functions</a>.
                              </p>
                              <p class="p"><a class="xref" href="index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">Table 15</a> lists the maximum surface width, height, and depth depending on the compute
                                 capability of the device.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="surface-object-api"><a name="surface-object-api" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#surface-object-api" name="surface-object-api" shape="rect">3.2.12.2.1.&nbsp;Surface Object API</a></h3>
                              <div class="body conbody">
                                 <p class="p">A surface object is created using <samp class="ph codeph">cudaCreateSurfaceObject()</samp> from a
                                    resource description of type <samp class="ph codeph">struct cudaResourceDesc</samp>.
                                 </p>
                                 <p class="p">The following code sample applies some simple transformation kernel to a texture.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Simple copy kernel</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> copyKernel(cudaSurfaceObject_t inputSurfObj,
                           cudaSurfaceObject_t outputSurfObj,
                           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height) 
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate surface coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) {
        uchar4 data;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Read from input surface</span>
        surf2Dread(&amp;data,  inputSurfObj, x * 4, y);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write to output surface</span>
        surf2Dwrite(data, outputSurfObj, x * 4, y);
    }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height = 1024;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width = 1024;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate and set some host data</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *h_data =
        (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *)std::malloc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>) * width * height * 4);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; height * width * 4; ++i)
        h_data[i] = i;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate CUDA arrays in device memory</span>
    cudaChannelFormatDesc channelDesc =
        cudaCreateChannelDesc(8, 8, 8, 8, cudaChannelFormatKindUnsigned);
    cudaArray_t cuInputArray;
    cudaMallocArray(&amp;cuInputArray, &amp;channelDesc, width, height,
                    cudaArraySurfaceLoadStore);
    cudaArray_t cuOutputArray;
    cudaMallocArray(&amp;cuOutputArray, &amp;channelDesc, width, height,
                    cudaArraySurfaceLoadStore);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set pitch of the source (the width in memory in bytes of the 2D array</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// pointed to by src, including padding), we dont have any padding</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> size_t spitch = 4 * width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy data located at address h_data in host memory to device memory</span>
    cudaMemcpy2DToArray(cuInputArray, 0, 0, h_data, spitch,
                        4 * width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>), height,
                        cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Specify surface</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaResourceDesc resDesc;
    memset(&amp;resDesc, 0, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(resDesc));
    resDesc.resType = cudaResourceTypeArray;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create the surface objects</span>
    resDesc.res.array.array = cuInputArray;
    cudaSurfaceObject_t inputSurfObj = 0;
    cudaCreateSurfaceObject(&amp;inputSurfObj, &amp;resDesc);
    resDesc.res.array.array = cuOutputArray;
    cudaSurfaceObject_t outputSurfObj = 0;
    cudaCreateSurfaceObject(&amp;outputSurfObj, &amp;resDesc);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> threadsperBlock(16, 16);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> numBlocks((width + threadsperBlock.x - 1) / threadsperBlock.x,
                    (height + threadsperBlock.y - 1) / threadsperBlock.y);
    copyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>numBlocks, threadsperBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(inputSurfObj, outputSurfObj, width,
                                                height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy data from device back to host</span>
    cudaMemcpy2DFromArray(h_data, spitch, cuOutputArray, 0, 0,
                            4 * width * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>), height,
                            cudaMemcpyDeviceToHost);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Destroy surface objects</span>
    cudaDestroySurfaceObject(inputSurfObj);
    cudaDestroySurfaceObject(outputSurfObj);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFreeArray(cuInputArray);
    cudaFreeArray(cuOutputArray);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free host memory</span>
    free(h_data);

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="surface-reference-api"><a name="surface-reference-api" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#surface-reference-api" name="surface-reference-api" shape="rect">3.2.12.2.2.&nbsp;[[DEPRECATED]] Surface Reference API</a></h3>
                              <div class="body conbody">
                                 <p class="p">Surface Reference API is deprecated.</p>
                                 <p class="p">A surface reference is declared at file scope as a variable of type surface:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, Type&gt; surfRef;</pre><p class="p">where <samp class="ph codeph">Type</samp> specifies the type of the surface reference and is equal
                                    to <samp class="ph codeph">cudaSurfaceType1D</samp>, <samp class="ph codeph">cudaSurfaceType2D</samp>,
                                    <samp class="ph codeph">cudaSurfaceType3D</samp>, <samp class="ph codeph">cudaSurfaceTypeCubemap</samp>,
                                    <samp class="ph codeph">cudaSurfaceType1DLayered</samp>,
                                    <samp class="ph codeph">cudaSurfaceType2DLayered</samp>, or
                                    <samp class="ph codeph">cudaSurfaceTypeCubemapLayered</samp>; <samp class="ph codeph">Type</samp> is an
                                    optional argument which defaults to cudaSurfaceType1D. A surface reference can only
                                    be declared as a static global variable and cannot be passed as an argument to a
                                    function.
                                 </p>
                                 <p class="p">Before a kernel can use a surface reference to access a CUDA array, the surface
                                    reference must be bound to the CUDA array using
                                    <samp class="ph codeph">cudaBindSurfaceToArray()</samp>.
                                 </p>
                                 <p class="p">The following code samples bind a surface reference to a CUDA array
                                    <samp class="ph codeph">cuArray</samp>:
                                 </p>
                                 <ul class="ul">
                                    <li class="li">
                                       <p class="p">Using the low-level API:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2D&gt; surfRef;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>Reference* surfRefPtr;
cudaGetSurfaceReference(&amp;surfRefPtr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"surfRef"</span>);
cudaChannelFormatDesc channelDesc;
cudaGetChannelDesc(&amp;channelDesc, cuArray);
cudaBindSurfaceToArray(surfRef, cuArray, &amp;channelDesc);</pre></li>
                                    <li class="li">
                                       <p class="p">Using the high-level API:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2D&gt; surfRef;
cudaBindSurfaceToArray(surfRef, cuArray);</pre></li>
                                 </ul>
                                 <p class="p">A CUDA array must be read and written using surface functions of matching
                                    dimensionality and type and via a surface reference of matching dimensionality;
                                    otherwise, the results of reading and writing the CUDA array are undefined.
                                 </p>
                                 <p class="p">Unlike texture memory, surface memory uses byte addressing. This means that the
                                    x-coordinate used to access a texture element via texture functions needs to be
                                    multiplied by the byte size of the element to access the same element via a surface
                                    function. For example, the element at texture coordinate x of a one-dimensional
                                    floating-point CUDA array bound to a texture reference <samp class="ph codeph">texRef</samp> and a
                                    surface reference <samp class="ph codeph">surfRef</samp> is read using<samp class="ph codeph"> tex1d(texRef,
                                       x)</samp> via <samp class="ph codeph">texRef</samp>, but <samp class="ph codeph">surf1Dread(surfRef,
                                       4*x)</samp> via <samp class="ph codeph">surfRef</samp>. Similarly, the element at texture
                                    coordinate <em class="ph i">x</em> and <em class="ph i">y</em> of a two-dimensional floating-point CUDA array
                                    bound to a texture reference <samp class="ph codeph">texRef</samp> and a surface reference
                                    <samp class="ph codeph">surfRef</samp> is accessed using <samp class="ph codeph">tex2d(texRef, x, y)</samp>
                                    via <samp class="ph codeph">texRef</samp>, but <samp class="ph codeph">surf2Dread(surfRef, 4*x, y)</samp> via
                                    <samp class="ph codeph">surfRef</samp> (the byte offset of the y-coordinate is internally
                                    calculated from the underlying line pitch of the CUDA array).
                                 </p>
                                 <p class="p">The following code sample applies some simple transformation kernel to a texture.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 2D surfaces</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, 2&gt; inputSurfRef;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, 2&gt; outputSurfRef;
            
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Simple copy kernel</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> copyKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height) 
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate surface coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) {
        uchar4 data;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Read from input surface</span>
        surf2Dread(&amp;data,  inputSurfRef, x * 4, y);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write to output surface</span>
        surf2Dwrite(data, outputSurfRef, x * 4, y);
    }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate CUDA arrays in device memory</span>
    cudaChannelFormatDesc channelDesc =
             cudaCreateChannelDesc(8, 8, 8, 8,
                                   cudaChannelFormatKindUnsigned);
    cudaArray* cuInputArray;
    cudaMallocArray(&amp;cuInputArray, &amp;channelDesc, width, height,
                    cudaArraySurfaceLoadStore);
    cudaArray* cuOutputArray;
    cudaMallocArray(&amp;cuOutputArray, &amp;channelDesc, width, height,
                    cudaArraySurfaceLoadStore);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy to device memory some data located at address h_data</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// in host memory </span>
    cudaMemcpyToArray(cuInputArray, 0, 0, h_data, size,
                      cudaMemcpyHostToDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Bind the arrays to the surface references</span>
    cudaBindSurfaceToArray(inputSurfRef, cuInputArray);
    cudaBindSurfaceToArray(outputSurfRef, cuOutputArray);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(16, 16);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid((width  + dimBlock.x - 1) / dimBlock.x,
                 (height + dimBlock.y - 1) / dimBlock.y);
    copyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(width, height);


    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free device memory</span>
    cudaFreeArray(cuInputArray);
    cudaFreeArray(cuOutputArray);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="cubemap-surfaces"><a name="cubemap-surfaces" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#cubemap-surfaces" name="cubemap-surfaces" shape="rect">3.2.12.2.3.&nbsp;Cubemap Surfaces</a></h3>
                              <div class="body conbody">
                                 <p class="p"> Cubemap surfaces are accessed using<samp class="ph codeph">surfCubemapread()</samp> and
                                    <samp class="ph codeph">surfCubemapwrite()</samp> (<a class="xref" href="index.html#surfcubemapread" shape="rect">surfCubemapread</a> and
                                    <a class="xref" href="index.html#surfcubemapwrite" shape="rect">surfCubemapwrite</a>) as a two-dimensional layered surface, i.e., using an
                                    integer index denoting a face and two floating-point texture coordinates addressing
                                    a texel within the layer corresponding to this face. Faces are ordered as indicated
                                    in <a class="xref" href="index.html#cubemap-textures__cubemap-fetch" shape="rect">Table 2</a>. 
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="cubemap-layered-surfaces"><a name="cubemap-layered-surfaces" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#cubemap-layered-surfaces" name="cubemap-layered-surfaces" shape="rect">3.2.12.2.4.&nbsp;Cubemap Layered Surfaces</a></h3>
                              <div class="body conbody">
                                 <p class="p"> Cubemap layered surfaces are accessed using
                                    <samp class="ph codeph">surfCubemapLayeredread()</samp> and
                                    <samp class="ph codeph">surfCubemapLayeredwrite()</samp> (<a class="xref" href="index.html#surfcubemaplayeredread" shape="rect">surfCubemapLayeredread()</a> and <a class="xref" href="index.html#surfcubemaplayeredwrite" shape="rect">surfCubemapLayeredwrite()</a>) as a
                                    two-dimensional layered surface, i.e., using an integer index denoting a face of one
                                    of the cubemaps and two floating-point texture coordinates addressing a texel within
                                    the layer corresponding to this face. Faces are ordered as indicated in <a class="xref" href="index.html#cubemap-textures__cubemap-fetch" shape="rect">Table 2</a>, so index ((2 * 6) + 3), for example,
                                    accesses the fourth face of the third cubemap. 
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="cuda-arrays"><a name="cuda-arrays" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#cuda-arrays" name="cuda-arrays" shape="rect">3.2.12.3.&nbsp;CUDA Arrays</a></h3>
                           <div class="body conbody">
                              <p class="p">CUDA arrays are opaque memory layouts optimized for texture fetching. They are one
                                 dimensional, two dimensional, or three-dimensional and composed of elements, each of
                                 which has 1, 2 or 4 components that may be signed or unsigned 8-, 16-, or 32-bit
                                 integers, 16-bit floats, or 32-bit floats. CUDA arrays are only accessible by
                                 kernels through texture fetching as described in <a class="xref" href="index.html#texture-memory" shape="rect">Texture Memory</a> or
                                 surface reading and writing as described in <a class="xref" href="index.html#surface-memory" shape="rect">Surface Memory</a>.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="read-write-coherency"><a name="read-write-coherency" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#read-write-coherency" name="read-write-coherency" shape="rect">3.2.12.4.&nbsp;Read/Write Coherency</a></h3>
                           <div class="body conbody">
                              <p class="p"> The texture and surface memory is cached (see <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>) and within the same
                                 kernel call, the cache is not kept coherent with respect to global memory writes and
                                 surface memory writes, so any texture fetch or surface read to an address that has
                                 been written to via a global write or a surface write in the same kernel call
                                 returns undefined data. In other words, a thread can safely read some texture or
                                 surface memory location only if this memory location has been updated by a previous
                                 kernel call or memory copy, but not if it has been previously updated by the same
                                 thread or another thread from the same kernel call. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="graphics-interoperability"><a name="graphics-interoperability" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#graphics-interoperability" name="graphics-interoperability" shape="rect">3.2.13.&nbsp;Graphics Interoperability</a></h3>
                        <div class="body conbody">
                           <p class="p">Some resources from OpenGL and Direct3D may be mapped into the address space of CUDA,
                              either to enable CUDA to read data written by OpenGL or Direct3D, or to enable CUDA
                              to write data for consumption by OpenGL or Direct3D. 
                           </p>
                           <p class="p"> A resource must be registered to CUDA before it can be mapped using the functions
                              mentioned in <a class="xref" href="index.html#opengl-interoperability" shape="rect">OpenGL Interoperability</a> and <a class="xref" href="index.html#direct3d-interoperability" shape="rect">Direct3D Interoperability</a>. These functions return a pointer to a CUDA
                              graphics resource of type <samp class="ph codeph">struct cudaGraphicsResource</samp>. Registering
                              a resource is potentially high-overhead and therefore typically called only once per
                              resource. A CUDA graphics resource is unregistered using
                              <samp class="ph codeph">cudaGraphicsUnregisterResource()</samp>. Each CUDA context which
                              intends to use the resource is required to register it separately. 
                           </p>
                           <p class="p">Once a resource is registered to CUDA, it can be mapped and unmapped as many times as
                              necessary using <samp class="ph codeph">cudaGraphicsMapResources()</samp> and
                              <samp class="ph codeph">cudaGraphicsUnmapResources()</samp>.
                              <samp class="ph codeph">cudaGraphicsResourceSetMapFlags()</samp> can be called to specify
                              usage hints (write-only, read-only) that the CUDA driver can use to optimize
                              resource management. 
                           </p>
                           <p class="p">A mapped resource can be read from or written to by kernels using the device memory
                              address returned by <samp class="ph codeph">cudaGraphicsResourceGetMappedPointer()</samp> for
                              buffers and<samp class="ph codeph"> cudaGraphicsSubResourceGetMappedArray()</samp> for CUDA
                              arrays. 
                           </p>
                           <p class="p">Accessing a resource through OpenGL, Direct3D, or another CUDA context while it is
                              mapped produces undefined results. <a class="xref" href="index.html#opengl-interoperability" shape="rect">OpenGL Interoperability</a> and <a class="xref" href="index.html#direct3d-interoperability" shape="rect">Direct3D Interoperability</a> give specifics for each graphics API and
                              some code samples. <a class="xref" href="index.html#sli-interoperability" shape="rect">SLI Interoperability</a> gives specifics for when the
                              system is in SLI mode. 
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="opengl-interoperability"><a name="opengl-interoperability" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#opengl-interoperability" name="opengl-interoperability" shape="rect">3.2.13.1.&nbsp;OpenGL Interoperability</a></h3>
                           <div class="body conbody">
                              <p class="p">The OpenGL resources that may be mapped into the address space of CUDA are OpenGL
                                 buffer, texture, and renderbuffer objects.
                              </p>
                              <p class="p"> A buffer object is registered using <samp class="ph codeph">cudaGraphicsGLRegisterBuffer()</samp>.
                                 In CUDA, it appears as a device pointer and can therefore be read and written by
                                 kernels or via <samp class="ph codeph">cudaMemcpy()</samp> calls. 
                              </p>
                              <p class="p">A texture or renderbuffer object is registered using
                                 <samp class="ph codeph">cudaGraphicsGLRegisterImage()</samp>. In CUDA, it appears as a CUDA
                                 array. Kernels can read from the array by binding it to a texture or surface
                                 reference. They can also write to it via the surface write functions if the resource
                                 has been registered with the
                                 <samp class="ph codeph">cudaGraphicsRegisterFlagsSurfaceLoadStore</samp> flag. The array can
                                 also be read and written via <samp class="ph codeph">cudaMemcpy2D()</samp> calls.
                                 <samp class="ph codeph">cudaGraphicsGLRegisterImage()</samp> supports all texture formats with
                                 1, 2, or 4 components and an internal type of float (e.g.,
                                 <samp class="ph codeph">GL_RGBA_FLOAT32</samp>), normalized integer (e.g., <samp class="ph codeph">GL_RGBA8,
                                    GL_INTENSITY16</samp>), and unnormalized integer (e.g.,
                                 <samp class="ph codeph">GL_RGBA8UI</samp>) (please note that since unnormalized integer
                                 formats require OpenGL 3.0, they can only be written by shaders, not the fixed
                                 function pipeline). 
                              </p>
                              <p class="p">The OpenGL context whose resources are being shared has to be current to the host
                                 thread making any OpenGL interoperability API calls.
                              </p>
                              <p class="p">Please note: When an OpenGL texture is made bindless (say for example by requesting
                                 an image or texture handle using the glGetTextureHandle*/glGetImageHandle* APIs) it
                                 cannot be registered with CUDA. The application needs to register the texture for
                                 interop before requesting an image or texture handle.
                              </p>
                              <p class="p"> The following code sample uses a kernel to dynamically modify a 2D
                                 <samp class="ph codeph">width</samp> x <samp class="ph codeph">height</samp> grid of vertices stored in a
                                 vertex buffer object: 
                              </p><pre xml:space="preserve">GLuint positionsVBO;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaGraphicsResource* positionsVBO_CUDA;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize OpenGL and GLUT for device 0</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// and make the OpenGL context current</span>
    ...
    glutDisplayFunc(display);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Explicitly set device 0</span>
    cudaSetDevice(0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create buffer object and register it with CUDA</span>
    glGenBuffers(1, &amp;positionsVBO);
    glBindBuffer(GL_ARRAY_BUFFER, positionsVBO);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> size = width * height * 4 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);
    glBufferData(GL_ARRAY_BUFFER, size, 0, GL_DYNAMIC_DRAW);
    glBindBuffer(GL_ARRAY_BUFFER, 0);
    cudaGraphicsGLRegisterBuffer(&amp;positionsVBO_CUDA,
                                 positionsVBO,
                                 cudaGraphicsMapFlagsWriteDiscard);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch rendering loop</span>
    glutMainLoop();

    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> display()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Map buffer object for writing from CUDA</span>
    float4* positions;
    cudaGraphicsMapResources(1, &amp;positionsVBO_CUDA, 0);
    size_t num_bytes; 
    cudaGraphicsResourceGetMappedPointer((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>**)&amp;positions,
                                         &amp;num_bytes,  
                                         positionsVBO_CUDA));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Execute kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(16, 16, 1);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(positions, time,
                                          width, height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Unmap buffer object</span>
    cudaGraphicsUnmapResources(1, &amp;positionsVBO_CUDA, 0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Render from buffer object</span>
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
    glBindBuffer(GL_ARRAY_BUFFER, positionsVBO);
    glVertexPointer(4, GL_FLOAT, 0, 0);
    glEnableClientState(GL_VERTEX_ARRAY);
    glDrawArrays(GL_POINTS, 0, width * height);
    glDisableClientState(GL_VERTEX_ARRAY);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Swap buffers</span>
    glutSwapBuffers();
    glutPostRedisplay();
}</pre><p class="p"></p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> deleteVBO()
{
    cudaGraphicsUnregisterResource(positionsVBO_CUDA);
    glDeleteBuffers(1, &amp;positionsVBO);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> createVertices(float4* positions, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> time,
                               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate uv coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> u = x / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> v = y / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// calculate simple sine wave pattern</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> freq = 4.0f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write positions</span>
    positions[y * width + x] = make_float4(u, w, v, 1.0f);
}</pre><p class="p"> On Windows and for Quadro GPUs, <samp class="ph codeph">cudaWGLGetDevice()</samp> can be used to
                                 retrieve the CUDA device associated to the handle returned by
                                 <samp class="ph codeph">wglEnumGpusNV()</samp>. Quadro GPUs offer higher performance OpenGL
                                 interoperability than GeForce and Tesla GPUs in a multi-GPU configuration where
                                 OpenGL rendering is performed on the Quadro GPU and CUDA computations are performed
                                 on other GPUs in the system. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="direct3d-interoperability"><a name="direct3d-interoperability" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#direct3d-interoperability" name="direct3d-interoperability" shape="rect">3.2.13.2.&nbsp;Direct3D Interoperability</a></h3>
                           <div class="body conbody">
                              <p class="p">Direct3D interoperability is supported for Direct3D 9Ex, Direct3D 10, and Direct3D
                                 11.
                              </p>
                              <p class="p">A CUDA context may interoperate only with Direct3D devices that fulfill the following
                                 criteria: Direct3D 9Ex devices must be created with <samp class="ph codeph">DeviceType</samp> set
                                 to <samp class="ph codeph">D3DDEVTYPE_HAL</samp> and <samp class="ph codeph">BehaviorFlags</samp> with the
                                 <samp class="ph codeph">D3DCREATE_HARDWARE_VERTEXPROCESSING</samp> flag; Direct3D 10 and
                                 Direct3D 11 devices must be created with <samp class="ph codeph">DriverType</samp> set to
                                 <samp class="ph codeph">D3D_DRIVER_TYPE_HARDWARE</samp>.
                              </p>
                              <p class="p">The Direct3D resources that may be mapped into the address space of CUDA are Direct3D
                                 buffers, textures, and surfaces. These resources are registered using
                                 <samp class="ph codeph">cudaGraphicsD3D9RegisterResource()</samp>,
                                 <samp class="ph codeph">cudaGraphicsD3D10RegisterResource()</samp>, and
                                 <samp class="ph codeph">cudaGraphicsD3D11RegisterResource()</samp>. 
                              </p>
                              <p class="p"> The following code sample uses a kernel to dynamically modify a 2D
                                 <samp class="ph codeph">width</samp> x <samp class="ph codeph">height</samp> grid of vertices stored in a
                                 vertex buffer object. 
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="direct3d-9-version"><a name="direct3d-9-version" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#direct3d-9-version" name="direct3d-9-version" shape="rect">3.2.13.2.1.&nbsp;Direct3D 9 Version</a></h3>
                              <div class="body conbody"><pre xml:space="preserve">IDirect3D9* D3D;
IDirect3DDevice9* device;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> CUSTOMVERTEX {
    FLOAT x, y, z;
    DWORD color;
};
IDirect3DVertexBuffer9* positionsVB;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaGraphicsResource* positionsVB_CUDA;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> dev;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize Direct3D</span>
    D3D = Direct3DCreate9Ex(D3D_SDK_VERSION);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get a CUDA-enabled adapter</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> adapter = 0;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (; adapter &lt; g_pD3D-&gt;GetAdapterCount(); adapter++) {
        D3DADAPTER_IDENTIFIER9 adapterId;
        g_pD3D-&gt;GetAdapterIdentifier(adapter, 0, &amp;adapterId);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaD3D9GetDevice(&amp;dev, adapterId.DeviceName)
            == cudaSuccess)
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">break</span>;
    }

     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create device</span>
    ...
    D3D-&gt;CreateDeviceEx(adapter, D3DDEVTYPE_HAL, hWnd,
                        D3DCREATE_HARDWARE_VERTEXPROCESSING,
                        &amp;params, NULL, &amp;device);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Use the same device</span>
    cudaSetDevice(dev);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create vertex buffer and register it with CUDA</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> size = width * height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(CUSTOMVERTEX);
    device-&gt;CreateVertexBuffer(size, 0, D3DFVF_CUSTOMVERTEX,
                               D3DPOOL_DEFAULT, &amp;positionsVB, 0);
    cudaGraphicsD3D9RegisterResource(&amp;positionsVB_CUDA,
                                     positionsVB,
                                     cudaGraphicsRegisterFlagsNone);
    cudaGraphicsResourceSetMapFlags(positionsVB_CUDA,
                                    cudaGraphicsMapFlagsWriteDiscard);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch rendering loop</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (...) {
        ...
        Render();
        ...
    }
    ...
}
</pre><p class="p"></p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Render()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Map vertex buffer for writing from CUDA</span>
    float4* positions;
    cudaGraphicsMapResources(1, &amp;positionsVB_CUDA, 0);
    size_t num_bytes; 
    cudaGraphicsResourceGetMappedPointer((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>**)&amp;positions,
                                         &amp;num_bytes,  
                                         positionsVB_CUDA));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Execute kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(16, 16, 1);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(positions, time,
                                          width, height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Unmap vertex buffer</span>
    cudaGraphicsUnmapResources(1, &amp;positionsVB_CUDA, 0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Draw and present</span>
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> releaseVB()
{
    cudaGraphicsUnregisterResource(positionsVB_CUDA);
    positionsVB-&gt;Release();
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> createVertices(float4* positions, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> time,
                               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate uv coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> u = x / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> v = y / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate simple sine wave pattern</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> freq = 4.0f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write positions</span>
    positions[y * width + x] =
                make_float4(u, w, v, __int_as_float(0xff00ff00));
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="direct3d-10-version"><a name="direct3d-10-version" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#direct3d-10-version" name="direct3d-10-version" shape="rect">3.2.13.2.2.&nbsp;Direct3D 10 Version</a></h3>
                              <div class="body conbody"><pre xml:space="preserve">ID3D10Device* device;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> CUSTOMVERTEX {
    FLOAT x, y, z;
    DWORD color;
};
ID3D10Buffer* positionsVB;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaGraphicsResource* positionsVB_CUDA;
            
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> dev;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get a CUDA-enabled adapter</span>
    IDXGIFactory* factory;
    CreateDXGIFactory(__uuidof(IDXGIFactory), (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>**)&amp;factory);
    IDXGIAdapter* adapter = 0;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; !adapter; ++i) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (FAILED(factory-&gt;EnumAdapters(i, &amp;adapter))
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">break</span>;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaD3D10GetDevice(&amp;dev, adapter) == cudaSuccess)
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">break</span>;
        adapter-&gt;Release();
    }
    factory-&gt;Release();
            
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create swap chain and device</span>
    ...
    D3D10CreateDeviceAndSwapChain(adapter, 
                                  D3D10_DRIVER_TYPE_HARDWARE, 0, 
                                  D3D10_CREATE_DEVICE_DEBUG,
                                  D3D10_SDK_VERSION, 
                                  &amp;swapChainDesc, &amp;swapChain,
                                  &amp;device);
    adapter-&gt;Release();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Use the same device</span>
    cudaSetDevice(dev);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create vertex buffer and register it with CUDA</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> size = width * height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(CUSTOMVERTEX);
    D3D10_BUFFER_DESC bufferDesc;
    bufferDesc.Usage          = D3D10_USAGE_DEFAULT;
    bufferDesc.ByteWidth      = size;
    bufferDesc.BindFlags      = D3D10_BIND_VERTEX_BUFFER;
    bufferDesc.CPUAccessFlags = 0;
    bufferDesc.MiscFlags      = 0;
    device-&gt;CreateBuffer(&amp;bufferDesc, 0, &amp;positionsVB);
    cudaGraphicsD3D10RegisterResource(&amp;positionsVB_CUDA,
                                      positionsVB,
                                      cudaGraphicsRegisterFlagsNone);
                                      cudaGraphicsResourceSetMapFlags(positionsVB_CUDA,
                                      cudaGraphicsMapFlagsWriteDiscard);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch rendering loop</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (...) {
        ...
        Render();
        ...
    }
    ...
}</pre><p class="p"></p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Render()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Map vertex buffer for writing from CUDA</span>
    float4* positions;
    cudaGraphicsMapResources(1, &amp;positionsVB_CUDA, 0);
    size_t num_bytes; 
    cudaGraphicsResourceGetMappedPointer((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>**)&amp;positions,
                                         &amp;num_bytes,  
                                         positionsVB_CUDA));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Execute kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(16, 16, 1);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(positions, time,
                                          width, height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Unmap vertex buffer</span>
    cudaGraphicsUnmapResources(1, &amp;positionsVB_CUDA, 0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Draw and present</span>
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> releaseVB()
{
    cudaGraphicsUnregisterResource(positionsVB_CUDA);
    positionsVB-&gt;Release();
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> createVertices(float4* positions, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> time,
                               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate uv coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> u = x / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> v = y / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate simple sine wave pattern</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> freq = 4.0f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;
            
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write positions</span>
    positions[y * width + x] =
                make_float4(u, w, v, __int_as_float(0xff00ff00));
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="direct3d-11-version"><a name="direct3d-11-version" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#direct3d-11-version" name="direct3d-11-version" shape="rect">3.2.13.2.3.&nbsp;Direct3D 11 Version</a></h3>
                              <div class="body conbody"><pre xml:space="preserve">ID3D11Device* device;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> CUSTOMVERTEX {
    FLOAT x, y, z;
    DWORD color;
};
ID3D11Buffer* positionsVB;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cudaGraphicsResource* positionsVB_CUDA;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> dev;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get a CUDA-enabled adapter</span>
    IDXGIFactory* factory;
    CreateDXGIFactory(__uuidof(IDXGIFactory), (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>**)&amp;factory);
    IDXGIAdapter* adapter = 0;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; !adapter; ++i) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (FAILED(factory-&gt;EnumAdapters(i, &amp;adapter))
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">break</span>;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaD3D11GetDevice(&amp;dev, adapter) == cudaSuccess)
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">break</span>;
        adapter-&gt;Release();
    }
    factory-&gt;Release();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create swap chain and device</span>
    ...
    sFnPtr_D3D11CreateDeviceAndSwapChain(adapter, 
                                         D3D11_DRIVER_TYPE_HARDWARE,
                                         0, 
                                         D3D11_CREATE_DEVICE_DEBUG,
                                         featureLevels, 3,
                                         D3D11_SDK_VERSION, 
                                         &amp;swapChainDesc, &amp;swapChain,
                                         &amp;device,
                                         &amp;featureLevel,
                                         &amp;deviceContext);
    adapter-&gt;Release();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Use the same device</span>
    cudaSetDevice(dev);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create vertex buffer and register it with CUDA</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> size = width * height * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(CUSTOMVERTEX);
    D3D11_BUFFER_DESC bufferDesc;
    bufferDesc.Usage          = D3D11_USAGE_DEFAULT;
    bufferDesc.ByteWidth      = size;
    bufferDesc.BindFlags      = D3D11_BIND_VERTEX_BUFFER;
    bufferDesc.CPUAccessFlags = 0;
    bufferDesc.MiscFlags      = 0;
    device-&gt;CreateBuffer(&amp;bufferDesc, 0, &amp;positionsVB);
    cudaGraphicsD3D11RegisterResource(&amp;positionsVB_CUDA,
                                      positionsVB,
                                      cudaGraphicsRegisterFlagsNone);
    cudaGraphicsResourceSetMapFlags(positionsVB_CUDA,
                                    cudaGraphicsMapFlagsWriteDiscard);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch rendering loop</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (...) {
        ...
        Render();
        ...
    }
    ...
}</pre><p class="p"></p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Render()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Map vertex buffer for writing from CUDA</span>
    float4* positions;
    cudaGraphicsMapResources(1, &amp;positionsVB_CUDA, 0);
    size_t num_bytes; 
    cudaGraphicsResourceGetMappedPointer((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>**)&amp;positions,
                                         &amp;num_bytes,  
                                         positionsVB_CUDA));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Execute kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(16, 16, 1);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>dimGrid, dimBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(positions, time,
                                          width, height);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Unmap vertex buffer</span>
    cudaGraphicsUnmapResources(1, &amp;positionsVB_CUDA, 0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Draw and present</span>
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> releaseVB()
{
    cudaGraphicsUnregisterResource(positionsVB_CUDA);
    positionsVB-&gt;Release();
}

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> createVertices(float4* positions, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> time,
                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> height)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.y * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.y + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate uv coordinates</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> u = x / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)width;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> v = y / (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Calculate simple sine wave pattern</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> freq = 4.0f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write positions</span>
    positions[y * width + x] =
                make_float4(u, w, v, __int_as_float(0xff00ff00));
}</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="sli-interoperability"><a name="sli-interoperability" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#sli-interoperability" name="sli-interoperability" shape="rect">3.2.13.3.&nbsp;SLI Interoperability</a></h3>
                           <div class="body conbody">
                              <p class="p">In a system with multiple GPUs, all CUDA-enabled GPUs are accessible via the CUDA
                                 driver and runtime as separate devices. There are however special considerations as
                                 described below when the system is in SLI mode.
                              </p>
                              <p class="p">First, an allocation in one CUDA device on one GPU will consume memory on other GPUs
                                 that are part of the SLI configuration of the Direct3D or OpenGL device. Because of
                                 this, allocations may fail earlier than otherwise expected.
                              </p>
                              <p class="p">Second, applications should create multiple CUDA contexts, one for each GPU in the
                                 SLI configuration. While this is not a strict requirement, it avoids unnecessary
                                 data transfers between devices. The application can use the
                                 <samp class="ph codeph">cudaD3D[9|10|11]GetDevices()</samp> for Direct3D and
                                 <samp class="ph codeph">cudaGLGetDevices()</samp> for OpenGL set of calls to identify the CUDA
                                 device handle(s) for the device(s) that are performing the rendering in the current
                                 and next frame. Given this information the application will typically choose the
                                 appropriate device and map Direct3D or OpenGL resources to the CUDA device returned
                                 by <samp class="ph codeph">cudaD3D[9|10|11]GetDevices()</samp> or
                                 <samp class="ph codeph">cudaGLGetDevices()</samp> when the <samp class="ph codeph">deviceList</samp>
                                 parameter is set to <samp class="ph codeph">cudaD3D[9|10|11]DeviceListCurrentFrame</samp> or
                                 <samp class="ph codeph">cudaGLDeviceListCurrentFrame</samp>.
                              </p>
                              <p class="p">Please note that resource returned from
                                 <samp class="ph codeph">cudaGraphicsD9D[9|10|11]RegisterResource</samp> and
                                 <samp class="ph codeph">cudaGraphicsGLRegister[Buffer|Image]</samp> must be only used on
                                 device the registration happened. Therefore on SLI configurations when data for
                                 different frames is computed on different CUDA devices it is necessary to register
                                 the resources for each separatly.
                              </p>
                              <p class="p"> See <a class="xref" href="index.html#direct3d-interoperability" shape="rect">Direct3D Interoperability</a> and <a class="xref" href="index.html#opengl-interoperability" shape="rect">OpenGL Interoperability</a> for details on how the CUDA runtime
                                 interoperate with Direct3D and OpenGL, respectively. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" id="external-resource-interoperability"><a name="external-resource-interoperability" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#external-resource-interoperability" name="external-resource-interoperability" shape="rect">3.2.14.&nbsp;External Resource Interoperability</a></h3>
                        <div class="body conbody">
                           <p class="p">External resource interoperability allows CUDA to import certain resources that are
                              explicitly exported by other APIs. These objects are typically exported by other
                              APIs using handles native to the Operating System, like file descriptors on Linux or
                              NT handles on Windows. They could also be exported using other unified interfaces
                              such as the NVIDIA Software Communication Interface. There are two types of
                              resources that can be imported: memory objects and synchronization objects.
                           </p>
                           <p class="p">Memory objects can be imported into CUDA using
                              <samp class="ph codeph">cudaImportExternalMemory()</samp>. An imported memory object can be
                              accessed from within kernels using device pointers mapped onto the memory object via
                              <samp class="ph codeph">cudaExternalMemoryGetMappedBuffer()</samp>or CUDA mipmapped arrays
                              mapped via <samp class="ph codeph">cudaExternalMemoryGetMappedMipmappedArray()</samp>. Depending
                              on the type of memory object, it may be possible for more than one mapping to be
                              setup on a single memory object. The mappings must match the mappings setup in the
                              exporting API. Any mismatched mappings result in undefined behavior. Imported memory
                              objects must be freed using <samp class="ph codeph">cudaDestroyExternalMemory()</samp>. Freeing a
                              memory object does not free any mappings to that object. Therefore, any device
                              pointers mapped onto that object must be explicitly freed using
                              <samp class="ph codeph">cudaFree()</samp> and any CUDA mipmapped arrays mapped onto that
                              object must be explicitly freed using <samp class="ph codeph">cudaFreeMipmappedArray()</samp>. It
                              is illegal to access mappings to an object after it has been destroyed.
                           </p>
                           <p class="p">Synchronization objects can be imported into CUDA using
                              <samp class="ph codeph">cudaImportExternalSemaphore()</samp>. An imported synchronization
                              object can then be signaled using
                              <samp class="ph codeph">cudaSignalExternalSemaphoresAsync()</samp> and waited on using
                              <samp class="ph codeph">cudaWaitExternalSemaphoresAsync()</samp>. It is illegal to issue a
                              wait before the corresponding signal has been issued. Also, depending on the type of
                              the imported synchronization object, there may be additional constraints imposed on
                              how they can be signaled and waited on, as described in subsequent sections.
                              Imported semaphore objects must be freed using
                              <samp class="ph codeph">cudaDestroyExternalSemaphore()</samp>. All outstanding signals and
                              waits must have completed before the semaphore object is destroyed.
                           </p>
                        </div>
                        <div class="topic concept nested3" id="vulkan-interoperability"><a name="vulkan-interoperability" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#vulkan-interoperability" name="vulkan-interoperability" shape="rect">3.2.14.1.&nbsp;Vulkan Interoperability</a></h3>
                           <div class="topic concept nested4" id="matching-device-uuids-vul-int"><a name="matching-device-uuids-vul-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#matching-device-uuids-vul-int" name="matching-device-uuids-vul-int" shape="rect">3.2.14.1.1.&nbsp;Matching device UUIDs</a></h3>
                              <div class="body conbody">
                                 <p class="p">When importing memory and synchronization objects exported by Vulkan, they must be
                                    imported and mapped on the same device as they were created on. The CUDA device that
                                    corresponds to the Vulkan physical device on which the objects were created can be
                                    determined by comparing the UUID of a CUDA device with that of the Vulkan physical
                                    device, as shown in the following code sample. Note that the Vulkan physical device
                                    should not be part of a device group that contains more than one Vulkan physical
                                    device. The device group as returned by vkEnumeratePhysicalDeviceGroups that
                                    contains the given Vulkan physical device must have a physical device count of
                                    1.
                                 </p><pre xml:space="preserve">
int getCudaDeviceForVulkanPhysicalDevice(VkPhysicalDevice vkPhysicalDevice) {
    VkPhysicalDeviceIDProperties vkPhysicalDeviceIDProperties = {};
    vkPhysicalDeviceIDProperties.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_ID_PROPERTIES;
    vkPhysicalDeviceIDProperties.pNext = NULL;

    VkPhysicalDeviceProperties2 vkPhysicalDeviceProperties2 = {};
    vkPhysicalDeviceProperties2.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2;
    vkPhysicalDeviceProperties2.pNext = &amp;vkPhysicalDeviceIDProperties;

    vkGetPhysicalDeviceProperties2(vkPhysicalDevice, &amp;vkPhysicalDeviceProperties2);

    int cudaDeviceCount;
    cudaGetDeviceCount(&amp;cudaDeviceCount);

    for (int cudaDevice = 0; cudaDevice &lt; cudaDeviceCount; cudaDevice++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&amp;deviceProp, cudaDevice);
        if (!memcmp(&amp;deviceProp.uuid, vkPhysicalDeviceIDProperties.deviceUUID, VK_UUID_SIZE)) {
            return cudaDevice;
        }
    }
    return cudaInvalidDeviceId;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-memory-objects-vul-int"><a name="importing-memory-objects-vul-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-memory-objects-vul-int" name="importing-memory-objects-vul-int" shape="rect">3.2.14.1.2.&nbsp;Importing memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">On Linux and Windows 10, both dedicated and non-dedicated memory objects exported by
                                    Vulkan can be imported into CUDA. On Windows 7, only dedicated memory objects can be
                                    imported. When importing a Vulkan dedicated memory object, the flag
                                    <samp class="ph codeph">cudaExternalMemoryDedicated</samp> must be set.
                                 </p>
                                 <p class="p">A Vulkan memory object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT</samp> can be imported
                                    into CUDA using the file descriptor associated with that object as shown below. Note
                                    that CUDA assumes ownership of the file descriptor once it is imported. Using the
                                    file descriptor after a successful import results in undefined behavior.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importVulkanMemoryObjectFromFileDescriptor(int fd, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueFd;
    desc.handle.fd = fd;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    // Input parameter 'fd' should not be used beyond this point as CUDA has assumed ownership of it

    return extMem;
}
</pre><p class="p">A Vulkan memory object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT</samp> can be imported
                                    into CUDA using the NT handle associated with that object as shown below. Note that
                                    CUDA does not assume ownership of the NT handle and it is the applications
                                    responsibility to close the handle when it is not required anymore. The NT handle
                                    holds a reference to the resource, so it must be explicitly freed before the
                                    underlying memory can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importVulkanMemoryObjectFromNTHandle(HANDLE handle, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;
    desc.handle.win32.handle = handle;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}
</pre><p class="p">A Vulkan memory object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT</samp> can also be
                                    imported using a named handle if one exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importVulkanMemoryObjectFromNamedNTHandle(LPCWSTR name, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;
    desc.handle.win32.name = (void *)name;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    return extMem;
}
</pre><p class="p">A Vulkan memory object exported using
                                    VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT can be imported into CUDA using
                                    the globally shared D3DKMT handle associated with that object as shown below. Since
                                    a globally shared D3DKMT handle does not hold a reference to the underlying memory
                                    it is automatically destroyed when all other references to the resource are
                                    destroyed.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importVulkanMemoryObjectFromKMTHandle(HANDLE handle, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueWin32Kmt;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    return extMem;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-buffers-onto-imported-memory-objects-vul-int"><a name="mapping-buffers-onto-imported-memory-objects-vul-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-buffers-onto-imported-memory-objects-vul-int" name="mapping-buffers-onto-imported-memory-objects-vul-int" shape="rect">3.2.14.1.3.&nbsp;Mapping buffers onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A device pointer can be mapped onto an imported memory object as shown below. The
                                    offset and size of the mapping must match that specified when creating the mapping
                                    using the corresponding Vulkan API. All mapped device pointers must be freed using
                                    <samp class="ph codeph">cudaFree()</samp>.
                                 </p><pre xml:space="preserve">
void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {

    void *ptr = NULL;

    cudaExternalMemoryBufferDesc desc = {};

 

    memset(&amp;desc, 0, sizeof(desc));

 

    desc.offset = offset;

    desc.size = size;

 

    cudaExternalMemoryGetMappedBuffer(&amp;ptr, extMem, &amp;desc);

 

    // Note: ptr must eventually be freed using cudaFree()
        
    return ptr;

}</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-mipmapped-arrays-onto-imported-memory-objects-vul-int"><a name="mapping-mipmapped-arrays-onto-imported-memory-objects-vul-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-vul-int" name="mapping-mipmapped-arrays-onto-imported-memory-objects-vul-int" shape="rect">3.2.14.1.4.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A CUDA mipmapped array can be mapped onto an imported memory object as shown below.
                                    The offset, dimensions, format and number of mip levels must match that specified
                                    when creating the mapping using the corresponding Vulkan API. Additionally, if the
                                    mipmapped array is bound as a color target in Vulkan, the flag<samp class="ph codeph">
                                       cudaArrayColorAttachment</samp> must be set. All mapped mipmapped arrays must
                                    be freed using <samp class="ph codeph">cudaFreeMipmappedArray()</samp>. The following code sample
                                    shows how to convert Vulkan parameters into the corresponding CUDA parameters when
                                    mapping mipmapped arrays onto imported memory objects.
                                 </p><pre xml:space="preserve">
cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: mipmap must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&amp;mipmap, extMem, &amp;desc);

    return mipmap;
}

cudaChannelFormatDesc getCudaChannelFormatDescForVulkanFormat(VkFormat format)
{
    cudaChannelFormatDesc d;

    memset(&amp;d, 0, sizeof(d));

    switch (format) {
    case VK_FORMAT_R8_UINT:             d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R8_SINT:             d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R8G8_UINT:           d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R8G8_SINT:           d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R8G8B8A8_UINT:       d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R8G8B8A8_SINT:       d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R16_UINT:            d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R16_SINT:            d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R16G16_UINT:         d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R16G16_SINT:         d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R16G16B16A16_UINT:   d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R16G16B16A16_SINT:   d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32_UINT:            d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R32_SINT:            d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32_SFLOAT:          d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case VK_FORMAT_R32G32_UINT:         d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R32G32_SINT:         d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32G32_SFLOAT:       d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case VK_FORMAT_R32G32B32A32_UINT:   d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R32G32B32A32_SINT:   d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32G32B32A32_SFLOAT: d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindFloat;    break;
    default: assert(0);
    }</pre><p class="p"></p><pre xml:space="preserve">

    return d;
}

cudaExtent getCudaExtentForVulkanExtent(VkExtent3D vkExt, uint32_t arrayLayers, VkImageViewType vkImageViewType) {
    cudaExtent e = { 0, 0, 0 };
 
    switch (vkImageViewType) {
    case VK_IMAGE_VIEW_TYPE_1D:         e.width = vkExt.width; e.height = 0;            e.depth = 0;           break;
    case VK_IMAGE_VIEW_TYPE_2D:         e.width = vkExt.width; e.height = vkExt.height; e.depth = 0;           break;
    case VK_IMAGE_VIEW_TYPE_3D:         e.width = vkExt.width; e.height = vkExt.height; e.depth = vkExt.depth; break;
    case VK_IMAGE_VIEW_TYPE_CUBE:       e.width = vkExt.width; e.height = vkExt.height; e.depth = arrayLayers; break;
    case VK_IMAGE_VIEW_TYPE_1D_ARRAY:   e.width = vkExt.width; e.height = 0;            e.depth = arrayLayers; break;
    case VK_IMAGE_VIEW_TYPE_2D_ARRAY:   e.width = vkExt.width; e.height = vkExt.height; e.depth = arrayLayers; break;
    case VK_IMAGE_VIEW_TYPE_CUBE_ARRAY: e.width = vkExt.width; e.height = vkExt.height; e.depth = arrayLayers; break;
    default: assert(0);
    }

    return e;
}

unsigned int getCudaMipmappedArrayFlagsForVulkanImage(VkImageViewType vkImageViewType, VkImageUsageFlags vkImageUsageFlags, bool allowSurfaceLoadStore) {
    unsigned int flags = 0;

    switch (vkImageViewType) {
    case VK_IMAGE_VIEW_TYPE_CUBE:       flags |= cudaArrayCubemap;                    break;
    case VK_IMAGE_VIEW_TYPE_CUBE_ARRAY: flags |= cudaArrayCubemap | cudaArrayLayered; break;
    case VK_IMAGE_VIEW_TYPE_1D_ARRAY:   flags |= cudaArrayLayered;                    break;
    case VK_IMAGE_VIEW_TYPE_2D_ARRAY:   flags |= cudaArrayLayered;                    break;
    default: break;
    }

    if (vkImageUsageFlags &amp; VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT) {
        flags |= cudaArrayColorAttachment;
    }

    if (allowSurfaceLoadStore) {
        flags |= cudaArraySurfaceLoadStore;
    }
    return flags;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-synchronization-objects-vul-int"><a name="importing-synchronization-objects-vul-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-synchronization-objects-vul-int" name="importing-synchronization-objects-vul-int" shape="rect">3.2.14.1.5.&nbsp;Importing synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A Vulkan semaphore object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT</samp>can be imported
                                    into CUDA using the file descriptor associated with that object as shown below. Note
                                    that CUDA assumes ownership of the file descriptor once it is imported. Using the
                                    file descriptor after a successful import results in undefined behavior.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importVulkanSemaphoreObjectFromFileDescriptor(int fd) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};
    
    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueFd;
    desc.handle.fd = fd;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Input parameter 'fd' should not be used beyond this point as CUDA has assumed ownership of it

    return extSem;
}
</pre><p class="p">A Vulkan semaphore object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT</samp> can be
                                    imported into CUDA using the NT handle associated with that object as shown below.
                                    Note that CUDA does not assume ownership of the NT handle and it is the
                                    applications responsibility to close the handle when it is not required anymore.
                                    The NT handle holds a reference to the resource, so it must be explicitly freed
                                    before the underlying semaphore can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importVulkanSemaphoreObjectFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}
</pre><p class="p">A Vulkan semaphore object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT</samp> can also be
                                    imported using a named handle if one exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importVulkanSemaphoreObjectFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    return extSem;
}
</pre><p class="p">A Vulkan semaphore object exported using
                                    <samp class="ph codeph">VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT</samp> can be
                                    imported into CUDA using the globally shared D3DKMT handle associated with that
                                    object as shown below. Since a globally shared D3DKMT handle does not hold a
                                    reference to the underlying semaphore it is automatically destroyed when all other
                                    references to the resource are destroyed.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importVulkanSemaphoreObjectFromKMTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt;
    desc.handle.win32.handle = (void *)handle;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    return extSem;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="signaling-waiting-on-imported-synchronization-objects-vul-int"><a name="signaling-waiting-on-imported-synchronization-objects-vul-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#signaling-waiting-on-imported-synchronization-objects-vul-int" name="signaling-waiting-on-imported-synchronization-objects-vul-int" shape="rect">3.2.14.1.6.&nbsp;Signaling/waiting on imported synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">An imported Vulkan semaphore object can be signaled as shown below. Signaling such a
                                    semaphore object sets it to the signaled state. The corresponding wait that waits on
                                    this signal must be issued in Vulkan. Additionally, the wait that waits on this
                                    signal must be issued after this signal has been issued.
                                 </p><pre xml:space="preserve">
void signalExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&amp;params, 0, sizeof(params));

    cudaSignalExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}
</pre><p class="p">An imported Vulkan semaphore object can be waited on as shown below. Waiting on such
                                    a semaphore object waits until it reaches the signaled state and then resets it back
                                    to the unsignaled state. The corresponding signal that this wait is waiting on must
                                    be issued in Vulkan. Additionally, the signal must be issued before this wait can be
                                    issued.
                                 </p><pre xml:space="preserve">
void waitExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&amp;params, 0, sizeof(params));

    cudaWaitExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}
</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" id="opengl-interoperability-ext-res-int"><a name="opengl-interoperability-ext-res-int" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#opengl-interoperability-ext-res-int" name="opengl-interoperability-ext-res-int" shape="rect">3.2.14.2.&nbsp;OpenGL Interoperability</a></h3>
                           <div class="body conbody">
                              <p class="p">Traditional OpenGL-CUDA interop as outlined in section 3.2.12.1 works by CUDA
                                 directly consuming handles created in OpenGL. However, since OpenGL can also consume
                                 memory and synchronization objects created in Vulkan, there exists an alternative
                                 approach to doing OpenGL-CUDA interop. Essentially, memory and synchronization
                                 objects exported by Vulkan could be imported into both, OpenGL and CUDA, and then
                                 used to coordinate memory accesses between OpenGL and CUDA. Please refer to the
                                 following OpenGL extensions for further details on how to import memory and
                                 synchronization objects exported by Vulkan:
                              </p>
                              <p class="p">GL_EXT_memory_object</p>
                              <p class="p">GL_EXT_memory_object_fd </p>
                              <p class="p">GL_EXT_memory_object_win32</p>
                              <p class="p">GL_EXT_semaphore</p>
                              <p class="p">GL_EXT_semaphore_fd</p>
                              <p class="p">GL_EXT_semaphore_win32</p>
                           </div>
                        </div>
                        <div class="topic concept nested3" id="direct3d-12-interoperability"><a name="direct3d-12-interoperability" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#direct3d-12-interoperability" name="direct3d-12-interoperability" shape="rect">3.2.14.3.&nbsp;Direct3D 12 Interoperability</a></h3>
                           <div class="topic concept nested4" id="matching-device-luids-dir3d-12-int"><a name="matching-device-luids-dir3d-12-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#matching-device-luids-dir3d-12-int" name="matching-device-luids-dir3d-12-int" shape="rect">3.2.14.3.1.&nbsp;Matching device LUIDs</a></h3>
                              <div class="body conbody">
                                 <p class="p">When importing memory and synchronization objects exported by Direct3D 12, they must
                                    be imported and mapped on the same device as they were created on. The CUDA device
                                    that corresponds to the Direct3D 12 device on which the objects were created can be
                                    determined by comparing the LUID of a CUDA device with that of the Direct3D 12
                                    device, as shown in the following code sample. Note that the Direct3D 12 device must
                                    not be created on a linked node adapter. I.e. the node count as returned by
                                    <samp class="ph codeph">ID3D12Device::GetNodeCount</samp> must be 1. 
                                 </p><pre xml:space="preserve">
int getCudaDeviceForD3D12Device(ID3D12Device *d3d12Device) {
    LUID d3d12Luid = d3d12Device-&gt;GetAdapterLuid();

    int cudaDeviceCount;
    cudaGetDeviceCount(&amp;cudaDeviceCount);

    for (int cudaDevice = 0; cudaDevice &lt; cudaDeviceCount; cudaDevice++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&amp;deviceProp, cudaDevice);
        char *cudaLuid = deviceProp.luid;

        if (!memcmp(&amp;d3d12Luid.LowPart, cudaLuid, sizeof(d3d12Luid.LowPart)) &amp;&amp;
            !memcmp(&amp;d3d12Luid.HighPart, cudaLuid + sizeof(d3d12Luid.LowPart), sizeof(d3d12Luid.HighPart))) {
            return cudaDevice;
        }
    }
    return cudaInvalidDeviceId;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-memory-objects-dir3d-12-int"><a name="importing-memory-objects-dir3d-12-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-memory-objects-dir3d-12-int" name="importing-memory-objects-dir3d-12-int" shape="rect">3.2.14.3.2.&nbsp;Importing memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A shareable Direct3D 12 heap memory object, created by setting the flag
                                    <samp class="ph codeph">D3D12_HEAP_FLAG_SHARED</samp> in the call to
                                    <samp class="ph codeph">ID3D12Device::CreateHeap</samp>, can be imported into CUDA using the
                                    NT handle associated with that object as shown below. Note that it is the
                                    applications responsibility to close the NT handle when it is not required anymore.
                                    The NT handle holds a reference to the resource, so it must be explicitly freed
                                    before the underlying memory can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D12HeapFromNTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Heap;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}
</pre><p class="p">A shareable Direct3D 12 heap memory object can also be imported using a named handle
                                    if one exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D12HeapFromNamedNTHandle(LPCWSTR name, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Heap;
    desc.handle.win32.name = (void *)name;
    desc.size = size;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    return extMem;
}
</pre><p class="p">A shareable Direct3D 12 committed resource, created by setting the flag
                                    <samp class="ph codeph">D3D12_HEAP_FLAG_SHARED</samp> in the call to
                                    <samp class="ph codeph">D3D12Device::CreateCommittedResource</samp>, can be imported into CUDA
                                    using the NT handle associated with that object as shown below. When importing a
                                    Direct3D 12 committed resource, the flag
                                    <samp class="ph codeph">cudaExternalMemoryDedicated</samp> must be set. Note that it is the
                                    applications responsibility to close the NT handle when it is not required anymore.
                                    The NT handle holds a reference to the resource, so it must be explicitly freed
                                    before the underlying memory can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D12CommittedResourceFromNTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Resource;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}
</pre><p class="p">A shareable Direct3D 12 committed resource can also be imported using a named handle
                                    if one exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D12CommittedResourceFromNamedNTHandle(LPCWSTR name, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Resource;
    desc.handle.win32.name = (void *)name;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    return extMem;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-buffers-onto-imported-memory-objects-dir3d-12-int"><a name="mapping-buffers-onto-imported-memory-objects-dir3d-12-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-buffers-onto-imported-memory-objects-dir3d-12-int" name="mapping-buffers-onto-imported-memory-objects-dir3d-12-int" shape="rect">3.2.14.3.3.&nbsp;Mapping buffers onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A device pointer can be mapped onto an imported memory object as shown below. The
                                    offset and size of the mapping must match that specified when creating the mapping
                                    using the corresponding Direct3D 12 API. All mapped device pointers must be freed
                                    using <samp class="ph codeph">cudaFree()</samp>.
                                 </p><pre xml:space="preserve">
void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {
    void *ptr = NULL;
    cudaExternalMemoryBufferDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.size = size;

    cudaExternalMemoryGetMappedBuffer(&amp;ptr, extMem, &amp;desc);

    // Note: ptr must eventually be freed using cudaFree()
    return ptr;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-12-int"><a name="mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-12-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-12-int" name="mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-12-int" shape="rect">3.2.14.3.4.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A CUDA mipmapped array can be mapped onto an imported memory object as shown below.
                                    The offset, dimensions, format and number of mip levels must match that specified
                                    when creating the mapping using the corresponding Direct3D 12 API. Additionally, if
                                    the mipmapped array can be bound as a render target in Direct3D 12, the flag
                                    <samp class="ph codeph">cudaArrayColorAttachment</samp> must be set. All mapped mipmapped
                                    arrays must be freed using <samp class="ph codeph">cudaFreeMipmappedArray()</samp>. The following
                                    code sample shows how to convert Vulkan parameters into the corresponding CUDA
                                    parameters when mapping mipmapped arrays onto imported memory objects.
                                 </p><pre xml:space="preserve">
cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: mipmap must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&amp;mipmap, extMem, &amp;desc);

    return mipmap;
}

cudaChannelFormatDesc getCudaChannelFormatDescForDxgiFormat(DXGI_FORMAT dxgiFormat)
{
    cudaChannelFormatDesc d;

    memset(&amp;d, 0, sizeof(d));

    switch (dxgiFormat) {
    case DXGI_FORMAT_R8_UINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8_SINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8_UINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8_SINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8B8A8_UINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8B8A8_SINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16_UINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16_SINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16_UINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16_SINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16B16A16_UINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16B16A16_SINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_UINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32_SINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_FLOAT:          d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32_UINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32_SINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32_FLOAT:       d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32B32A32_UINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32B32A32_SINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32B32A32_FLOAT: d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindFloat;    break;
    default: assert(0);</pre><p class="p"></p><pre xml:space="preserve">
    }

    return d;
}

cudaExtent getCudaExtentForD3D12Extent(UINT64 width, UINT height, UINT16 depthOrArraySize, D3D12_SRV_DIMENSION d3d12SRVDimension) {
    cudaExtent e = { 0, 0, 0 };

    switch (d3d12SRVDimension) {
    case D3D12_SRV_DIMENSION_TEXTURE1D:        e.width = width; e.height = 0;      e.depth = 0;                break;
    case D3D12_SRV_DIMENSION_TEXTURE2D:        e.width = width; e.height = height; e.depth = 0;                break;
    case D3D12_SRV_DIMENSION_TEXTURE3D:        e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURECUBE:      e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURE1DARRAY:   e.width = width; e.height = 0;      e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURE2DARRAY:   e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURECUBEARRAY: e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    default: assert(0);
    }

    return e;
}

unsigned int getCudaMipmappedArrayFlagsForD3D12Resource(D3D12_SRV_DIMENSION d3d12SRVDimension, D3D12_RESOURCE_FLAGS d3d12ResourceFlags, bool allowSurfaceLoadStore) {
    unsigned int flags = 0;

    switch (d3d12SRVDimension) {
    case D3D12_SRV_DIMENSION_TEXTURECUBE:      flags |= cudaArrayCubemap;                    break;
    case D3D12_SRV_DIMENSION_TEXTURECUBEARRAY: flags |= cudaArrayCubemap | cudaArrayLayered; break;
    case D3D12_SRV_DIMENSION_TEXTURE1DARRAY:   flags |= cudaArrayLayered;                    break;
    case D3D12_SRV_DIMENSION_TEXTURE2DARRAY:   flags |= cudaArrayLayered;                    break;
    default: break;
    }

    if (d3d12ResourceFlags &amp; D3D12_RESOURCE_FLAG_ALLOW_RENDER_TARGET) {
        flags |= cudaArrayColorAttachment;
    }
    if (allowSurfaceLoadStore) {
        flags |= cudaArraySurfaceLoadStore;
    }

    return flags;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-synchronization-objects-dir3d-12-int"><a name="importing-synchronization-objects-dir3d-12-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-synchronization-objects-dir3d-12-int" name="importing-synchronization-objects-dir3d-12-int" shape="rect">3.2.14.3.5.&nbsp;Importing synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A shareable Direct3D 12 fence object, created by setting the flag
                                    <samp class="ph codeph">D3D12_FENCE_FLAG_SHARED</samp> in the call to
                                    <samp class="ph codeph">ID3D12Device::CreateFence</samp>, can be imported into CUDA using the
                                    NT handle associated with that object as shown below. Note that it is the
                                    applications responsibility to close the handle when it is not required anymore.
                                    The NT handle holds a reference to the resource, so it must be explicitly freed
                                    before the underlying semaphore can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D12FenceFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D12Fence;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}
</pre><p class="p">A shareable Direct3D 12 fence object can also be imported using a named handle if one
                                    exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D12FenceFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};
 
    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D12Fence;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    return extSem;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="signaling-waiting-on-imported-synchronization-objects-dir3d-12-int"><a name="signaling-waiting-on-imported-synchronization-objects-dir3d-12-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#signaling-waiting-on-imported-synchronization-objects-dir3d-12-int" name="signaling-waiting-on-imported-synchronization-objects-dir3d-12-int" shape="rect">3.2.14.3.6.&nbsp;Signaling/waiting on imported synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">An imported Direct3D 12 fence object can be signaled as shown below. Signaling such a
                                    fence object sets its value to the one specified. The corresponding wait that waits
                                    on this signal must be issued in Direct3D 12. Additionally, the wait that waits on
                                    this signal must be issued after this signal has been issued.
                                 </p><pre xml:space="preserve">
void signalExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&amp;params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaSignalExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}
</pre><p class="p">An imported Direct3D 12 fence object can be waited on as shown below. Waiting on such
                                    a fence object waits until its value becomes greater than or equal to the specified
                                    value. The corresponding signal that this wait is waiting on must be issued in
                                    Direct3D 12. Additionally, the signal must be issued before this wait can be
                                    issued.
                                 </p><pre xml:space="preserve">
void waitExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&amp;params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaWaitExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}
</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" id="direct3d11-interoperability"><a name="direct3d11-interoperability" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#direct3d11-interoperability" name="direct3d11-interoperability" shape="rect">3.2.14.4.&nbsp;Direct3D 11 Interoperability</a></h3>
                           <div class="topic concept nested4" id="matching-device-luids-dir3d-11-int"><a name="matching-device-luids-dir3d-11-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#matching-device-luids-dir3d-11-int" name="matching-device-luids-dir3d-11-int" shape="rect">3.2.14.4.1.&nbsp;Matching device LUIDs</a></h3>
                              <div class="body conbody">
                                 <p class="p">When importing memory and synchronization objects exported by Direct3D 11, they must
                                    be imported and mapped on the same device as they were created on. The CUDA device
                                    that corresponds to the Direct3D 11 device on which the objects were created can be
                                    determined by comparing the LUID of a CUDA device with that of the Direct3D 11
                                    device, as shown in the following code sample.
                                 </p><pre xml:space="preserve">
int getCudaDeviceForD3D11Device(ID3D11Device *d3d11Device) {
    IDXGIDevice *dxgiDevice;
    d3d11Device-&gt;QueryInterface(__uuidof(IDXGIDevice), (void **)&amp;dxgiDevice);

    IDXGIAdapter *dxgiAdapter;
    dxgiDevice-&gt;GetAdapter(&amp;dxgiAdapter);

    DXGI_ADAPTER_DESC dxgiAdapterDesc;
    dxgiAdapter-&gt;GetDesc(&amp;dxgiAdapterDesc);

    LUID d3d11Luid = dxgiAdapterDesc.AdapterLuid;

    int cudaDeviceCount;
    cudaGetDeviceCount(&amp;cudaDeviceCount);

    for (int cudaDevice = 0; cudaDevice &lt; cudaDeviceCount; cudaDevice++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&amp;deviceProp, cudaDevice);
        char *cudaLuid = deviceProp.luid;

        if (!memcmp(&amp;d3d11Luid.LowPart, cudaLuid, sizeof(d3d11Luid.LowPart)) &amp;&amp;
            !memcmp(&amp;d3d11Luid.HighPart, cudaLuid + sizeof(d3d11Luid.LowPart), sizeof(d3d11Luid.HighPart))) {
            return cudaDevice;
        }
    }
    return cudaInvalidDeviceId;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-memory-objects-dir3d-11-int"><a name="importing-memory-objects-dir3d-11-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-memory-objects-dir3d-11-int" name="importing-memory-objects-dir3d-11-int" shape="rect">3.2.14.4.2.&nbsp;Importing memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A shareable Direct3D 11 texture resource, viz, <samp class="ph codeph">ID3D11Texture1D</samp>,
                                    <samp class="ph codeph">ID3D11Texture2D</samp> or <samp class="ph codeph">ID3D11Texture3D</samp>, can be
                                    created by setting either the <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED</samp> or
                                    <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED_KEYEDMUTEX</samp> (on Windows 7) or
                                    <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED_NTHANDLE</samp> (on Windows 10) when
                                    calling <samp class="ph codeph">ID3D11Device:CreateTexture1D</samp>,
                                    <samp class="ph codeph">ID3D11Device:CreateTexture2D</samp> or
                                    <samp class="ph codeph">ID3D11Device:CreateTexture3D</samp> respectively. A shareable Direct3D
                                    11 buffer resource, <samp class="ph codeph">ID3D11Buffer</samp>, can be created by specifying
                                    either of the above flags when calling <samp class="ph codeph">ID3D11Device::CreateBuffer</samp>.
                                    A shareable resource created by specifying the
                                    <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED_NTHANDLE</samp> can be imported into CUDA
                                    using the NT handle associated with that object as shown below. Note that it is the
                                    applications responsibility to close the NT handle when it is not required anymore.
                                    The NT handle holds a reference to the resource, so it must be explicitly freed
                                    before the underlying memory can be freed. When importing a Direct3D 11 resource,
                                    the flag <samp class="ph codeph">cudaExternalMemoryDedicated</samp> must be set.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D11ResourceFromNTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D11Resource;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}
</pre><p class="p">A shareable Direct3D 11 resource can also be imported using a named handle if one
                                    exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D11ResourceFromNamedNTHandle(LPCWSTR name, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D11Resource;
    desc.handle.win32.name = (void *)name;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    return extMem;
}
</pre><p class="p">A shareable Direct3D 11 resource, created by specifying the
                                    <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED</samp> or
                                    <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED_KEYEDMUTEX</samp>, can be imported into
                                    CUDA using the globally shared <samp class="ph codeph">D3DKMT</samp> handle associated with that
                                    object as shown below. Since a globally shared <samp class="ph codeph">D3DKMT</samp> handle does
                                    not hold a reference to the underlying memory it is automatically destroyed when all
                                    other references to the resource are destroyed.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importD3D11ResourceFromKMTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D11ResourceKmt;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&amp;extMem, &amp;desc);

    return extMem;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-buffers-onto-imported-memory-objects-dir3d-11-int"><a name="mapping-buffers-onto-imported-memory-objects-dir3d-11-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-buffers-onto-imported-memory-objects-dir3d-11-int" name="mapping-buffers-onto-imported-memory-objects-dir3d-11-int" shape="rect">3.2.14.4.3.&nbsp;Mapping buffers onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A device pointer can be mapped onto an imported memory object as shown below. The
                                    offset and size of the mapping must match that specified when creating the mapping
                                    using the corresponding Direct3D 11 API. All mapped device pointers must be freed
                                    using <samp class="ph codeph">cudaFree()</samp>.
                                 </p><pre xml:space="preserve">
void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {
    void *ptr = NULL;
    cudaExternalMemoryBufferDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.size = size;

    cudaExternalMemoryGetMappedBuffer(&amp;ptr, extMem, &amp;desc);

    // Note: ptr must eventually be freed using cudaFree()
    return ptr;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-11-int"><a name="mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-11-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-11-int" name="mapping-mipmapped-arrays-onto-imported-memory-objects-dir3d-11-int" shape="rect">3.2.14.4.4.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A CUDA mipmapped array can be mapped onto an imported memory object as shown below.
                                    The offset, dimensions, format and number of mip levels must match that specified
                                    when creating the mapping using the corresponding Direct3D 11 API. Additionally, if
                                    the mipmapped array can be bound as a render target in Direct3D 12, the flag
                                    <samp class="ph codeph">cudaArrayColorAttachment</samp> must be set. All mapped mipmapped
                                    arrays must be freed using <samp class="ph codeph">cudaFreeMipmappedArray()</samp>. The following
                                    code sample shows how to convert Direct3D 11 parameters into the corresponding CUDA
                                    parameters when mapping mipmapped arrays onto imported memory objects.
                                 </p><pre xml:space="preserve">
cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: mipmap must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&amp;mipmap, extMem, &amp;desc);

    return mipmap;
}

cudaChannelFormatDesc getCudaChannelFormatDescForDxgiFormat(DXGI_FORMAT dxgiFormat)
{
    cudaChannelFormatDesc d;
    memset(&amp;d, 0, sizeof(d));
    switch (dxgiFormat) {
    case DXGI_FORMAT_R8_UINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8_SINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8_UINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8_SINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8B8A8_UINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8B8A8_SINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16_UINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16_SINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16_UINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16_SINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16B16A16_UINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16B16A16_SINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_UINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32_SINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_FLOAT:          d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32_UINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32_SINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32_FLOAT:       d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32B32A32_UINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32B32A32_SINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32B32A32_FLOAT: d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindFloat;    break;
    default: assert(0);
    }</pre><p class="p"></p><pre xml:space="preserve">

    return d;
}

cudaExtent getCudaExtentForD3D11Extent(UINT64 width, UINT height, UINT16 depthOrArraySize, D3D12_SRV_DIMENSION d3d11SRVDimension) {
    cudaExtent e = { 0, 0, 0 };

    switch (d3d11SRVDimension) {
    case D3D11_SRV_DIMENSION_TEXTURE1D:        e.width = width; e.height = 0;      e.depth = 0;                break;
    case D3D11_SRV_DIMENSION_TEXTURE2D:        e.width = width; e.height = height; e.depth = 0;                break;
    case D3D11_SRV_DIMENSION_TEXTURE3D:        e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURECUBE:      e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURE1DARRAY:   e.width = width; e.height = 0;      e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURE2DARRAY:   e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURECUBEARRAY: e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    default: assert(0);
    }
    return e;
}

unsigned int getCudaMipmappedArrayFlagsForD3D12Resource(D3D11_SRV_DIMENSION d3d11SRVDimension, D3D11_BIND_FLAG d3d11BindFlags, bool allowSurfaceLoadStore) {
    unsigned int flags = 0;

    switch (d3d11SRVDimension) {
    case D3D11_SRV_DIMENSION_TEXTURECUBE:      flags |= cudaArrayCubemap;                    break;
    case D3D11_SRV_DIMENSION_TEXTURECUBEARRAY: flags |= cudaArrayCubemap | cudaArrayLayered; break;
    case D3D11_SRV_DIMENSION_TEXTURE1DARRAY:   flags |= cudaArrayLayered;                    break;
    case D3D11_SRV_DIMENSION_TEXTURE2DARRAY:   flags |= cudaArrayLayered;                    break;
    default: break;
    }

    if (d3d11BindFlags &amp; D3D11_BIND_RENDER_TARGET) {
        flags |= cudaArrayColorAttachment;
    }

    if (allowSurfaceLoadStore) {
        flags |= cudaArraySurfaceLoadStore;
    }

    return flags;
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-synchronization-objects-dir3d-11-int"><a name="importing-synchronization-objects-dir3d-11-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-synchronization-objects-dir3d-11-int" name="importing-synchronization-objects-dir3d-11-int" shape="rect">3.2.14.4.5.&nbsp;Importing synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A shareable Direct3D 11 fence object, created by setting the flag
                                    <samp class="ph codeph">D3D11_FENCE_FLAG_SHARED</samp> in the call to
                                    <samp class="ph codeph">ID3D11Device5::CreateFence</samp>, can be imported into CUDA using the
                                    NT handle associated with that object as shown below. Note that it is the
                                    applications responsibility to close the handle when it is not required anymore.
                                    The NT handle holds a reference to the resource, so it must be explicitly freed
                                    before the underlying semaphore can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D11FenceFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D11Fence;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}
</pre><p class="p">A shareable Direct3D 11 fence object can also be imported using a named handle if one
                                    exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D11FenceFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D11Fence;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    return extSem;
}</pre><p class="p">A shareable Direct3D 11 keyed mutex object associated with a shareable Direct3D 11
                                    resource, viz, <samp class="ph codeph">IDXGIKeyedMutex</samp>, created by setting the flag
                                    <samp class="ph codeph">D3D11_RESOURCE_MISC_SHARED_KEYEDMUTEX</samp>, can be imported into
                                    CUDA using the NT handle associated with that object as shown below. Note that it is
                                    the applications responsibility to close the handle when it is not required
                                    anymore. The NT handle holds a reference to the resource, so it must be explicitly
                                    freed before the underlying semaphore can be freed.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D11KeyedMutexFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeKeyedMutex;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}</pre><p class="p">A shareable Direct3D 11 keyed mutex object can also be imported using a named handle
                                    if one exists as shown below.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D11KeyedMutexFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeKeyedMutex;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    return extSem;
}</pre><p class="p">A shareable Direct3D 11 keyed mutex object can be imported into CUDA using the
                                    globally shared D3DKMT handle associated with that object as shown below. Since a
                                    globally shared D3DKMT handle does not hold a reference to the underlying memory it
                                    is automatically destroyed when all other references to the resource are
                                    destroyed.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importD3D11FenceFromKMTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeKeyedMutexKmt;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" id="signaling-waiting-on-imported-synchronization-objects-dir3d-11-int"><a name="signaling-waiting-on-imported-synchronization-objects-dir3d-11-int" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#signaling-waiting-on-imported-synchronization-objects-dir3d-11-int" name="signaling-waiting-on-imported-synchronization-objects-dir3d-11-int" shape="rect">3.2.14.4.6.&nbsp;Signaling/waiting on imported synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">An imported Direct3D 11 fence object can be signaled as shown below. Signaling such a
                                    fence object sets its value to the one specified. The corresponding wait that waits
                                    on this signal must be issued in Direct3D 11. Additionally, the wait that waits on
                                    this signal must be issued after this signal has been issued.
                                 </p><pre xml:space="preserve">
void signalExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&amp;params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaSignalExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}</pre><p class="p">An imported Direct3D 11 fence object can be waited on as shown below. Waiting on such
                                    a fence object waits until its value becomes greater than or equal to the specified
                                    value. The corresponding signal that this wait is waiting on must be issued in
                                    Direct3D 11. Additionally, the signal must be issued before this wait can be
                                    issued.
                                 </p><pre xml:space="preserve">
void waitExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&amp;params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaWaitExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}</pre><p class="p">An imported Direct3D 11 keyed mutex object can be signaled as shown below. Signaling
                                    such a keyed mutex object by specifying a key value releases the keyed mutex for
                                    that value. The corresponding wait that waits on this signal must be issued in
                                    Direct3D 11 with the same key value. Additionally, the Direct3D 11 wait must be
                                    issued after this signal has been issued.
                                 </p><pre xml:space="preserve">
void signalExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long key, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&amp;params, 0, sizeof(params));

    params.params.keyedmutex.key = key;

    cudaSignalExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}
</pre><p class="p">An imported Direct3D 11 keyed mutex object can be waited on as shown below. A timeout
                                    value in milliseconds is needed when waiting on such a keyed mutex. The wait
                                    operation waits until the keyed mutex value is equal to the specified key value or
                                    until the timeout has elapsed. The timeout interval can also be an infinite value.
                                    In case an infinite value is specified the timeout never elapses. The windows
                                    INFINITE macro must be used to specify an infinite timeout. The corresponding signal
                                    that this wait is waiting on must be issued in Direct3D 11. Additionally, the
                                    Direct3D 11 signal must be issued before this wait can be issued.
                                 </p><pre xml:space="preserve">
void waitExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long key, unsigned int timeoutMs, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&amp;params, 0, sizeof(params));

    params.params.keyedmutex.key = key;
    params.params.keyedmutex.timeoutMs = timeoutMs;

    cudaWaitExternalSemaphoresAsync(&amp;extSem, &amp;params, 1, stream);
}</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" id="nvidia-softwarcommunication-interface-interoperability-nvsci"><a name="nvidia-softwarcommunication-interface-interoperability-nvsci" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#nvidia-softwarcommunication-interface-interoperability-nvsci" name="nvidia-softwarcommunication-interface-interoperability-nvsci" shape="rect">3.2.14.5.&nbsp;NVIDIA Software Communication Interface Interoperability (NVSCI)</a></h3>
                           <div class="body conbody">
                              <p class="p">NvSciBuf and NvSciSync are interfaces developed for serving the following purposes - </p>
                              <p class="p">Allow applications to allocate and exchange buffers in memory - NvSciBuf</p>
                              <p class="p">Allow applications to manage synchronization objects at operation boundaries -
                                 NvSciSync
                              </p>
                              <p class="p">More details on these interfaces are available at -NvSciBuf - <a class="xref" href="https://docs.nvidia.com/drive/active/5.1.6.0L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide%2FGraphics%2Fnvsci_nvscibuf.html%23" target="_blank" shape="rect"><u class="ph u">https://docs.nvidia.com/drive/active/5.1.6.0L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide%2FGraphics%2Fnvsci_nvscibuf.html%23</u></a></p>
                              <p class="p">NvSciSync - <a class="xref" href="https://docs.nvidia.com/drive/active/5.1.6.0L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide%2FGraphics%2Fnvsci_nvscisync.html%23wwpID0E0PM0HA" target="_blank" shape="rect"><u class="ph u">https://docs.nvidia.com/drive/active/5.1.6.0L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide%2FGraphics%2Fnvsci_nvscisync.html%23wwpID0E0PM0HA</u></a></p>
                           </div>
                           <div class="topic concept nested4" id="importing-memory-objects-nvsci"><a name="importing-memory-objects-nvsci" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-memory-objects-nvsci" name="importing-memory-objects-nvsci" shape="rect">3.2.14.5.1.&nbsp;Importing memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">For allocating an NvSciBuf object compatible with a given CUDA device, the
                                    corresponding GPU id must be set with NvSciBufGeneralAttrKey_GpuId in the NvSciBuf
                                    attribute list as shown below. For more details on how to allocate and maintain
                                    NvSciBuf objects refer to <a class="xref" href="https://docs.nvidia.com/drive/active/5.1.6.0L/nvvib_docs/DRIVE_OS_Linux_SDK_Development_Guide/baggage/group__nvscibuf__obj__api.html#ga3a1be8a02e29ce4c92e2ed27fa9ea828" target="_blank" shape="rect"><u class="ph u">https://docs.nvidia.com/drive/active/5.1.6.0L/nvvib_docs/DRIVE_OS_Linux_SDK_Development_Guide/baggage/group__nvscibuf__obj__api.html#ga3a1be8a02e29ce4c92e2ed27fa9ea828</u></a>.
                                 </p><pre xml:space="preserve">
NvSciBufObj createNvSciBufObject() {
   // Raw Buffer Attributes for CUDA
    NvSciBufType bufType = NvSciBufType_RawBuffer;
    uint64_t rawsize = SIZE;
    uint64_t align = 0;
    bool cpuaccess_flag = true;
    NvSciBufAttrValAccessPerm perm = NvSciBufAccessPerm_ReadWrite; 

    uint64_t gpuId[] = {};
    cuDeviceGetUuid(&amp;uuid, dev));
    gpuid[0] = uuid.bytes;
  
    // Fill in values
    NvSciBufAttrKeyValuePair rawbuffattrs[] = {                              
         { NvSciBufGeneralAttrKey_Types, &amp;bufType, sizeof(bufType) },        
         { NvSciBufRawBufferAttrKey_Size, &amp;rawsize, sizeof(rawsize) },       
         { NvSciBufRawBufferAttrKey_Align, &amp;align, sizeof(align) },          
         { NvSciBufGeneralAttrKey_NeedCpuAccess, &amp;cpuaccess_flag,            
                sizeof(cpuaccess_flag) },
         { NvSciBufGeneralAttrKey_RequiredPerm, &amp;perm, sizeof(perm) },
         { NvSciBufGeneralAttrKey_GpuId, &amp;gpuid, sizeof(gpuId) },                                 
    };                                                                       

    // Create list by setting attributes
    err = NvSciBufAttrListSetAttrs(attrListBuffer, rawbuffattrs,               
            sizeof(rawbuffattrs)/sizeof(NvSciBufAttrKeyValuePair)); 
                    
    NvSciBufAttrListCreate(NvSciBufModule, &amp;attrListBuffer);

    // Reconcile And Allocate
    NvSciBufAttrListReconcile(&amp;attrListBuffer, 1, &amp;attrListReconciledBuffer,    
                       &amp;attrListConflictBuffer)
    NvSciBufObjAlloc(attrListReconciledBuffer, &amp;bufferObjRaw);
    return bufferObjRaw;
}</pre><p class="p">The allocated NvSciBuf memory object can be imported in CUDA using the NvSciBufObj
                                    handle as shown below. Application should query the allocated NvSciBufObj for
                                    attributes required for filling CUDA External Memory Descriptor. Note that the
                                    attribute list and NvSciBuf objects should be maintained by the application. If the
                                    NvSciBuf object imported into CUDA is also mapped by other drivers, then the
                                    application must use NvSciSync objects (Refer <u class="ph u">3.2.13.5.4 Importing
                                       synchronization objects) </u> as appropriate barriers to maintain coherence
                                    between CUDA and the other drivers.
                                 </p><pre xml:space="preserve">
cudaExternalMemory_t importNvSciBufObject (NvSciBufObj bufferObjRaw) {
    /*************** Query NvSciBuf Object **************/
    NvSciBufAttrKeyValuePair bufattrs[] = {
{NvSciBufRawBufferAttrKey_Size, NULL, 0},
    };
    NvSciBufAttrListGetAttrs(retList, bufattrs, 
        sizeof(bufattrs)/sizeof(NvSciBufAttrKeyValuePair)));
    ret_size = *(static_cast&lt;const uint64_t*&gt;(bufattrs[0].value));

    /*************** NvSciBuf Registration With CUDA **************/

    // Fill up CUDA_EXTERNAL_MEMORY_HANDLE_DESC
    cudaExternalMemoryHandleDesc memHandleDesc;
    memset(&amp;memHandleDesc, 0, sizeof(memHandleDesc));
    memHandleDesc.type = cudaExternalMemoryHandleTypeNvSciBuf;
    memHandleDesc.handle.nvSciBufObject = bufferObjRaw;
    memHandleDesc.size = ret_size;
    cudaImportExternalMemory(&amp;extMemBuffer, &amp;memHandleDesc);
    return extMemBuffer;
 }
</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-buffers-onto-imported-memory-objects-nvsci"><a name="mapping-buffers-onto-imported-memory-objects-nvsci" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-buffers-onto-imported-memory-objects-nvsci" name="mapping-buffers-onto-imported-memory-objects-nvsci" shape="rect">3.2.14.5.2.&nbsp;Mapping buffers onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A device pointer can be mapped onto an imported memory object as shown below. The
                                    offset and size of the mapping can be filled as per the attributes of the allocated
                                    NvSciBufObj. All mapped device pointers must be freed using
                                    <samp class="ph codeph">cudaFree()</samp>.
                                 </p><pre xml:space="preserve">
void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {
    void *ptr = NULL;
    cudaExternalMemoryBufferDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.size = size;

    cudaExternalMemoryGetMappedBuffer(&amp;ptr, extMem, &amp;desc);

    // Note: ptr must eventually be freed using cudaFree()
    return ptr;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" id="mapping-mipmapped-arrays-onto-imported-memory-objects-nvsci"><a name="mapping-mipmapped-arrays-onto-imported-memory-objects-nvsci" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#mapping-mipmapped-arrays-onto-imported-memory-objects-nvsci" name="mapping-mipmapped-arrays-onto-imported-memory-objects-nvsci" shape="rect">3.2.14.5.3.&nbsp;Mapping mipmapped arrays onto imported memory objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">A CUDA mipmapped array can be mapped onto an imported memory object as shown below.
                                    The offset, dimensions and format can be filled as per the attributes of the
                                    allocated NvSciBufObj. The number of mip levels must be 1. All mapped mipmapped
                                    arrays must be freed using cudaFreeMipmappedArray(). The following code sample shows
                                    how to convert NvSciBuf attributes into the corresponding CUDA parameters when
                                    mapping mipmapped arrays onto imported memory objects.
                                 </p><pre xml:space="preserve">
cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: mipmap must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&amp;mipmap, extMem, &amp;desc);

    return mipmap;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" id="importing-synchronization-objects-nvsci"><a name="importing-synchronization-objects-nvsci" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#importing-synchronization-objects-nvsci" name="importing-synchronization-objects-nvsci" shape="rect">3.2.14.5.4.&nbsp;Importing synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">NvSciSync attributes that are compatible with a given CUDA device can be generated
                                    using cudaDeviceGetNvSciSyncAttributes(). The returned attribute list can be used to
                                    create a NvSciSyncObj that is guaranteed compatibility with a given CUDA device.
                                 </p><pre xml:space="preserve">
NvSciSyncObj createNvSciSyncObject() {
    NvSciSyncObj nvSciSyncObj
    int cudaDev0 = 0;
    int cudaDev1 = 1;
    NvSciSyncAttrList signalerAttrList = NULL;
    NvSciSyncAttrList waiterAttrList = NULL;
    NvSciSyncAttrList reconciledList = NULL;
    NvSciSyncAttrList newConflictList = NULL;

    NvSciSyncAttrListCreate(module, &amp;signalerAttrList);
    NvSciSyncAttrListCreate(module, &amp;waiterAttrList);
    NvSciSyncAttrList unreconciledList[2] = {NULL, NULL};
    unreconciledList[0] = signalerAttrList;
    unreconciledList[1] = waiterAttrList;

    cudaDeviceGetNvSciSyncAttributes(signalerAttrList, cudaDev0, CUDA_NVSCISYNC_ATTR_SIGNAL);
    cudaDeviceGetNvSciSyncAttributes(waiterAttrList, cudaDev1, CUDA_NVSCISYNC_ATTR_WAIT);
        
    NvSciSyncAttrListReconcile(unreconciledList, 2, &amp;reconciledList, &amp;newConflictList);

    NvSciSyncObjAlloc(reconciledList, &amp;nvSciSyncObj);

    return nvSciSyncObj;
}
</pre><p class="p">An NvSciSync object (created as above) can be imported into CUDA using the
                                    NvSciSyncObj handle as shown below. Note that ownership of the NvSciSyncObj handle
                                    continues to lie with the application even after it is imported.
                                 </p><pre xml:space="preserve">
cudaExternalSemaphore_t importNvSciSyncObject(void* nvSciSyncObj) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&amp;desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeNvSciSync;
    desc.handle.nvSciSyncObj = nvSciSyncObj;

    cudaImportExternalSemaphore(&amp;extSem, &amp;desc);

    // Deleting/Freeing the nvSciSyncObj beyond this point will lead to undefined behavior in CUDA
        
    return extSem;
}</pre></div>
                           </div>
                           <div class="topic concept nested4" id="signaling-waiting-on-imported-synchronization-objects-nvsci"><a name="signaling-waiting-on-imported-synchronization-objects-nvsci" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#signaling-waiting-on-imported-synchronization-objects-nvsci" name="signaling-waiting-on-imported-synchronization-objects-nvsci" shape="rect">3.2.14.5.5.&nbsp;Signaling/waiting on imported synchronization objects</a></h3>
                              <div class="body conbody">
                                 <p class="p">An imported NvSciSyncObj object can be signaled as outlined below. Signaling
                                    NvSciSync backed semaphore object initializes the <em class="ph i">fence</em> parameter passed as
                                    input. This fence parameter is waited upon by a wait operation that corresponds to
                                    the aforementioned signal. Additionally, the wait that waits on this signal must be
                                    issued after this signal has been issued. If the flags are set to
                                    <em class="ph i">cudaExternalSemaphoreSignalSkipNvSciBufMemSync then</em> memory
                                    synchronization operations (over all the imported NvSciBuf in this process) that are
                                    executed as a part of the signal operation by default are skipped.
                                 </p><pre xml:space="preserve">
void signalExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream, void *fence) {
    cudaExternalSemaphoreSignalParams signalParams = {};

    memset(&amp;signalParams, 0, sizeof(signalParams));

    signalParams.params.nvSciSync.fence = (void*)fence;
    signalParams.flags = 0; //OR cudaExternalSemaphoreSignalSkipNvSciBufMemSync

    cudaSignalExternalSemaphoresAsync(&amp;extSem, &amp;signalParams, 1, stream);

}</pre><p class="p">An imported NvSciSyncObj object can be waited upon as outlined below. Waiting on
                                    NvSciSync backed semaphore object waits until the input <em class="ph i">fence</em> parameter is
                                    signaled by the corresponding signaler. Additionally, the signal mustbe issued
                                    before the wait can be issued. If the flags are set to
                                    <em class="ph i">cudaExternalSemaphoreWaitSkipNvSciBufMemSync then</em> memory synchronization
                                    operations (over all the imported NvSciBuf in this process) that are executed as a
                                    part of the wait operation by default are skipped.
                                 </p><pre xml:space="preserve">
void waitExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream, void *fence) {
     cudaExternalSemaphoreWaitParams waitParams = {};

    memset(&amp;waitParams, 0, sizeof(waitParams));

    waitParams.params.nvSciSync.fence = (void*)fence;
    waitParams.flags = 0; //OR cudaExternalSemaphoreWaitSkipNvSciBufMemSync

    cudaWaitExternalSemaphoresAsync(&amp;extSem, &amp;waitParams, 1, stream);
}</pre></div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="cuda-user-objects"><a name="cuda-user-objects" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#cuda-user-objects" name="cuda-user-objects" shape="rect">3.2.15.&nbsp;CUDA User Objects</a></h3>
                        <div class="body conbody">
                           <p class="p">CUDA User Objects can be used to help manage the lifetime of resources used by asynchronous
                              work in CUDA. In particular, this feature is useful for <a class="xref" href="index.html#cuda-graphs" shape="rect">CUDA Graphs</a> and
                              <a class="xref" href="index.html#creating-a-graph-using-stream-capture" shape="rect">stream capture</a>.
                           </p>
                           <div class="p">Various resource management schemes are not compatible with CUDA graphs. Consider for
                              example an event-based pool or a synchronous-create, asynchronous-destroy scheme.
                              <pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Library API with pool allocation</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> libraryWork(cudaStream_t stream) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> &amp;resource = pool.claimTemporaryResource();
    resource.waitOnReadyEventInStream(stream);
    launchWork(stream, resource);
    resource.recordReadyEvent(stream);
}
</pre><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Library API with asynchronous resource deletion</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> libraryWork(cudaStream_t stream) {
    Resource *resource = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">new</span> Resource(...);
    launchWork(stream, resource);
    cudaStreamAddCallback(
        stream,
        [](cudaStream_t, cudaError_t, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *resource) {
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">delete</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static_cast</span>&lt;Resource *&gt;(resource);
        },
        resource,
        0);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error handling considerations not shown</span>
}
</pre></div>
                           <p class="p">These schemes are difficult with CUDA graphs because of the non-fixed pointer or handle
                              for the resource which requires indirection or graph update, and the synchronous CPU code
                              needed each time the work is submitted. They also do not work with stream capture if these
                              considerations are hidden from the caller of the library, and because of use of disallowed
                              APIs during capture. Various solutions exist such as exposing the resource to the
                              caller. CUDA user objects present another approach.
                           </p>
                           <p class="p">A CUDA user object associates a user-specified destructor callback with an internal refcount,
                              similar to C++ <samp class="ph codeph">shared_ptr</samp>. References may be owned by user code on the
                              CPU and by CUDA graphs. Note that for user-owned references, unlike C++ smart pointers,
                              there is no object representing the reference; users must track user-owned references
                              manually. A typical use case would be to immediately move the sole user-owned reference
                              to a CUDA graph after the user object is created.
                           </p>
                           <p class="p">When a reference is associated to a CUDA graph, CUDA will manage the graph operations
                              automatically. A cloned <samp class="ph codeph">cudaGraph_t</samp> retains a copy of every reference
                              owned by the source <samp class="ph codeph">cudaGraph_t</samp>, with the same multiplicity. An instantiated
                              <samp class="ph codeph">cudaGraphExec_t</samp> retains a copy of every reference in the source
                              <samp class="ph codeph">cudaGraph_t</samp>. When a <samp class="ph codeph">cudaGraphExec_t</samp> is destroyed without
                              being synchronized, the references are retained until the execution is completed.
                           </p>
                           <div class="p">Here is an example use.<pre xml:space="preserve">
cudaGraph_t graph;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Preexisting graph</span>

Object *object = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">new</span> Object;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// C++ object with possibly nontrivial destructor</span>
cudaUserObject_t cuObject;
cudaUserObjectCreate(
    &amp;cuObject,
    object,  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Here we use a CUDA-provided template wrapper for this API,</span>
             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// which supplies a callback to delete the C++ object pointer</span>
    1,  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initial refcount</span>
    cudaUserObjectNoDestructorSync  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Acknowledge that the callback cannot be</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// waited on via CUDA</span>
);
cudaGraphRetainUserObject(
    graph,
    cuObject,
    1,  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Number of references</span>
    cudaGraphUserObjectMove  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Transfer a reference owned by the caller (do</span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// not modify the total reference count)</span>
);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// No more references owned by this thread; no need to call release API</span>
cudaGraphExec_t graphExec;
cudaGraphInstantiate(&amp;graphExec, graph, nullptr, nullptr, 0);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Will retain a</span>
                                                               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// new reference</span>
cudaGraphDestroy(graph);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// graphExec still owns a reference</span>
cudaGraphLaunch(graphExec, 0);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Async launch has access to the user objects</span>
cudaGraphExecDestroy(graphExec);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch is not synchronized; the release</span>
                                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// will be deferred if needed</span>
cudaStreamSynchronize(0);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// After the launch is synchronized, the remaining</span>
                           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// reference is released and the destructor will</span>
                           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// execute. Note this happens asynchronously.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// If the destructor callback had signaled a synchronization object, it would</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// be safe to wait on it at this point.</span>
</pre></div>
                           <p class="p">References owned by graphs in child graph nodes are associated to the child graphs, not
                              the parents. If a child graph is updated or deleted, the references change accordingly.
                              If an executable graph or child graph is updated with <samp class="ph codeph">cudaGraphExecUpdate</samp>
                              or <samp class="ph codeph">cudaGraphExecChildGraphNodeSetParams</samp>, the references in the new source
                              graph are cloned and replace the references in the target graph. In either case, if
                              previous launches are not synchronized, any references which would be released are held
                              until the launches have finished executing.
                           </p>
                           <p class="p">There is not currently a mechanism to wait on user object destructors via a CUDA API.
                              Users may signal a synchronization object manually from the destructor code. In addition,
                              it is not legal to call CUDA APIs from the destructor, similar to the restriction on
                              <samp class="ph codeph">cudaLaunchHostFunc</samp>. This is to avoid blocking a CUDA internal shared
                              thread and preventing forward progress. It is legal to signal another thread to perform
                              an API call, if the dependency is one way and the thread doing the call cannot block
                              forward progress of CUDA work.
                           </p>
                           <p class="p">User objects are created with <samp class="ph codeph">cudaUserObjectCreate</samp>, which is a
                              good starting point to browse related APIs.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="versioning-and-compatibility"><a name="versioning-and-compatibility" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#versioning-and-compatibility" name="versioning-and-compatibility" shape="rect">3.3.&nbsp;Versioning and Compatibility</a></h3>
                     <div class="body conbody">
                        <p class="p">There are two version numbers that developers should care about when developing a
                           CUDA application: The compute capability that describes the general specifications
                           and features of the compute device (see <a class="xref" href="index.html#compute-capability" shape="rect">Compute Capability</a>) and the version of the CUDA
                           driver API that describes the features supported by the driver API and runtime.
                        </p>
                        <p class="p">The version of the driver API is defined in the driver header file as
                           <samp class="ph codeph">CUDA_VERSION</samp>. It allows developers to check whether their
                           application requires a newer device driver than the one currently installed. This is
                           important, because the driver API is <dfn class="term">backward compatible</dfn>, meaning that
                           applications, plug-ins, and libraries (including the CUDA runtime) compiled against a
                           particular version of the driver API will continue to work on subsequent device
                           driver releases as illustrated in <a class="xref" href="index.html#versioning-and-compatibility__driver-api-is-backward-but-not-forward-compatible" shape="rect">Figure 11</a>. The driver API is not <dfn class="term">forward compatible</dfn>, which means that
                           applications, plug-ins, and libraries (including the CUDA runtime) compiled against a
                           particular version of the driver API will not work on previous versions of the
                           device driver.
                        </p>
                        <p class="p">It is important to note that there are limitations on the mixing and matching of
                           versions that is supported:
                        </p>
                        <ul class="ul">
                           <li class="li">Since only one version of the CUDA Driver can be installed at a time on a
                              system, the installed driver must be of the same or higher version than the
                              maximum Driver API version against which any application, plug-ins, or libraries
                              that must run on that system were built.
                           </li>
                           <li class="li">All plug-ins and libraries used by an application must use the same version of
                              the CUDA Runtime unless they statically link to the Runtime, in which case
                              multiple versions of the runtime can coexist in the same process space. Note
                              that if <samp class="ph codeph">nvcc</samp> is used to link the application, the static
                              version of the CUDA Runtime library will be used by default, and all CUDA
                              Toolkit libraries are statically linked against the CUDA Runtime.
                           </li>
                           <li class="li">All plug-ins and libraries used by an application must use the same version of
                              any libraries that use the runtime (such as cuFFT, cuBLAS, ...) unless
                              statically linking to those libraries.
                           </li>
                        </ul>
                        <div class="fig fignone" id="versioning-and-compatibility__driver-api-is-backward-but-not-forward-compatible"><a name="versioning-and-compatibility__driver-api-is-backward-but-not-forward-compatible" shape="rect">
                              <!-- --></a><span class="figcap">Figure 11. The Driver API Is Backward but Not Forward Compatible</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="../common/graphics/compatibility-of-cuda-versions.png" alt="The Driver API Is Backward but Not Forward Compatible."></img></div><br clear="none"></br></div>
                        <p class="p">For Tesla GPU products, CUDA 10 introduced a new forward-compatible upgrade path for the user-mode
                           components of the CUDA Driver. This feature is described in
                           <a class="xref" href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html" target="_blank" shape="rect">CUDA Compatibility</a>.
                           The requirements on the CUDA Driver version described here apply to the version of the user-mode
                           components.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-modes"><a name="compute-modes" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-modes" name="compute-modes" shape="rect">3.4.&nbsp;Compute Modes</a></h3>
                     <div class="body conbody">
                        <p class="p">On Tesla solutions running Windows Server 2008 and later or Linux, one can set any
                           device in a system in one of the three following modes using NVIDIA's System
                           Management Interface (nvidia-smi), which is a tool distributed as part of the
                           driver:
                        </p>
                        <ul class="ul">
                           <li class="li"><dfn class="term">Default</dfn> compute mode: Multiple host threads can use the device (by
                              calling <samp class="ph codeph">cudaSetDevice()</samp> on this device, when using the runtime
                              API, or by making current a context associated to the device, when using the
                              driver API) at the same time. 
                           </li>
                           <li class="li"><dfn class="term">Exclusive-process</dfn> compute mode: Only one CUDA context may be
                              created on the device across all processes in the system. The context may be
                              current to as many threads as desired within the process that created that
                              context. 
                           </li>
                           <li class="li"><dfn class="term">Prohibited</dfn> compute mode: No CUDA context can be created on the
                              device. 
                           </li>
                        </ul>
                        <p class="p">This means, in particular, that a host thread using the runtime API without
                           explicitly calling <samp class="ph codeph">cudaSetDevice()</samp> might be associated with a
                           device other than device 0 if device 0 turns out to be in prohibited mode or in
                           exclusive-process mode and used by another process.
                           <samp class="ph codeph">cudaSetValidDevices()</samp> can be used to set a device from a
                           prioritized list of devices. 
                        </p>
                        <p class="p"> Note also that, for devices featuring the Pascal architecture onwards (compute
                           capability with major revision number 6 and higher), there exists support for
                           Compute Preemption. This allows compute tasks to be preempted at instruction-level
                           granularity, rather than thread block granularity as in prior Maxwell and Kepler GPU
                           architecture, with the benefit that applications with long-running kernels can be
                           prevented from either monopolizing the system or timing out. However, there will be
                           context switch overheads associated with Compute Preemption, which is automatically
                           enabled on those devices for which support exists. The individual attribute query
                           function <samp class="ph codeph">cudaDeviceGetAttribute()</samp> with the attribute
                           <samp class="ph codeph">cudaDevAttrComputePreemptionSupported</samp> can be used to determine
                           if the device in use supports Compute Preemption. Users wishing to avoid context
                           switch overheads associated with different processes can ensure that only one
                           process is active on the GPU by selecting exclusive-process mode. 
                        </p>
                        <p class="p"> Applications may query the compute mode of a device by checking the
                           <samp class="ph codeph">computeMode</samp> device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>). 
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="mode-switches"><a name="mode-switches" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#mode-switches" name="mode-switches" shape="rect">3.5.&nbsp;Mode Switches</a></h3>
                     <div class="body conbody">
                        <p class="p"> GPUs that have a display output dedicate some DRAM memory to the so-called
                           <dfn class="term">primary surface</dfn>, which is used to refresh the display device whose
                           output is viewed by the user. When users initiate a <dfn class="term">mode switch</dfn> of the
                           display by changing the resolution or bit depth of the display (using NVIDIA control
                           panel or the Display control panel on Windows), the amount of memory needed for the
                           primary surface changes. For example, if the user changes the display resolution
                           from 1280x1024x32-bit to 1600x1200x32-bit, the system must dedicate 7.68 MB to the
                           primary surface rather than 5.24 MB. (Full-screen graphics applications running with
                           anti-aliasing enabled may require much more display memory for the primary surface.)
                           On Windows, other events that may initiate display mode switches include launching a
                           full-screen DirectX application, hitting Alt+Tab to task switch away from a
                           full-screen DirectX application, or hitting Ctrl+Alt+Del to lock the computer. 
                        </p>
                        <p class="p">If a mode switch increases the amount of memory needed for the primary surface, the
                           system may have to cannibalize memory allocations dedicated to CUDA applications.
                           Therefore, a mode switch results in any call to the CUDA runtime to fail and return
                           an invalid context error.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="tesla-compute-cluster-mode-for-windows"><a name="tesla-compute-cluster-mode-for-windows" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#tesla-compute-cluster-mode-for-windows" name="tesla-compute-cluster-mode-for-windows" shape="rect">3.6.&nbsp;Tesla Compute Cluster Mode for Windows</a></h3>
                     <div class="body conbody">
                        <p class="p">Using NVIDIA's System Management Interface (<dfn class="term">nvidia-smi</dfn>), the Windows
                           device driver can be put in TCC (Tesla Compute Cluster) mode for devices of the
                           Tesla and Quadro Series of compute capability 2.0 and higher.
                        </p>
                        <p class="p">This mode has the following primary benefits:</p>
                        <ul class="ul">
                           <li class="li">It makes it possible to use these GPUs in cluster nodes with non-NVIDIA
                              integrated graphics;
                           </li>
                           <li class="li">It makes these GPUs available via Remote Desktop, both directly and via cluster
                              management systems that rely on Remote Desktop;
                           </li>
                           <li class="li">It makes these GPUs available to applications running as a Windows service
                              (i.e., in Session 0).
                           </li>
                        </ul>
                        <p class="p">However, the TCC mode removes support for any graphics functionality.</p>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="hardware-implementation"><a name="hardware-implementation" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#hardware-implementation" name="hardware-implementation" shape="rect">4.&nbsp;Hardware Implementation</a></h2>
                  <div class="body conbody">
                     <p class="p"> The NVIDIA GPU architecture is built around a scalable array of multithreaded
                        <dfn class="term">Streaming Multiprocessors</dfn> (<dfn class="term">SMs</dfn>). When a
                        CUDA program on the host CPU invokes a kernel grid, the blocks of the grid
                        are enumerated and distributed to multiprocessors with available execution
                        capacity. The threads of a thread block execute concurrently on one
                        multiprocessor, and multiple thread blocks can execute concurrently on one
                        multiprocessor. As thread blocks terminate, new blocks are launched on the
                        vacated multiprocessors. 
                     </p>
                     <p class="p">A multiprocessor is designed to execute hundreds of threads concurrently. To manage such a large
                        amount of threads, it employs a unique architecture called <dfn class="term">SIMT</dfn>
                        (<dfn class="term">Single-Instruction, Multiple-Thread</dfn>) that is described in
                        <a class="xref" href="index.html#simt-architecture" shape="rect">SIMT Architecture</a>. The instructions are pipelined, leveraging instruction-level
                        parallelism within a single thread, as well as extensive thread-level parallelism through
                        simultaneous hardware multithreading as detailed in <a class="xref" href="index.html#hardware-multithreading" shape="rect">Hardware Multithreading</a>. Unlike
                        CPU cores, they are issued in order and there is no branch prediction or speculative
                        execution.
                        
                     </p>
                     <p class="p"><a class="xref" href="index.html#simt-architecture" shape="rect">SIMT Architecture</a> and <a class="xref" href="index.html#hardware-multithreading" shape="rect">Hardware Multithreading</a> describe the architecture
                        features of the streaming multiprocessor that are common to all devices.
                        <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>,
                        <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a>,
                        <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>,
                        and <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a> provide the specifics for devices
                        of compute capabilities 3.x, 5.x, 6.x, and 7.x respectively. 
                     </p>
                     <p class="p">The NVIDIA GPU architecture uses a little-endian representation.</p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="simt-architecture"><a name="simt-architecture" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#simt-architecture" name="simt-architecture" shape="rect">4.1.&nbsp;SIMT Architecture</a></h3>
                     <div class="body conbody">
                        <p class="p">The multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel
                           threads called <dfn class="term">warps</dfn>. Individual threads composing a warp start together at the
                           same program address, but they have their own instruction address counter and register
                           state and are therefore free to branch and execute independently. The term <dfn class="term">warp</dfn>
                           originates from weaving, the first parallel thread technology. A <dfn class="term">half-warp</dfn> is
                           either the first or second half of a warp. A <dfn class="term">quarter-warp</dfn> is either the first,
                           second, third, or fourth quarter of a warp.
                           
                        </p>
                        <p class="p">When a multiprocessor is given one or more thread blocks to execute, it partitions them into
                           warps and each warp gets scheduled by a <dfn class="term">warp scheduler</dfn> for execution. The way a
                           block is partitioned into warps is always the same; each warp contains threads of
                           consecutive, increasing thread IDs with the first warp containing thread 0. <a class="xref" href="index.html#thread-hierarchy" shape="rect">Thread Hierarchy</a>
                           describes how thread IDs relate to thread indices in the block.
                           
                        </p>
                        <p class="p">A warp executes one common instruction at a time, so full efficiency is realized when all 32 threads of a warp agree on their
                           execution path. If threads of a warp diverge via a data-dependent conditional branch, the warp executes each branch path taken,
                           disabling threads that are not on that path. Branch divergence occurs only within a warp; different warps execute independently
                           regardless of whether they are executing common or disjoint code paths.
                        </p>
                        <p class="p">The SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction
                           controls multiple processing elements. A key difference is that SIMD vector organizations expose the SIMD width to the software,
                           whereas SIMT instructions specify the execution and branching behavior of a single thread. In contrast with SIMD vector machines,
                           SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code
                           for coordinated threads. For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however,
                           substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge.
                           In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when
                           designing for correctness but must be considered in the code structure when designing for peak performance. Vector architectures,
                           on the other hand, require the software to coalesce loads into vectors and manage divergence manually.
                        </p>
                        <p class="p">Prior to Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask
                           specifying the active threads of the warp. As a result, threads from the same warp in divergent regions or different states
                           of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks
                           or mutexes can easily lead to deadlock, depending on which warp the contending threads come from.
                        </p>
                        <p class="p">Starting with the Volta architecture, <dfn class="term">Independent Thread Scheduling</dfn> allows full concurrency between threads, regardless of warp. With Independent Thread Scheduling, the GPU maintains execution
                           state per thread, including a program counter and call stack, and can yield execution at a per-thread granularity, either
                           to make better use of execution resources or to allow one thread to wait for data to be produced by another. A schedule optimizer
                           determines how to group active threads from the same warp together into SIMT units. This retains the high throughput of SIMT
                           execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity.
                        </p>
                        <p class="p">Independent Thread Scheduling can lead to a rather different set of threads participating in the executed code than intended
                           if the developer made assumptions about warp-synchronicity<a name="fnsrc_2" href="#fntarg_2" shape="rect"><sup>2</sup></a> of previous hardware architectures. In particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions)
                           should be revisited to ensure compatibility with Volta and beyond. See <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a> for further details.
                        </p>
                        <div class="section" id="simt-architecture__notes"><a name="simt-architecture__notes" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Notes</h3>
                           <p class="p">The threads of a warp that are participating in the current instruction are
                              called the <dfn class="term">active</dfn> threads, whereas threads not on the
                              current instruction are <dfn class="term">inactive</dfn> (disabled). Threads can be inactive
                              for a variety of reasons including having exited earlier than other threads of their warp,
                              having taken a different branch path than the branch path currently executed by the warp,
                              or being the last threads of a block whose number of threads is not a multiple of the warp size.
                           </p>
                           <p class="p">If a non-atomic instruction executed by a warp writes to the same location in global or shared memory for more than one of
                              the threads of the warp, the number of serialized writes that occur to that location varies depending on the compute capability
                              of the device (see <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>, <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a>, <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>, and <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a>), and which thread performs the final write is undefined.
                           </p>
                           <p class="p">If an <a class="xref" href="index.html#atomic-functions" shape="rect">atomic</a> instruction executed by a warp reads, modifies, and writes to the same location in global  memory for more than one of the
                              threads of the warp, each read/modify/write to that location occurs and they are all serialized, but the order in which they
                              occur is undefined.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="hardware-multithreading"><a name="hardware-multithreading" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#hardware-multithreading" name="hardware-multithreading" shape="rect">4.2.&nbsp;Hardware Multithreading</a></h3>
                     <div class="body conbody">
                        <p class="p">The execution context (program counters, registers, etc.) for each warp processed by a multiprocessor is maintained on-chip
                           during the entire lifetime of the warp. Therefore, switching from one execution context to another has no cost, and at every
                           instruction issue time, a warp scheduler selects a warp that has threads ready to execute its next instruction (the <a class="xref" href="index.html#simt-architecture__notes" shape="rect">active threads</a> of the warp) and issues the instruction to those threads.
                        </p>
                        <p class="p">In particular, each multiprocessor has a set of 32-bit registers that are partitioned among the warps, and a <dfn class="term">parallel data cache</dfn> or <dfn class="term">shared memory</dfn> that is partitioned among the thread blocks.
                        </p>
                        <p class="p">
                           The number of blocks and warps that can reside and be processed together on the multiprocessor for a given kernel depends
                           on the amount of registers and shared memory used by the kernel and the amount of registers and shared memory available on
                           the multiprocessor. There are also a maximum number of resident blocks and a maximum number of resident warps per multiprocessor.
                           These limits as well the amount of registers and shared memory available on the multiprocessor are a function of the compute
                           capability of the device and are given in Appendix <a class="xref" href="index.html#compute-capabilities" shape="rect">Compute Capabilities</a>. If there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel
                           will fail to launch.
                        </p>
                        <p class="p">The total number of warps in a block is as follows:</p>
                        <p class="p d4p_eqn_block">
                           <math xmlns="http://www.w3.org/1998/Math/MathML">
                              <mrow>
                                 <mtext>ceil</mtext>
                                 <mrow>
                                    <mo>(</mo>
                                    <mfrac>
                                       <mrow>
                                          <mi>T</mi>
                                       </mrow>
                                       <mrow>
                                          <msub>
                                             <mrow>
                                                <mi>W</mi>
                                             </mrow>
                                             <mrow>
                                                <mi>s</mi>
                                                <mi>i</mi>
                                                <mi>z</mi>
                                                <mi>e</mi>
                                             </mrow>
                                          </msub>
                                       </mrow>
                                    </mfrac>
                                    <mo>,</mo>
                                    <mn>1</mn>
                                    <mo>)</mo>
                                 </mrow>
                              </mrow>
                           </math>
                        </p>
                        <ul class="ul">
                           <li class="li"><em class="ph i">T</em> is the number of threads per block,
                           </li>
                           <li class="li"><em class="ph i">W<sub class="ph sub">size</sub></em> is the warp size, which is equal to 32,
                           </li>
                           <li class="li">ceil(x, y) is equal to x rounded up to the nearest multiple of y.</li>
                        </ul>
                        <p class="p">The total number of registers and total amount of shared memory allocated for a block are documented in the CUDA Occupancy
                           Calculator provided in the CUDA Toolkit.
                        </p>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="performance-guidelines"><a name="performance-guidelines" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#performance-guidelines" name="performance-guidelines" shape="rect">5.&nbsp;Performance Guidelines</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="overall-performance-optimization-strategies"><a name="overall-performance-optimization-strategies" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#overall-performance-optimization-strategies" name="overall-performance-optimization-strategies" shape="rect">5.1.&nbsp;Overall Performance Optimization Strategies</a></h3>
                     <div class="body conbody">
                        <p class="p">Performance optimization revolves around three basic strategies:</p>
                        <ul class="ul">
                           <li class="li">Maximize parallel execution to achieve maximum utilization;</li>
                           <li class="li">Optimize memory usage to achieve maximum memory throughput;</li>
                           <li class="li">Optimize instruction usage to achieve maximum instruction
                              throughput.
                           </li>
                           <li class="li">Mimize memory thrashing.</li>
                        </ul>
                        <p class="p">Which strategies will yield the best performance gain for a particular
                           portion of an application depends on the performance limiters for that
                           portion; optimizing instruction usage of a kernel that is mostly limited
                           by memory accesses will not yield any significant performance gain, for
                           example. Optimization efforts should therefore be constantly directed by
                           measuring and monitoring the performance limiters, for example using the
                           CUDA profiler. Also, comparing the floating-point operation throughput or
                           memory throughput - whichever makes more sense - of a particular kernel
                           to the corresponding peak theoretical throughput of the device indicates
                           how much room for improvement there is for the kernel.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="maximize-utilization"><a name="maximize-utilization" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#maximize-utilization" name="maximize-utilization" shape="rect">5.2.&nbsp;Maximize Utilization</a></h3>
                     <div class="body conbody">
                        <p class="p">To maximize utilization the application should be structured in a way that it exposes as much parallelism as possible and
                           efficiently maps this parallelism to the various components of the system to keep them busy most of the time.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="application-level"><a name="application-level" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#application-level" name="application-level" shape="rect">5.2.1.&nbsp;Application Level</a></h3>
                        <div class="body conbody">
                           <p class="p">At a high level, the application should maximize parallel execution between the host, the devices, and the bus connecting
                              the host to the devices, by using asynchronous functions calls and streams as described in <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>. It should assign to each processor the type of work it does best: serial workloads to the host; parallel workloads to the
                              devices.
                           </p>
                           <p class="p">For the parallel workloads, at points in the algorithm where parallelism is broken because some
                              threads need to synchronize in order to share data with each other, there are two cases:
                              Either these threads belong to the same block, in which case they should use
                              <samp class="ph codeph">__syncthreads()</samp> and share data through shared memory within the
                              same kernel invocation, or they belong to different blocks, in which case they must
                              share data through global memory using two separate kernel invocations, one for writing
                              to and one for reading from global memory. The second case is much less optimal since it
                              adds the overhead of extra kernel invocations and global memory traffic. Its occurrence
                              should therefore be minimized by mapping the algorithm to the CUDA programming model in
                              such a way that the computations that require inter-thread communication are performed
                              within a single thread block as much as possible.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="device-level"><a name="device-level" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#device-level" name="device-level" shape="rect">5.2.2.&nbsp;Device Level</a></h3>
                        <div class="body conbody">
                           <p class="p">At a lower level, the application should maximize parallel execution between the multiprocessors of a device.</p>
                           <p class="p">Multiple kernels can execute concurrently on a device, so maximum utilization can also be achieved by using streams to enable
                              enough kernels to execute concurrently as described in <a class="xref" href="index.html#asynchronous-concurrent-execution" shape="rect">Asynchronous Concurrent Execution</a>.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="multiprocessor-level"><a name="multiprocessor-level" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#multiprocessor-level" name="multiprocessor-level" shape="rect">5.2.3.&nbsp;Multiprocessor Level</a></h3>
                        <div class="body conbody">
                           <p class="p">At an even lower level, the application should maximize parallel execution between the various functional units within a multiprocessor.</p>
                           <p class="p">As described in <a class="xref" href="index.html#hardware-multithreading" shape="rect">Hardware Multithreading</a>, a GPU
                              multiprocessor primarily relies on thread-level parallelism to maximize utilization of its functional
                              units. Utilization is therefore directly linked to the number of resident warps. 
                              
                              At every instruction issue time, a warp scheduler selects an instruction that is ready to execute.
                              This instruction can be another independent instruction of the same warp, exploiting
                              instruction-level parallelism, or more commonly an instruction of another warp, exploiting
                              thread-level parallelism. If a ready to execute instruction is selected it is issued to the
                              <a class="xref" href="index.html#simt-architecture__notes" shape="rect">active</a> threads of the warp.
                              The number of clock cycles it takes for a warp to be ready to execute its next instruction is called
                              the <dfn class="term">latency</dfn>, and full utilization is achieved when all warp schedulers always have some
                              instruction to issue for some warp at every clock cycle during that latency period, or in other
                              words, when latency is completely "hidden". The number of instructions required to hide a latency of
                              L clock cycles depends on the respective throughputs of these instructions (see
                              <a class="xref" href="index.html#arithmetic-instructions" shape="rect">Arithmetic Instructions</a> for the throughputs of various arithmetic instructions). If
                              we assume instructions with maximum throughput, it is equal to:
                           </p>
                           <ul class="ul">
                              <li class="li"><em class="ph i">4L</em> for devices of compute capability 5.x, 6.1, 6.2, 7.x and 8.x since for these devices, a 
                                 multiprocessor issues one instruction per warp over one clock cycle for four warps at a time,
                                 as mentioned in <a class="xref" href="index.html#compute-capabilities" shape="rect">Compute Capabilities</a>.
                              </li>
                              <li class="li"><em class="ph i">2L</em> for devices of compute capability 6.0 since for these devices, the two instructions
                                 issued every cycle are one instruction for two different warps.
                              </li>
                              <li class="li"><em class="ph i">8L</em> for devices of compute capability 3.x since for these devices, the eight
                                 instructions issued every cycle are four pairs for four different warps, each pair being for
                                 the same warp.
                              </li>
                           </ul>
                           <p class="p">The most common reason a warp is not ready to execute its next instruction is that the instruction's input operands are not
                              available yet.
                           </p>
                           <p class="p">If all input operands are registers, latency is caused by register dependencies, i.e.,
                              some of the input operands are written by some previous instruction(s) whose execution
                              has not completed yet. In this case, the latency is equal to the execution time of the previous 
                              instruction and the warp schedulers must schedule instructions of other warps during that time. 
                              Execution time varies depending on the instruction. On devices of compute capability 7.x, for
                              most arithmetic instructions, it is typically 4 clock cycles. This means that 16 active warps per 
                              multiprocessor (4 cycles, 4 warp schedulers) are required to hide arithmetic instruction
                              latencies (assuming that warps execute instructions with maximum throughput, otherwise fewer
                              warps are needed). If the individual warps exhibit instruction-level parallelism, i.e. have
                              multiple independent instructions in their instruction stream, fewer warps are needed because
                              multiple independent instructions from a single warp can be issued back to back.
                              
                           </p>
                           <p class="p">
                              If some input operand resides in off-chip memory, the latency is much higher: typically hundreds of
                              clock cycles. The number of warps required to keep the warp schedulers busy during such high latency
                              periods depends on the kernel code and its degree of instruction-level parallelism. In general, more
                              warps are required if the ratio of the number of instructions with no off-chip memory operands (i.e.,
                              arithmetic instructions most of the time) to the number of instructions with off-chip memory operands
                              is low (this ratio is commonly called the arithmetic intensity of the program).
                              
                           </p>
                           <p class="p">Another reason a warp is not ready to execute its next instruction is that it is waiting at some memory fence (<a class="xref" href="index.html#memory-fence-functions" shape="rect">Memory Fence Functions</a>) or synchronization point (<a class="xref" href="index.html#memory-fence-functions" shape="rect">Memory Fence Functions</a>). A synchronization point can force the multiprocessor to idle as more and more warps wait for other warps in the same block
                              to complete execution of instructions prior to the synchronization point. Having multiple resident blocks per multiprocessor
                              can help reduce idling in this case, as warps from different blocks do not need to wait for each other at synchronization
                              points.
                           </p>
                           <p class="p">The number of blocks and warps residing on each multiprocessor for a given kernel call depends on the execution configuration
                              of the call (<a class="xref" href="index.html#execution-configuration" shape="rect">Execution Configuration</a>), the memory resources of the multiprocessor, and the resource requirements of the kernel as described in <a class="xref" href="index.html#hardware-multithreading" shape="rect">Hardware Multithreading</a>. Register and shared memory usage are reported by the compiler when compiling with the <samp class="ph codeph">-ptxas-options=-v</samp> option.
                              
                           </p>
                           <p class="p">The total amount of shared memory required for a block is equal to the sum of the amount of statically allocated shared memory
                              and the amount of dynamically allocated shared memory.
                           </p>
                           <p class="p">The number of registers used by a kernel can have a significant impact on the number of resident warps. For example, for devices
                              of compute capability 6.x, if a kernel uses 64 registers and each block has 512 threads and requires very little shared memory,
                              then two blocks (i.e., 32 warps) can reside on the multiprocessor since they require 2x512x64 registers, which exactly matches
                              the number of registers available on the multiprocessor. But as soon as the kernel uses one more register, only one block
                              (i.e., 16 warps) can be resident since two blocks would require 2x512x65 registers, which are more registers than are available
                              on the multiprocessor. Therefore, the compiler attempts to minimize register usage while keeping register spilling (see <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>) and the number of instructions to a minimum. Register usage can be controlled using the <samp class="ph codeph">maxrregcount</samp> compiler option or launch bounds as described in <a class="xref" href="index.html#launch-bounds" shape="rect">Launch Bounds</a>.
                              
                           </p>
                           <p class="p">
                              The register file is organized as 32-bit registers. So, each variable stored in a register needs
                              at least one 32-bit register, e.g. a <samp class="ph codeph">double</samp> variable uses two 32-bit registers.
                              
                           </p>
                           <p class="p">The effect of execution configuration on performance for a given kernel call generally depends on the kernel code. Experimentation
                              is therefore recommended. Applications can also parameterize execution configurations based on register file size and shared
                              memory size, which depends on the compute capability of the device, as well as on the number of multiprocessors and memory
                              bandwidth of the device, all of which can be queried using the runtime (see reference manual).
                           </p>
                           <p class="p">The number of threads per block should be chosen as a multiple of the warp size to avoid wasting computing resources with
                              under-populated warps as much as possible.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="occupancy-calculator"><a name="occupancy-calculator" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#occupancy-calculator" name="occupancy-calculator" shape="rect">5.2.3.1.&nbsp;Occupancy Calculator</a></h3>
                           <div class="body conbody">
                              <p class="p">Several API functions exist to assist programmers in choosing thread block size based on register and shared memory requirements.</p>
                              <ul class="ul">
                                 <li class="li">
                                    The occupancy calculator API, <samp class="ph codeph">cudaOccupancyMaxActiveBlocksPerMultiprocessor</samp>, can provide an occupancy prediction based on the block size and shared memory usage of a kernel. This function reports occupancy
                                    in terms of the number of concurrent thread blocks per multiprocessor.
                                    
                                    <ul class="ul">
                                       <li class="li">
                                          Note that this value can be converted to other metrics.  Multiplying by the number of warps per block yields the number of
                                          concurrent warps per multiprocessor; further dividing concurrent warps by max warps per multiprocessor gives the occupancy
                                          as a percentage.
                                          
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li">
                                    The occupancy-based launch configurator APIs, <samp class="ph codeph">cudaOccupancyMaxPotentialBlockSize</samp> and <samp class="ph codeph">cudaOccupancyMaxPotentialBlockSizeVariableSMem</samp>, heuristically calculate an execution configuration that achieves the maximum multiprocessor-level occupancy.
                                    
                                 </li>
                              </ul>
                              <p class="p">The following code sample calculates the occupancy of MyKernel. It then reports the occupancy level with the ratio between
                                 concurrent warps versus maximum warps per multiprocessor.
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *d, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *b)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> idx = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x;
    d[idx] = a[idx] * b[idx];
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> numBlocks;        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Occupancy in terms of active blocks</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> blockSize = 32;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// These variables are used to convert occupancy to warps</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> device;
    cudaDeviceProp prop;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> activeWarps;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> maxWarps;

    cudaGetDevice(&amp;device);
    cudaGetDeviceProperties(&amp;prop, device);
    
    cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &amp;numBlocks,
        MyKernel,
        blockSize,
        0);

    activeWarps = numBlocks * blockSize / prop.warpSize;
    maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;

    std::cout &lt;&lt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Occupancy: "</span> &lt;&lt; (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>)activeWarps / maxWarps * 100 &lt;&lt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"%"</span> &lt;&lt; std::endl;
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre><p class="p">The following code sample configures an occupancy-based kernel launch of MyKernel according to the user input.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *array, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> arrayCount)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> idx = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (idx &lt; arrayCount) {
        array[idx] *= array[idx];
    }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> launchMyKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *array, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> arrayCount)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> blockSize;      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The launch configurator returned block size</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> minGridSize;    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The minimum grid size needed to achieve the</span>
                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// maximum occupancy for a full device</span>
                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// launch</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> gridSize;       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The actual grid size needed, based on input</span>
                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// size</span>

    cudaOccupancyMaxPotentialBlockSize(
        &amp;minGridSize,
        &amp;blockSize,
        (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)MyKernel,
        0,
        arrayCount);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Round up according to array size</span>
    gridSize = (arrayCount + blockSize - 1) / blockSize;

    MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>gridSize, blockSize<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(array, arrayCount);
    cudaDeviceSynchronize();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// If interested, the occupancy can be calculated with</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cudaOccupancyMaxActiveBlocksPerMultiprocessor</span>

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre><p class="p">The CUDA Toolkit also provides a self-documenting, standalone occupancy calculator and launch configurator implementation
                                 in <samp class="ph codeph">&lt;CUDA_Toolkit_Path&gt;/include/cuda_occupancy.h</samp> for any use cases that cannot depend on the CUDA software stack. A spreadsheet version of the occupancy calculator is also
                                 provided. The spreadsheet version is particularly useful as a learning tool that visualizes the impact of changes to the parameters
                                 that affect occupancy (block size, registers per thread, and shared memory per thread).
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="maximize-memory-throughput"><a name="maximize-memory-throughput" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#maximize-memory-throughput" name="maximize-memory-throughput" shape="rect">5.3.&nbsp;Maximize Memory Throughput</a></h3>
                     <div class="body conbody">
                        <p class="p">The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth.</p>
                        <p class="p">That means minimizing data transfers between the host and the device, as detailed in <a class="xref" href="index.html#data-transfer-between-host-and-device" shape="rect">Data Transfer between Host and Device</a>, since these have much lower bandwidth than data transfers between global memory and the device.
                        </p>
                        <p class="p">That also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared
                           memory and caches (i.e., L1 cache and L2 cache available on devices of compute capability 2.x and higher, texture cache and
                           constant cache available on all devices).
                        </p>
                        <p class="p">Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it. As illustrated
                           in <a class="xref" href="index.html#cuda-c-runtime" shape="rect">CUDA Runtime</a>, a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each
                           thread of a block:
                           
                        </p>
                        <ul class="ul">
                           <li class="li">Load data from device memory to shared memory,</li>
                           <li class="li">Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were
                              populated by different threads,
                           </li>
                           <li class="li">Process the data in shared memory,</li>
                           <li class="li">Synchronize again if necessary to make sure that shared memory has been updated with the results,</li>
                           <li class="li">Write the results back to device memory.</li>
                        </ul>
                        <p class="p">For some applications (e.g., for which global memory access patterns are data-dependent), a traditional 
                           hardware-managed cache is more appropriate to exploit data locality. As mentioned in 
                           <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>, <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a> and 
                           <a class="xref" href="index.html#compute-capability-8-x" shape="rect">Compute Capability 8.x</a>, for devices of compute capability 3.x, 7.x and 8.x, 
                           the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus 
                           shared memory is configurable for each kernel call.
                        </p>
                        <p class="p">The throughput of memory accesses by a kernel can vary by an order of magnitude depending on
                           access pattern for each type of memory. The next step in maximizing memory throughput is therefore to
                           organize memory accesses as optimally as possible based on the optimal memory access patterns
                           described in <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>. This optimization is especially important for
                           global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and
                           arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact
                           on performance.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="data-transfer-between-host-and-device"><a name="data-transfer-between-host-and-device" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#data-transfer-between-host-and-device" name="data-transfer-between-host-and-device" shape="rect">5.3.1.&nbsp;Data Transfer between Host and Device</a></h3>
                        <div class="body conbody">
                           <p class="p">Applications should strive to minimize data transfer between the host and the device. One way to 
                              accomplish this is to move more code from the host to the device, even if that means running kernels
                              that do not expose enough parallelism to execute on the device with full efficiency. Intermediate
                              data structures may be created in device memory, operated on by the device, and destroyed without
                              ever being mapped by the host or copied to host memory.
                           </p>
                           <p class="p">Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always
                              performs better than making each transfer separately.
                           </p>
                           <p class="p">On systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked
                              host memory as described in <a class="xref" href="index.html#page-locked-host-memory" shape="rect">Page-Locked Host Memory</a>.
                           </p>
                           <p class="p">In addition, when using mapped page-locked memory (<a class="xref" href="index.html#mapped-memory" shape="rect">Mapped Memory</a>), there is no need to allocate any device memory and explicitly copy data between device and host memory. Data transfers
                              are implicitly performed each time the kernel accesses the mapped memory. For maximum performance, these memory accesses must
                              be coalesced as with accesses to global memory (see <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>). Assuming that they are and that the mapped memory is read or written only once, using mapped page-locked memory instead
                              of explicit copies between device and host memory can be a win for performance.
                           </p>
                           <p class="p">On integrated systems where device memory and host memory are physically the same, any copy between host and device memory
                              is superfluous and mapped page-locked memory should be used instead. Applications may query a device is <samp class="ph codeph">integrated</samp> by checking that the integrated device property (see <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>) is equal to 1.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="device-memory-accesses"><a name="device-memory-accesses" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#device-memory-accesses" name="device-memory-accesses" shape="rect">5.3.2.&nbsp;Device Memory Accesses</a></h3>
                        <div class="body conbody">
                           <p class="p">An instruction that accesses addressable memory (i.e., global, local,
                              shared, constant, or texture memory) might need to be re-issued multiple
                              times depending on the distribution of the memory addresses across the
                              threads within the warp.  How the distribution affects the instruction
                              throughput this way is specific to each type of memory and described in
                              the following sections. For example, for global memory, as a general
                              rule, the more scattered the addresses are, the more reduced the
                              throughput is.
                           </p>
                           <div class="section">
                              <h4 class="title sectiontitle">Global Memory</h4>
                              <p class="p">Global memory resides in device memory and device memory is accessed
                                 via 32-, 64-, or 128-byte memory transactions. These memory
                                 transactions must be naturally aligned: Only the 32-, 64-, or 128-byte
                                 segments of device memory that are aligned to their size (i.e., whose
                                 first address is a multiple of their size) can be read or written by
                                 memory transactions.
                              </p>
                              <p class="p">When a warp executes an instruction that accesses global memory, it
                                 coalesces the memory accesses of the threads within the warp into one
                                 or more of these memory transactions depending on the size of the word
                                 accessed by each thread and the distribution of the memory addresses
                                 across the threads. In general, the more transactions are necessary,
                                 the more unused words are transferred in addition to the words accessed
                                 by the threads, reducing the instruction throughput accordingly. For
                                 example, if a 32-byte memory transaction is generated for each thread's
                                 4-byte access, throughput is divided by 8.
                              </p>
                              <p class="p">How many transactions are necessary and how much throughput is
                                 ultimately affected varies with the compute capability of the device.
                                 <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>,
                                 <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a>,
                                 <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>,
                                 <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a> and 
                                 <a class="xref" href="index.html#compute-capability-8-x" shape="rect">Compute Capability 8.x</a> give more details on how global
                                 memory accesses are handled for various compute capabilities.
                              </p>
                              <p class="p">To maximize global memory throughput, it is therefore important to
                                 maximize coalescing by:
                              </p>
                              <ul class="ul">
                                 <li class="li">Following the most optimal access patterns based on <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>, <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a>, <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>, <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a> and <a class="xref" href="index.html#compute-capability-8-x" shape="rect">Compute Capability 8.x</a></li>
                                 <li class="li">Using data types that meet the size and alignment requirement
                                    detailed in the section Size and Alignment Requirement below,
                                 </li>
                                 <li class="li">Padding data in some cases, for example, when accessing a
                                    two-dimensional array as described in the section Two-Dimensional Arrays below.
                                 </li>
                              </ul>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Size and Alignment Requirement</h4>
                              <p class="p">Global memory instructions support reading or writing words of size
                                 equal to 1, 2, 4, 8, or 16 bytes. Any access (via a variable or a
                                 pointer) to data residing in global memory compiles to a single global
                                 memory instruction if and only if the size of the data type is 1, 2, 4,
                                 8, or 16 bytes and the data is naturally aligned (i.e., its address is
                                 a multiple of that size).
                              </p>
                              <p class="p">If this size and alignment requirement is not fulfilled, the access
                                 compiles to multiple instructions with interleaved access patterns that
                                 prevent these instructions from fully coalescing. It is therefore
                                 recommended to use types that meet this requirement for data that
                                 resides in global memory.
                              </p>
                              <p class="p">The alignment requirement is automatically fulfilled for the 
                                 <a class="xref" href="index.html#built-in-vector-types" shape="rect">Built-in Vector Types</a>.
                              </p>
                              <p class="p">For structures, the size and alignment requirements can be enforced by
                                 the compiler using the alignment specifiers<samp class="ph codeph"> __align__(8) or
                                    __align__(16)</samp>, such as
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> __align__(8) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y;
};</pre><p class="p">or</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> __align__(16) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z;
};</pre><p class="p">Any address of a variable residing in global memory or returned by one
                                 of the memory allocation routines from the driver or runtime API is
                                 always aligned to at least 256 bytes.
                              </p>
                              <p class="p">Reading non-naturally aligned 8-byte or 16-byte words produces
                                 incorrect results (off by a few words), so special care must be taken
                                 to maintain alignment of the starting address of any value or array of
                                 values of these types. A typical case where this might be easily
                                 overlooked is when using some custom global memory allocation scheme,
                                 whereby the allocations of multiple arrays (with multiple calls to
                                 <samp class="ph codeph">cudaMalloc()</samp> or <samp class="ph codeph">cuMemAlloc()</samp>) is
                                 replaced by the allocation of a single large block of memory
                                 partitioned into multiple arrays, in which case the starting address of
                                 each array is offset from the block's starting address.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Two-Dimensional Arrays</h4>
                              <p class="p">A common global memory access pattern is when each thread of index
                                 (tx,ty) uses the following address to access one element of a 2D array
                                 of width width, located at address BaseAddress of type
                                 <samp class="ph codeph">type*</samp> (where type meets the requirement described in
                                 <a class="xref" href="index.html#maximize-utilization" shape="rect">Maximize Utilization</a>):
                              </p><pre xml:space="preserve">BaseAddress + width * ty + tx</pre><p class="p">For these accesses to be fully coalesced, both the width of the thread
                                 block and the width of the array must be a multiple of the warp size.
                              </p>
                              <p class="p">In particular, this means that an array whose width is not a multiple
                                 of this size will be accessed much more efficiently if it is actually
                                 allocated with a width rounded up to the closest multiple of this size
                                 and its rows padded accordingly. The cudaMallocPitch() and
                                 cuMemAllocPitch() functions and associated memory copy functions
                                 described in the reference manual enable programmers to write
                                 non-hardware-dependent code to allocate arrays that conform to these
                                 constraints.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Local Memory</h4>
                              <p class="p">Local memory accesses only occur for some automatic variables as
                                 mentioned in <a class="xref" href="index.html#variable-memory-space-specifiers" shape="rect">Variable Memory Space Specifiers</a>. Automatic
                                 variables that the compiler is likely to place in local memory are:
                              </p>
                              <ul class="ul">
                                 <li class="li">Arrays for which it cannot determine that they are indexed with
                                    constant quantities,
                                 </li>
                                 <li class="li">Large structures or arrays that would consume too much register
                                    space,
                                 </li>
                                 <li class="li">Any variable if the kernel uses more registers than available (this
                                    is also known as <em class="ph i">register spilling</em>).
                                 </li>
                              </ul>
                              <p class="p">Inspection of the <dfn class="term">PTX</dfn> assembly code (obtained by
                                 compiling with the <samp class="ph codeph">-ptx</samp> or<samp class="ph codeph">-keep</samp>
                                 option) will tell if a variable has been placed in local memory during
                                 the first compilation phases as it will be declared using the
                                 <samp class="ph codeph">.local</samp> mnemonic and accessed using the
                                 <samp class="ph codeph">ld.local</samp> and <samp class="ph codeph">st.local</samp> mnemonics. Even
                                 if it has not, subsequent compilation phases might still decide
                                 otherwise though if they find it consumes too much register space for
                                 the targeted architecture: Inspection of the <em class="ph i">cubin</em> object using
                                 <samp class="ph codeph">cuobjdump</samp> will tell if this is the case.  Also, the
                                 compiler reports total local memory usage per kernel
                                 (<samp class="ph codeph">lmem</samp>) when compiling with the
                                 <samp class="ph codeph">--ptxas-options=-v</samp> option. Note that some mathematical
                                 functions have implementation paths that might access local memory.
                              </p>
                              <p class="p">The local memory space resides in device memory, so local memory
                                 accesses have the same high latency and low bandwidth as global memory
                                 accesses and are subject to the same requirements for memory coalescing
                                 as described in <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>. Local memory
                                 is however organized such that consecutive 32-bit words are accessed by
                                 consecutive thread IDs. Accesses are therefore fully coalesced as long
                                 as all threads in a warp access the same relative address (e.g., same
                                 index in an array variable, same member in a structure variable).
                              </p>
                              <p class="p">On some devices of compute capability 3.x local memory accesses
                                 are always cached in L1 and L2 in the same way as global memory
                                 accesses (see <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>).
                              </p>
                              <p class="p">
                                 On devices of compute capability 5.x and 6.x, local memory accesses
                                 are always cached in L2 in the same way as global memory
                                 accesses (see <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a> and <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>).
                                 
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Shared Memory</h4>
                              <p class="p">Because it is on-chip, shared memory has much higher bandwidth and
                                 much lower latency than local or global memory.
                              </p>
                              <p class="p">To achieve high bandwidth, shared memory is divided into equally-sized
                                 memory modules, called banks, which can be accessed simultaneously. Any
                                 memory read or write request made of <em class="ph i">n</em> addresses that fall in
                                 <em class="ph i">n</em> distinct memory banks can therefore be serviced
                                 simultaneously, yielding an overall bandwidth that is <em class="ph i">n</em> times as
                                 high as the bandwidth of a single module.
                              </p>
                              <p class="p">However, if two addresses of a memory request fall in the same memory
                                 bank, there is a bank conflict and the access has to be serialized. The
                                 hardware splits a memory request with bank conflicts into as many
                                 separate conflict-free requests as necessary, decreasing throughput by
                                 a factor equal to the number of separate memory requests. If the number
                                 of separate memory requests is <em class="ph i">n</em>, the initial memory request is
                                 said to cause <em class="ph i">n</em>-way bank conflicts.
                              </p>
                              <p class="p">To get maximum performance, it is therefore important to understand
                                 how memory addresses map to memory banks in order to schedule the
                                 memory requests so as to minimize bank conflicts. This is described in
                                 <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>,  
                                 <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a>,  
                                 <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>, 
                                 <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a>, and  
                                 <a class="xref" href="index.html#compute-capability-8-x" shape="rect">Compute Capability 8.x</a> for devices of compute capability
                                 3.x, 5.x, 6.x, 7.x and 8.x, respectively.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Constant Memory</h4>
                              <p class="p">The constant memory space resides in device memory and is cached in
                                 the constant cache.
                              </p>
                              <p class="p">A request is then split into as many separate requests as there are
                                 different memory addresses in the initial request, decreasing
                                 throughput by a factor equal to the number of separate requests.
                              </p>
                              <p class="p">The resulting requests are then serviced at the throughput of the
                                 constant cache in case of a cache hit, or at the throughput of device
                                 memory otherwise.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Texture and Surface Memory</h4>
                              <p class="p">The texture and surface memory spaces reside in device memory and are
                                 cached in texture cache, so a texture fetch or surface read costs one
                                 memory read from device memory only on a cache miss, otherwise it just
                                 costs one read from texture cache. The texture cache is optimized for
                                 2D spatial locality, so threads of the same warp that read texture or
                                 surface addresses that are close together in 2D will achieve best
                                 performance. Also, it is designed for streaming fetches with a constant
                                 latency; a cache hit reduces DRAM bandwidth demand but not fetch
                                 latency.
                              </p>
                              <p class="p">Reading device memory through texture or surface fetching present some
                                 benefits that can make it an advantageous alternative to reading device
                                 memory from global or constant memory:
                              </p>
                              <ul class="ul">
                                 <li class="li">If the memory reads do not follow the access patterns that global
                                    or constant memory reads must follow to get good performance, higher
                                    bandwidth can be achieved providing that there is locality in the
                                    texture fetches or surface reads;
                                 </li>
                                 <li class="li">Addressing calculations are performed outside the kernel by
                                    dedicated units;
                                 </li>
                                 <li class="li">Packed data may be broadcast to separate variables in a single
                                    operation;
                                 </li>
                                 <li class="li">8-bit and 16-bit integer input data may be optionally converted to
                                    32 bit floating-point values in the range [0.0, 1.0] or [-1.0, 1.0]
                                    (see <a class="xref" href="index.html#texture-memory" shape="rect">Texture Memory</a>).
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="maximize-instruction-throughput"><a name="maximize-instruction-throughput" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#maximize-instruction-throughput" name="maximize-instruction-throughput" shape="rect">5.4.&nbsp;Maximize Instruction Throughput</a></h3>
                     <div class="body conbody">
                        <p class="p">To maximize instruction throughput the application should:</p>
                        <ul class="ul">
                           <li class="li">Minimize the use of arithmetic instructions with low throughput; this includes trading precision for speed when it does not
                              affect the end result, such as using intrinsic instead of regular functions (intrinsic functions are listed in <a class="xref" href="index.html#intrinsic-functions" shape="rect">Intrinsic Functions</a>), single-precision instead of double-precision, or flushing denormalized numbers to zero;
                           </li>
                           <li class="li">Minimize divergent warps caused by control flow instructions as detailed in <a class="xref" href="index.html#control-flow-instructions" shape="rect">Control Flow Instructions</a></li>
                           <li class="li">Reduce the number of instructions, for example, by optimizing out synchronization points whenever possible as described in
                              <a class="xref" href="index.html#synchronization-instruction" shape="rect">Synchronization Instruction</a> or by using restricted pointers as described in <a class="xref" href="index.html#restrict" shape="rect">__restrict__</a>.
                           </li>
                        </ul>
                        <p class="p">In this section, throughputs are given in number of operations per clock cycle per multiprocessor. For a warp size of 32,
                           one instruction corresponds to 32 operations, so if N is the number of operations per clock cycle, the instruction throughput
                           is N/32 instructions per clock cycle.
                        </p>
                        <p class="p">All throughputs are for one multiprocessor. They must be multiplied by the number of multiprocessors in the device to get
                           throughput for the whole device.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="arithmetic-instructions"><a name="arithmetic-instructions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#arithmetic-instructions" name="arithmetic-instructions" shape="rect">5.4.1.&nbsp;Arithmetic Instructions</a></h3>
                        <div class="body conbody">
                           <p class="p"><a class="xref" href="index.html#arithmetic-instructions__throughput-native-arithmetic-instructions" title="(Number of Results per Clock Cycle per Multiprocessor)" shape="rect">Table 3</a>
                              gives the throughputs of the arithmetic instructions that are natively
                              supported in hardware for devices of various compute capabilities.
                           </p>
                           <div class="tablenoborder"><a name="arithmetic-instructions__throughput-native-arithmetic-instructions" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="arithmetic-instructions__throughput-native-arithmetic-instructions" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 3. Throughput of Native Arithmetic Instructions</span>. <span class="desc tabledesc">(Number of Results per Clock Cycle per Multiprocessor)</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row" valign="middle">
                                       <th class="entry" rowspan="2" align="center" valign="middle" width="10%" id="d225e7778" colspan="1">&nbsp;</th>
                                       <th class="entry" colspan="9" align="center" valign="middle" id="d225e7780" rowspan="1">Compute Capability</th>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7786" rowspan="1" colspan="1">3.5, 3.7</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7789" rowspan="1" colspan="1">5.0, 5.2</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7792" rowspan="1" colspan="1">5.3</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7795" rowspan="1" colspan="1">6.0</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7798" rowspan="1" colspan="1">6.1</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7802" rowspan="1" colspan="1">6.2</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7805" rowspan="1" colspan="1">7.x</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7808" rowspan="1" colspan="1">8.0</th>
                                       <th class="entry" align="center" valign="middle" width="10%" id="d225e7811" rowspan="1" colspan="1">8.6</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          16-bit floating-point add, multiply, multiply-add
                                          
                                       </td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792" rowspan="1">N/A</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7795" rowspan="1" colspan="1">256</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">128</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7802" rowspan="1" colspan="1">2</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7805" rowspan="1" colspan="1">256</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7808" rowspan="1" colspan="1">128</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7811" rowspan="1">256<a name="fnsrc_3" href="#fntarg_3" shape="rect"><sup>3</sup></a></td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">32-bit floating-point add, multiply, multiply-add</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">192</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">128</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">64</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">128</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">64</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780" rowspan="1" colspan="1">128</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">64-bit floating-point add, multiply, multiply-add</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1"> 64<a name="fnsrc_4" href="#fntarg_4" shape="rect"><sup>4</sup></a></td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">4</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">4</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7808" rowspan="1" colspan="1">
                                          32<a name="fnsrc_5" href="#fntarg_5" shape="rect"><sup>5</sup></a></td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7811" rowspan="1" colspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780" rowspan="1" colspan="1">2</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          32-bit floating-point reciprocal, reciprocal square root, base-2
                                          logarithm (<samp class="ph codeph">__log2f</samp>), base 2 exponential
                                          (<samp class="ph codeph">exp2f</samp>), sine (<samp class="ph codeph">__sinf</samp>), cosine
                                          (<samp class="ph codeph">__cosf</samp>)
                                          
                                       </td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795" rowspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">16</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">32</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">16</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">32-bit integer add, extended-precision add, subtract, extended-precision subtract</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">160</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">128</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">64</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">128</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          32-bit integer multiply, multiply-add, extended-precision multiply-add
                                          
                                       </td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="5" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805" rowspan="1">Multiple instruct.</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">64<a name="fnsrc_6" href="#fntarg_6" shape="rect"><sup>6</sup></a></td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          24-bit integer multiply (<samp class="ph codeph">__[u]mul24</samp>)
                                          
                                       </td>
                                       <td class="entry" colspan="9" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805 d225e7808 d225e7811" rowspan="1">Multiple instruct.</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">32-bit integer shift</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1"> 64<a name="fnsrc_7" href="#fntarg_7" shape="rect"><sup>7</sup></a></td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">64</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="5" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">compare, minimum, maximum</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">160</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">64</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="5" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          32-bit integer bit reverse
                                          
                                       </td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">64</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">64</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">16</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          Bit field extract/insert
                                          
                                       </td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">64</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">64</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">Multiple Instruct.</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">32-bit bitwise AND, OR, XOR</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">160</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">128</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">64</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">128</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          count of leading zeros, most significant non-sign bit
                                          
                                       </td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795" rowspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">16</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">32</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">16</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          population count
                                          
                                       </td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795" rowspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">16</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">32</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">16</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">warp shuffle</td>
                                       <td class="entry" colspan="6" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805" rowspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7808" rowspan="1" colspan="1">
                                          32<a name="fnsrc_8" href="#fntarg_8" shape="rect"><sup>8</sup></a></td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7811" rowspan="1">32</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">warp reduce</td>
                                       <td class="entry" colspan="7" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805 d225e7808" rowspan="1">Multiple instruct.</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7811" rowspan="1">16</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          sum of absolute difference
                                          
                                       </td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">64</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="5" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          SIMD video instructions <samp class="ph codeph">vabsdiff2</samp></td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">160</td>
                                       <td class="entry" colspan="8" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805 d225e7808 d225e7811" rowspan="1">Multiple instruct.</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          SIMD video instructions <samp class="ph codeph">vabsdiff4</samp></td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">160</td>
                                       <td class="entry" colspan="5" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805" rowspan="1">Multiple instruct.</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">All other SIMD video instructions</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">32</td>
                                       <td class="entry" colspan="8" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795 d225e7798 d225e7802 d225e7805 d225e7808 d225e7811" rowspan="1">Multiple instruct.</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">
                                          Type conversions from 8-bit and 16-bit integer to 32-bit integer
                                          types
                                          
                                       </td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1">128</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">16</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">32</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">64</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">Type conversions from and to 64-bit types</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7789" rowspan="1" colspan="1"> 32<a name="fnsrc_9" href="#fntarg_9" shape="rect"><sup>9</sup></a></td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7792 d225e7795" rowspan="1">4</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">16</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">4</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7808" rowspan="1" colspan="1">
                                          16<a name="fnsrc_10" href="#fntarg_10" shape="rect"><sup>10</sup></a></td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7811" rowspan="1" colspan="1">16</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780" rowspan="1" colspan="1">2</td>
                                    </tr>
                                    <tr class="row" valign="middle">
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7778 d225e7786" rowspan="1" colspan="1">All other type conversions</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7789 d225e7792 d225e7795" rowspan="1">32</td>
                                       <td class="entry" align="center" valign="middle" width="10%" headers="d225e7780 d225e7798" rowspan="1" colspan="1">16</td>
                                       <td class="entry" colspan="2" align="center" valign="middle" headers="d225e7780 d225e7802 d225e7805" rowspan="1">32</td>
                                       <td class="entry" colspan="3" align="center" valign="middle" headers="d225e7780 d225e7808 d225e7811" rowspan="1">16</td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                           <p class="p">Other instructions and functions are implemented on top of the native
                              instructions. The implementation may be different for devices of
                              different compute capabilities, and the number of native instructions
                              after compilation may fluctuate with every compiler version. For
                              complicated functions, there can be multiple code paths depending on
                              input. <samp class="ph codeph">cuobjdump</samp> can be used to inspect a particular
                              implementation in a <samp class="ph codeph">cubin</samp> object.
                           </p>
                           <p class="p">The implementation of some functions are readily available on the CUDA
                              header files (<samp class="ph codeph">math_functions.h</samp>,
                              <samp class="ph codeph">device_functions.h</samp>, ...).
                           </p>
                           <p class="p">In general, code compiled with <samp class="ph codeph">-ftz=true</samp> (denormalized
                              numbers are flushed to zero) tends to have higher performance than code
                              compiled with <samp class="ph codeph">-ftz=false</samp>. Similarly, code compiled with
                              <samp class="ph codeph">-prec div=false</samp> (less precise division) tends to have
                              higher performance code than code compiled with <samp class="ph codeph">-prec
                                 div=true</samp>, and code compiled with
                              <samp class="ph codeph">-prec-sqrt=false</samp> (less precise square root) tends to
                              have higher performance than code compiled with
                              <samp class="ph codeph">-prec-sqrt=true</samp>. The nvcc user manual describes these
                              compilation flags in more details.
                           </p>
                           <div class="section">
                              <h4 class="title sectiontitle">Single-Precision Floating-Point Division</h4>
                              <p class="p"><samp class="ph codeph">__fdividef(x, y)</samp> (see <a class="xref" href="index.html#intrinsic-functions" shape="rect">Intrinsic Functions</a>) provides faster single-precision
                                 floating-point division than the division operator.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Single-Precision Floating-Point Reciprocal Square Root</h4>
                              <p class="p">To preserve IEEE-754 semantics the compiler can optimize
                                 <samp class="ph codeph">1.0/sqrtf()</samp> into <samp class="ph codeph">rsqrtf()</samp> only when
                                 both reciprocal and square root are approximate, (i.e., with
                                 <samp class="ph codeph">-prec-div=false</samp> and
                                 <samp class="ph codeph">-prec-sqrt=false</samp>). It is therefore recommended to
                                 invoke <samp class="ph codeph">rsqrtf()</samp> directly where desired.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Single-Precision Floating-Point Square Root</h4>
                              <p class="p">Single-precision floating-point square root is implemented as a
                                 reciprocal square root followed by a reciprocal instead of a reciprocal
                                 square root followed by a multiplication so that it gives correct
                                 results for 0 and infinity.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Sine and Cosine</h4>
                              <p class="p"><samp class="ph codeph">sinf(x)</samp>, <samp class="ph codeph">cosf(x)</samp>,
                                 <samp class="ph codeph">tanf(x)</samp>, <samp class="ph codeph">sincosf(x)</samp>, and
                                 corresponding double-precision instructions are much more expensive and
                                 even more so if the argument x is large in magnitude.
                              </p>
                              <p class="p">More precisely, the argument reduction code (see <a class="xref" href="index.html#mathematical-functions" shape="rect">Mathematical Functions</a> for implementation) comprises two
                                 code paths referred to as the fast path and the slow path,
                                 respectively.
                              </p>
                              <p class="p">The fast path is used for arguments sufficiently small in magnitude
                                 and essentially consists of a few multiply-add operations. The slow
                                 path is used for arguments large in magnitude and consists of lengthy
                                 computations required to achieve correct results over the entire
                                 argument range.
                              </p>
                              <p class="p">At present, the argument reduction code for the trigonometric
                                 functions selects the fast path for arguments whose magnitude is less
                                 than <samp class="ph codeph">105615.0f</samp> for the single-precision functions, and
                                 less than <samp class="ph codeph">2147483648.0</samp> for the double-precision
                                 functions.
                              </p>
                              <p class="p">As the slow path requires more registers than the fast path, an
                                 attempt has been made to reduce register pressure in the slow path by
                                 storing some intermediate variables in local memory, which may affect
                                 performance because of local memory high latency and bandwidth (see
                                 <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>). At present, 28 bytes of local
                                 memory are used by single-precision functions, and 44 bytes are used by
                                 double-precision functions.  However, the exact amount is subject to
                                 change.
                              </p>
                              <p class="p">Due to the lengthy computations and use of local memory in the slow
                                 path, the throughput of these trigonometric functions is lower by one
                                 order of magnitude when the slow path reduction is required as opposed
                                 to the fast path reduction.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Integer Arithmetic</h4>
                              <p class="p">Integer division and modulo operation are costly as they compile to up to 20 instructions.
                                 They can be replaced with
                                 bitwise operations in some cases: If <samp class="ph codeph">n</samp> is a power of
                                 2, (<samp class="ph codeph">i/n</samp>) is equivalent to
                                 <samp class="ph codeph">(i&gt;&gt;log2(n))</samp> and <samp class="ph codeph">(i%n)</samp> is
                                 equivalent to (<samp class="ph codeph">i&amp;(n-1)</samp>); the compiler will perform
                                 these conversions if <samp class="ph codeph">n</samp> is literal.
                              </p>
                              <p class="p"><samp class="ph codeph">__brev</samp> and <samp class="ph codeph">__popc</samp> map to a single
                                 instruction and 
                                 <samp class="ph codeph">__brevll</samp> and <samp class="ph codeph">__popcll</samp> to a
                                 few instructions.
                              </p>
                              <p class="p"><samp class="ph codeph">__[u]mul24</samp> are legacy intrinsic functions that no longer have any reason to be used.
                                 
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Half Precision Arithmetic</h4>
                              <p class="p">In order to achieve good performance for 16-bit precision floating-point add, multiply or multiply-add,
                                 it is recommended that the <samp class="ph codeph">half2</samp> datatype is used for <samp class="ph codeph">half</samp> 
                                 precision and <samp class="ph codeph">__nv_bfloat162</samp> be used for <samp class="ph codeph">__nv_bfloat16</samp> precision. 
                                 Vector intrinsics (eg. <samp class="ph codeph">__hadd2</samp>, <samp class="ph codeph">__hsub2</samp>, 
                                 <samp class="ph codeph">__hmul2</samp>, <samp class="ph codeph">__hfma2</samp>) can then be used to do two operations
                                 in a single instruction. Using <samp class="ph codeph">half2</samp> or <samp class="ph codeph">__nv_bfloat162</samp> 
                                 in place of two calls using <samp class="ph codeph">half</samp> or <samp class="ph codeph">__nv_bfloat16</samp>
                                 may also help performance of other intrinsics, such as warp shuffles.
                              </p>
                              <p class="p">The intrinsic <samp class="ph codeph">__halves2half2</samp> is provided to convert two <samp class="ph codeph">half</samp> 
                                 precision values to the <samp class="ph codeph">half2</samp> datatype.
                              </p>
                              <p class="p">The intrinsic <samp class="ph codeph">__halves2bfloat162</samp> is provided to convert two <samp class="ph codeph">__nv_bfloat</samp> 
                                 precision values to the <samp class="ph codeph">__nv_bfloat162</samp> datatype.
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">Type Conversion</h4>
                              <p class="p">Sometimes, the compiler must insert conversion instructions,
                                 introducing additional execution cycles. This is the case for:
                              </p>
                              <ul class="ul">
                                 <li class="li">Functions operating on variables of type <samp class="ph codeph">char</samp> or
                                    <samp class="ph codeph">short</samp> whose operands generally need to be converted
                                    to <samp class="ph codeph">int</samp>,
                                 </li>
                                 <li class="li">Double-precision floating-point constants (i.e., those constants
                                    defined without any type suffix) used as input to single-precision
                                    floating-point computations (as mandated by C/C++ standards).
                                 </li>
                              </ul>
                              <p class="p">This last case can be avoided by using single-precision floating-point
                                 constants, defined with an <samp class="ph codeph">f</samp> suffix such as
                                 <samp class="ph codeph">3.141592653589793f</samp>, <samp class="ph codeph">1.0f</samp>,
                                 <samp class="ph codeph">0.5f</samp>.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="control-flow-instructions"><a name="control-flow-instructions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#control-flow-instructions" name="control-flow-instructions" shape="rect">5.4.2.&nbsp;Control Flow Instructions</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              
                              Any flow control instruction (<samp class="ph codeph">if</samp>, <samp class="ph codeph">switch</samp>, <samp class="ph codeph">do</samp>, <samp class="ph codeph">for</samp>, <samp class="ph codeph">while</samp>) can 
                              significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i.e., to follow
                              different execution paths). If this happens, the different executions paths have to be serialized, increasing the total number
                              of instructions executed for this warp.
                              
                           </p>
                           <p class="p">To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written
                              so as to minimize the number of divergent warps. This is possible because the distribution of the warps across the block is
                              deterministic as mentioned in <a class="xref" href="index.html#simt-architecture" shape="rect">SIMT Architecture</a>. A trivial example is when the controlling condition only depends on (<samp class="ph codeph">threadIdx / warpSize</samp>) where <samp class="ph codeph">warpSize</samp> is the warp size. In this case, no warp diverges since the controlling condition is perfectly aligned with the warps.
                              
                           </p>
                           <p class="p">Sometimes, the compiler may unroll loops or it may optimize out short <samp class="ph codeph">if</samp> or <samp class="ph codeph">switch</samp> blocks by using branch predication instead, as detailed below. In these cases, no warp can ever diverge. The programmer can
                              also control loop unrolling using the <samp class="ph codeph">#pragma unroll</samp> directive (see <a class="xref" href="index.html#pragma-unroll" shape="rect">#pragma unroll</a>).
                              
                           </p>
                           <p class="p">When using branch predication none of the instructions whose execution depends on the controlling condition gets skipped.
                              Instead, each of them is associated with a per-thread condition code or predicate that is set to true or false based on the
                              controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true
                              predicate are actually executed. Instructions with a false predicate do not write results, and also do not evaluate addresses
                              or read operands.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="synchronization-instruction"><a name="synchronization-instruction" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#synchronization-instruction" name="synchronization-instruction" shape="rect">5.4.3.&nbsp;Synchronization Instruction</a></h3>
                        <div class="body conbody">
                           <p class="p"> Throughput for <samp class="ph codeph">__syncthreads()</samp> is 128 operations per clock cycle for devices of compute capability 
                              3.x, 32 operations per clock cycle for devices of compute capability 6.0, 16 operations per clock cycle for devices 
                              of compute capability 7.x as well as 8.x and 64 operations per clock cycle for devices of compute capability 
                              5.x, 6.1 and 6.2.
                           </p>
                           <p class="p">
                              Note that <samp class="ph codeph">__syncthreads()</samp> can impact performance by forcing the multiprocessor
                              to idle as detailed in <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>.
                              
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="minimize-memory-thrashing"><a name="minimize-memory-thrashing" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#minimize-memory-thrashing" name="minimize-memory-thrashing" shape="rect">5.5.&nbsp;Minimize Memory Thrashing</a></h3>
                     <div class="body conbody">
                        <div class="p">Applications that constantly allocate and free memory too often may find that the
                           allocation calls tend to get slower over time up to a limit. This is typically expected due
                           to the nature of releasing memory back to the operating system for its own use. For best
                           performance in this regard, we recommend the following:
                           <ul class="ul">
                              <li class="li">Try to size your allocation to the problem at hand. Don't try to allocate all
                                 available memory with <samp class="ph codeph">cudaMalloc</samp> / <samp class="ph codeph">cudaMallocHost</samp> /
                                 <samp class="ph codeph">cuMemCreate</samp>, as this forces memory to be resident immediately and
                                 prevents other applications from being able to use that memory. This can put more
                                 pressure on operating system schedulers, or just prevent other applications using the
                                 same GPU from running entirely.
                              </li>
                              <li class="li">Try to allocate memory in appropriately sized allocations early in the application and
                                 allocations only when the application does not have any use for it. Reduce the number of
                                 <samp class="ph codeph">cudaMalloc</samp>+<samp class="ph codeph">cudaFree</samp> calls in the application,
                                 especially in performance-critical regions.
                              </li>
                              <li class="li">If an application cannot allocate enough device memory, consider falling back on other
                                 memory types such as <samp class="ph codeph">cudaMallocHost</samp> or
                                 <samp class="ph codeph">cudaMallocManaged</samp>, which may not be as performant, but will enable
                                 the application to make progress.
                              </li>
                              <li class="li">For platforms that support the feature, <samp class="ph codeph">cudaMallocManaged</samp> allows for
                                 oversubscription, and with the correct <samp class="ph codeph">cudaMemAdvise</samp> policies enabled,
                                 will allow the application to retain most if not all the performance of
                                 <samp class="ph codeph">cudaMalloc</samp>. <samp class="ph codeph">cudaMallocManaged</samp> also won't force an
                                 allocation to be resident until it is needed or prefetched, reducing the overall
                                 pressure on the operating system schedulers and better enabling multi-tenet use
                                 cases.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="cuda-enabled-gpus"><a name="cuda-enabled-gpus" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#cuda-enabled-gpus" name="cuda-enabled-gpus" shape="rect">A.&nbsp;CUDA-Enabled GPUs</a></h2>
                  <div class="body conbody">
                     <p class="p"><a class="xref" href="http://developer.nvidia.com/cuda-gpus" target="_blank" shape="rect">http://developer.nvidia.com/cuda-gpus</a> lists all
                        CUDA-enabled devices with their compute capability.
                     </p>
                     <p class="p">The compute capability, number of multiprocessors, clock frequency,
                        total amount of device memory, and other properties can be queried
                        using the runtime (see reference manual).
                     </p>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="c-language-extensions"><a name="c-language-extensions" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#c-language-extensions" name="c-language-extensions" shape="rect">B.&nbsp;C++ Language Extensions</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="function-declaration-specifiers"><a name="function-declaration-specifiers" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#function-declaration-specifiers" name="function-declaration-specifiers" shape="rect">B.1.&nbsp;Function Execution Space Specifiers</a></h3>
                     <div class="body conbody">
                        <p class="p">Function execution space specifiers denote whether a function executes on the host or on the device and whether it is callable
                           from the host or from the device.
                        </p>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="global"><a name="global" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#global" name="global" shape="rect">B.1.1.&nbsp;__global__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The <samp class="ph codeph">__global__</samp> execution space specifier declares a function as being a kernel. Such a function is:
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li">Executed on the device,</li>
                                 <li class="li">Callable from the host,</li>
                                 <li class="li">Callable from the device for devices of compute capability 3.2 or higher (see <a class="xref" href="index.html#cuda-dynamic-parallelism" shape="rect">CUDA Dynamic Parallelism</a> for more details).
                                 </li>
                              </ul>
                              <p class="p">
                                 A <samp class="ph codeph">__global__</samp> function must have void return type, and cannot be a member of a class.
                                 
                              </p>
                              <p class="p">
                                 Any call to a <samp class="ph codeph">__global__</samp> function must specify its execution configuration as described in <a class="xref" href="index.html#execution-configuration" shape="rect">Execution Configuration</a>.
                                 
                              </p>
                              <p class="p">
                                 A call to a <samp class="ph codeph">__global__</samp> function is asynchronous, meaning it returns before the device has completed its execution.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="device-function-specifier"><a name="device-function-specifier" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#device-function-specifier" name="device-function-specifier" shape="rect">B.1.2.&nbsp;__device__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The <samp class="ph codeph">__device__</samp> execution space specifier declares a function that is:
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li">Executed on the device,</li>
                                 <li class="li">Callable from the device only.</li>
                              </ul>
                              <p class="p">
                                 The <samp class="ph codeph">__global__</samp> and <samp class="ph codeph">__device__</samp> execution space specifiers cannot be used together.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="host"><a name="host" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#host" name="host" shape="rect">B.1.3.&nbsp;__host__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The <samp class="ph codeph">__host__</samp> execution space specifier declares a function that is:
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li">Executed on the host,</li>
                                 <li class="li">Callable from the host only.</li>
                              </ul>
                              <p class="p">
                                 It is equivalent to declare a function with only the <samp class="ph codeph">__host__</samp> execution space specifier or to declare it without any of the <samp class="ph codeph">__host__</samp>, <samp class="ph codeph">__device__</samp>, or <samp class="ph codeph">__global__</samp> execution space specifier; in either case the function is compiled for the host only.
                                 
                              </p>
                              <p class="p">
                                 The <samp class="ph codeph">__global__</samp> and <samp class="ph codeph">__host__</samp> execution space specifiers cannot be used together.
                                 
                              </p>
                              <p class="p">
                                 The <samp class="ph codeph">__device__</samp> and <samp class="ph codeph">__host__</samp> execution space specifiers can be used together however, in which case the function is compiled for both the host and the
                                 device. The <samp class="ph codeph">__CUDA_ARCH__</samp> macro introduced in <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a> can be used to differentiate code paths between host and device:
                                 
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> func()
{
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#if __CUDA_ARCH__ &gt;= 800</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code path for compute capability 8.x</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#elif __CUDA_ARCH__ &gt;= 700</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code path for compute capability 7.x</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#elif __CUDA_ARCH__ &gt;= 600</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code path for compute capability 6.x</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#elif __CUDA_ARCH__ &gt;= 500</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code path for compute capability 5.x</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#elif __CUDA_ARCH__ &gt;= 300</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code path for compute capability 3.x</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#elif !defined(__CUDA_ARCH__) </span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code path</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
}</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="cross-execution-undefined-behavior"><a name="cross-execution-undefined-behavior" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#cross-execution-undefined-behavior" name="cross-execution-undefined-behavior" shape="rect">B.1.4.&nbsp;Undefined behavior</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <div class="p">
                                 A 'cross-execution space' call has undefined behavior when:
                                 
                                 <ul class="ul">
                                    <li class="li">
                                       __CUDA_ARCH__ is defined, a call from within a
                                       __global__, __device__ or __host__ __device__ function
                                       to a __host__ function.
                                       
                                    </li>
                                    <li class="li">
                                       __CUDA_ARCH__ is undefined, a call from within a
                                       __host__ function to a __device__ function.
                                       <a name="fnsrc_11" href="#fntarg_11" shape="rect"><sup>11</sup></a></li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="noinline-and-forceinline"><a name="noinline-and-forceinline" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#noinline-and-forceinline" name="noinline-and-forceinline" shape="rect">B.1.5.&nbsp;__noinline__ and __forceinline__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The compiler inlines any <samp class="ph codeph">__device__</samp> function when deemed appropriate.
                                 
                              </p>
                              <p class="p">
                                 The <samp class="ph codeph">__noinline__</samp> function qualifier can be used as a hint for the compiler not to inline the function if possible.
                                 
                              </p>
                              <p class="p">
                                 The <samp class="ph codeph">__forceinline__</samp> function qualifier can be used to force the compiler to inline the function.
                                 
                              </p>
                              <p class="p"> The <samp class="ph codeph">__noinline__</samp> and <samp class="ph codeph">__forceinline__</samp> function qualifiers cannot be used together, and neither function
                                 qualifier can be applied to an inline function. 
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="variable-memory-space-specifiers"><a name="variable-memory-space-specifiers" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#variable-memory-space-specifiers" name="variable-memory-space-specifiers" shape="rect">B.2.&nbsp;Variable Memory Space Specifiers</a></h3>
                     <div class="body conbody">
                        <p class="p">Variable memory space specifiers denote the memory location on the device of a variable.</p>
                        <p class="p">
                           An automatic variable declared in device code without any of the <samp class="ph codeph">__device__</samp>,
                           <samp class="ph codeph">__shared__</samp> and <samp class="ph codeph">__constant__</samp> memory space specifiers described
                           in this section generally resides in a register. However in some cases the compiler
                           might choose to place it in local memory, which can have adverse performance
                           consequences as detailed in <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>.
                           
                        </p>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="device-variable-specifier"><a name="device-variable-specifier" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#device-variable-specifier" name="device-variable-specifier" shape="rect">B.2.1.&nbsp;__device__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The <samp class="ph codeph">__device__</samp> memory space specifier declares a variable that resides on the device.
                                 
                              </p>
                              <p class="p">At most one of the other memory space specifiers defined in the next three sections may be used together with <samp class="ph codeph">__device__</samp> to further denote which memory space the variable belongs to. If none of them is present, the variable:
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li">Resides in global memory space,</li>
                                 <li class="li">Has the lifetime of the CUDA context in which it is created,</li>
                                 <li class="li">Has a distinct object per device,</li>
                                 <li class="li">Is accessible from all the threads within the grid and from the host through the runtime library <samp class="ph codeph">(cudaGetSymbolAddress()</samp> / <samp class="ph codeph">cudaGetSymbolSize()</samp> / <samp class="ph codeph">cudaMemcpyToSymbol()</samp> / <samp class="ph codeph">cudaMemcpyFromSymbol()</samp>).
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="constant"><a name="constant" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#constant" name="constant" shape="rect">B.2.2.&nbsp;__constant__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The <samp class="ph codeph">__constant__</samp> memory space specifier, optionally used together with <samp class="ph codeph">__device__</samp>, declares a variable that:
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li">Resides in constant memory space,</li>
                                 <li class="li">Has the lifetime of the CUDA context in which it is created,</li>
                                 <li class="li">Has a distinct object per device,</li>
                                 <li class="li">Is accessible from all the threads within the grid and from the host through the runtime library (<samp class="ph codeph">cudaGetSymbolAddress()</samp> / <samp class="ph codeph">cudaGetSymbolSize()</samp> / <samp class="ph codeph">cudaMemcpyToSymbol()</samp> / <samp class="ph codeph">cudaMemcpyFromSymbol()</samp>).
                                    
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="shared"><a name="shared" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared" name="shared" shape="rect">B.2.3.&nbsp;__shared__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The <samp class="ph codeph">__shared__</samp> memory space specifier, optionally used together with <samp class="ph codeph">__device__</samp>, declares a variable that:
                              </p>
                              <ul class="ul">
                                 <li class="li">Resides in the shared memory space of a thread block,</li>
                                 <li class="li">Has the lifetime of the block,</li>
                                 <li class="li">Has a distinct object per block,</li>
                                 <li class="li">Is only accessible from all the threads within the block,</li>
                                 <li class="li">Does not have a constant address.</li>
                              </ul>
                              <p class="p">When declaring a variable in shared memory as an external array such as</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> shared[];
</pre><p class="p"> the size of the array is determined at launch time (see <a class="xref" href="index.html#execution-configuration" shape="rect">Execution Configuration</a>). All variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the
                                 array must be explicitly managed through offsets. For example, if one wants the equivalent of
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span> array0[128];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> array1[64];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>   array2[256];
</pre><p class="p">in dynamically allocated shared memory, one could declare and initialize the arrays the following way:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> array[];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> func()      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __device__ or __global__ function</span>
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span>* array0 = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span>*)array; 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* array1 = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)&amp;array0[128];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>*   array2 =   (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>*)&amp;array1[64];
}</pre><p class="p">Note that pointers need to be aligned to the type they point to, so the following code, for example, does not work since array1
                                 is not aligned to 4 bytes.
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> array[];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> func()      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __device__ or __global__ function</span>
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span>* array0 = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span>*)array; 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* array1 = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)&amp;array0[127];
}</pre><p class="p">Alignment requirements for the built-in vector types are listed in <a class="xref" href="index.html#vector-types__alignment-requirements-in-device-code" shape="rect">Table 4</a>.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="managed"><a name="managed" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#managed" name="managed" shape="rect">B.2.4.&nbsp;__managed__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 The <samp class="ph codeph">__managed__</samp> memory space specifier, optionally used together with <samp class="ph codeph">__device__</samp>, declares a variable that:
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li">
                                    Can be referenced from both device and host code, e.g., its address can
                                    be taken or it can be read or written directly from a device or host function.
                                    
                                 </li>
                                 <li class="li">Has the lifetime of an application.</li>
                              </ul>
                              
                              See <a class="xref" href="index.html#managed-specifier" shape="rect">__managed__ Memory Space Specifier</a> for more details.
                              
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="restrict"><a name="restrict" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#restrict" name="restrict" shape="rect">B.2.5.&nbsp;__restrict__</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p"><samp class="ph codeph">nvcc</samp> supports restricted pointers via the <samp class="ph codeph">__restrict__</samp>
                                 keyword.
                                 
                              </p>
                              <p class="p">Restricted pointers were introduced in C99 to alleviate the aliasing problem that exists in C-type languages, and which inhibits
                                 all kind of optimization from code re-ordering to common sub-expression elimination.
                              </p>
                              <p class="p">Here is an example subject to the aliasing issue, where use of restricted pointer can help the compiler to reduce the number
                                 of instructions:
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* a,
         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* b,
         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* c)
{
    c[0] = a[0] * b[0];
    c[1] = a[0] * b[0];
    c[2] = a[0] * b[0] * a[1];
    c[3] = a[0] * a[1];
    c[4] = a[0] * b[0];
    c[5] = b[0];
    ...
}</pre><p class="p">
                                 In C-type languages, the pointers <samp class="ph codeph">a</samp>, <samp class="ph codeph">b</samp>, and <samp class="ph codeph">c</samp> may be aliased, so any write through <samp class="ph codeph">c</samp> could modify elements of <samp class="ph codeph">a</samp> or <samp class="ph codeph">b</samp>. This means that to guarantee functional correctness, the compiler cannot load <samp class="ph codeph">a[0]</samp> and <samp class="ph codeph">b[0]</samp> into registers, multiply them, and store the result to both <samp class="ph codeph">c[0]</samp> and <samp class="ph codeph">c[1]</samp>, because the results would differ from the abstract execution model if, say, <samp class="ph codeph">a[0]</samp> is really the same location as <samp class="ph codeph">c[0]</samp>. So the compiler cannot take advantage of the common sub-expression. Likewise, the compiler cannot just reorder the computation
                                 of <samp class="ph codeph">c[4]</samp> into the proximity of the computation of <samp class="ph codeph">c[0]</samp> and <samp class="ph codeph">c[1]</samp> because the preceding write to <samp class="ph codeph">c[3]</samp> could change the inputs to the computation of <samp class="ph codeph">c[4]</samp>.
                                 
                              </p>
                              <p class="p">
                                 By making <samp class="ph codeph">a</samp>, <samp class="ph codeph">b</samp>, and <samp class="ph codeph">c</samp> restricted pointers, the programmer asserts to the compiler that the pointers are in fact not aliased, which in this case
                                 means writes through <samp class="ph codeph">c</samp> would never overwrite elements of <samp class="ph codeph">a</samp> or <samp class="ph codeph">b</samp>. This changes the function prototype as follows:
                                 
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> a,
         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> b,
         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> c);
</pre><p class="p">Note that all pointer arguments need to be made restricted for the compiler optimizer to derive any benefit. With the <samp class="ph codeph">__restrict__</samp> keywords added, the compiler can now reorder and do common sub-expression elimination at will, while retaining functionality
                                 identical with the abstract execution model:
                                 
                              </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> a,
         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> b,
         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> c)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> t0 = a[0];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> t1 = b[0];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> t2 = t0 * t1;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> t3 = a[1];
    c[0] = t2;
    c[1] = t2;
    c[4] = t2;
    c[2] = t2 * t3;
    c[3] = t0 * t3;
    c[5] = t1;
    ...
}
</pre><p class="p">The effects here are a reduced number of memory accesses and reduced number of computations. This is balanced by an increase
                                 in register pressure due to "cached" loads and common sub-expressions.
                              </p>
                              <p class="p">Since register pressure is a critical issue in many CUDA codes, use of restricted pointers can have negative performance impact
                                 on CUDA code, due to reduced occupancy.
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="built-in-vector-types"><a name="built-in-vector-types" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#built-in-vector-types" name="built-in-vector-types" shape="rect">B.3.&nbsp;Built-in Vector Types</a></h3>
                     <div class="topic reference nested2" xml:lang="en-US" id="vector-types"><a name="vector-types" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#vector-types" name="vector-types" shape="rect">B.3.1.&nbsp;char, short, int, long, longlong, float, double</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <div class="p">These are vector types derived from the basic integer and floating-point types. They are structures and the 1st, 2nd, 3rd,
                                 and 4th components are accessible through the fields <samp class="ph codeph">x</samp>, <samp class="ph codeph">y</samp>, <samp class="ph codeph">z</samp>, and <samp class="ph codeph">w</samp>, respectively. They all come with a constructor function of the form <samp class="ph codeph">make_&lt;type name&gt;</samp>; for example,
                                 <pre xml:space="preserve">
int2 make_int2(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y);
</pre>
                                 which creates a vector of type <samp class="ph codeph">int2</samp> with value<samp class="ph codeph">(x, y)</samp>.
                                 
                              </div>
                              <p class="p">The alignment requirements of the vector types are detailed in <a class="xref" href="index.html#vector-types__alignment-requirements-in-device-code" shape="rect">Table 4</a>.
                                 
                              </p>
                              <div class="tablenoborder"><a name="vector-types__alignment-requirements-in-device-code" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="vector-types__alignment-requirements-in-device-code" class="table" frame="border" border="1" rules="all">
                                    <caption><span class="tablecap">Table 4. Alignment Requirements</span></caption>
                                    <thead class="thead" align="left">
                                       <tr class="row">
                                          <th class="entry" valign="top" width="25%" id="d225e9417" rowspan="1" colspan="1">Type</th>
                                          <th class="entry" valign="top" width="75%" id="d225e9420" rowspan="1" colspan="1">Alignment</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">char1, uchar1</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">1</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">char2, uchar2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">2</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">char3, uchar3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">1</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">char4, uchar4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">short1, ushort1 </td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">2</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">short2, ushort2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">short3, ushort3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">2</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">short4, ushort4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">int1, uint1</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">int2, uint2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">int3, uint3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">int4, uint4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">long1, ulong1</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4 if sizeof(long) is equal to sizeof(int) 8, otherwise</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">long2, ulong2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8 if sizeof(long) is equal to sizeof(int), 16, otherwise </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">long3, ulong3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4 if sizeof(long) is equal to sizeof(int), 8, otherwise</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">long4, ulong4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">longlong1, ulonglong1</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">longlong2, ulonglong2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">longlong3, ulonglong3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">longlong4, ulonglong4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1"> float1</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">float2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">float3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">4</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">float4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">double1</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">double2</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">double3</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">8</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="25%" headers="d225e9417" rowspan="1" colspan="1">double4</td>
                                          <td class="entry" valign="top" width="75%" headers="d225e9420" rowspan="1" colspan="1">16</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="dim3"><a name="dim3" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#dim3" name="dim3" shape="rect">B.3.2.&nbsp;dim3</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 This type is an integer vector type based on <samp class="ph codeph">uint3</samp> that is used to specify dimensions. When defining a variable of type <samp class="ph codeph">dim3</samp>, any component left unspecified is initialized to 1.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="built-in-variables"><a name="built-in-variables" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#built-in-variables" name="built-in-variables" shape="rect">B.4.&nbsp;Built-in Variables</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Built-in variables specify the grid and block dimensions and the block and thread indices. They are only valid within functions
                              that are executed on the device.
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="griddim"><a name="griddim" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#griddim" name="griddim" shape="rect">B.4.1.&nbsp;gridDim</a></h3>
                        <div class="body refbody">
                           <div class="example">
                              <p class="p">This variable is of type <samp class="ph codeph">dim3</samp> (see <a class="xref" href="index.html#dim3" shape="rect">dim3</a>) and contains the dimensions of the grid.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="blockidx"><a name="blockidx" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#blockidx" name="blockidx" shape="rect">B.4.2.&nbsp;blockIdx</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 This variable is of type <samp class="ph codeph">uint3</samp> (see <a class="xref" href="index.html#vector-types" shape="rect">char, short, int, long, longlong, float, double</a>) and contains the block index within the grid.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="blockdim"><a name="blockdim" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#blockdim" name="blockdim" shape="rect">B.4.3.&nbsp;blockDim</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 This variable is of type <samp class="ph codeph">dim3</samp> (see <a class="xref" href="index.html#dim3" shape="rect">dim3</a>) and contains the dimensions of the block.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="threadidx"><a name="threadidx" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#threadidx" name="threadidx" shape="rect">B.4.4.&nbsp;threadIdx</a></h3>
                        <div class="body refbody">
                           <div class="example">
                              <p class="p">
                                 This variable is of type <samp class="ph codeph">uint3</samp> (see <a class="xref" href="index.html#vector-types" shape="rect">char, short, int, long, longlong, float, double</a> ) and contains the thread index within the block.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="warpsize"><a name="warpsize" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warpsize" name="warpsize" shape="rect">B.4.5.&nbsp;warpSize</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 This variable is of type <samp class="ph codeph">int</samp> and contains the warp size in threads (see <a class="xref" href="index.html#simt-architecture" shape="rect">SIMT Architecture</a> for the definition of a warp).
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="memory-fence-functions"><a name="memory-fence-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#memory-fence-functions" name="memory-fence-functions" shape="rect">B.5.&nbsp;Memory Fence Functions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">
                              The CUDA programming model assumes a device with a weakly-ordered memory model,
                              that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory
                              of a peer device
                              is not necessarily the order in which the data is observed being written by another CUDA or host thread.
                              It is undefined behaviour for two threads read from or write to the same memory location without synchronization.
                              
                           </p>
                           <div class="p">
                              In the following example, thread 1 executes <samp class="ph codeph">writeXY()</samp>, while thread 2 executes
                              <samp class="ph codeph">readXY()</samp>.
                              <pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> X = 1, Y = 2;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> writeXY()
{
    X = 10;
    Y = 20;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> readXY()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> B = Y;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> A = X;
}</pre>
                              The two threads read and write from the same memory locations <samp class="ph codeph">X</samp> and <samp class="ph codeph">Y</samp>
                              simultaneously. Any data-race is undefined behaviour, and has no defined semantics. The resulting
                              values for <samp class="ph codeph">A</samp> and <samp class="ph codeph">B</samp> can be anything.
                              
                           </div>
                           <p class="p">
                              Memory fence functions can be used to enforce some ordering on memory accesses.
                              The memory fence functions differ in the scope in which the orderings are enforced
                              but they are independent of the accessed memory space (shared memory, global memory,
                              page-locked host memory, and the memory of a peer device).
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __threadfence_block();</pre><div class="p">
                              ensures that:
                              
                              <ul class="ul">
                                 <li class="li">
                                    All writes to all memory made by the calling thread before the call to
                                    <samp class="ph codeph">__threadfence_block()</samp> are observed by all threads in the block of the calling thread
                                    as occurring before all writes to all memory made by the calling thread after the call to
                                    <samp class="ph codeph">__threadfence_block()</samp>;
                                    
                                 </li>
                                 <li class="li">
                                    All reads from all memory made by the calling thread before the call to
                                    <samp class="ph codeph">__threadfence_block()</samp> are ordered before all reads from all memory made by the calling thread after the call to
                                    <samp class="ph codeph">__threadfence_block()</samp>.
                                    
                                 </li>
                              </ul>
                           </div><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __threadfence();</pre><p class="p">
                              acts as <samp class="ph codeph">__threadfence_block()</samp> for all threads in the block of the calling thread and
                              also ensures that no writes to all memory made by the calling thread after the call to
                              <samp class="ph codeph">__threadfence()</samp> are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the
                              call to
                              <samp class="ph codeph">__threadfence()</samp>. Note that for this ordering guarantee to be true, the observing threads must truly observe the memory and not cached versions
                              of it;
                              this is ensured by using the <samp class="ph codeph">volatile</samp> keyword as detailed in <a class="xref" href="index.html#volatile-qualifier" shape="rect">Volatile Qualifier</a>.
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __threadfence_system();</pre><p class="p">
                              acts as <samp class="ph codeph">__threadfence_block()</samp> for all threads in the block of the calling thread and
                              also ensures that all writes to all memory made by the calling thread before the call to
                              <samp class="ph codeph">__threadfence_system()</samp> are observed by all threads in the device, host threads, and all threads in peer devices
                              as occurring before all writes to all memory
                              made by the calling thread after the call to <samp class="ph codeph">__threadfence_system()</samp>.
                              
                           </p>
                           <p class="p"><samp class="ph codeph">__threadfence_system()</samp> is only supported by devices of compute capability
                              2.x and higher.
                              
                           </p>
                           <div class="p">
                              In the previous code sample, we can insert fences in the codes as follows:
                              <pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> X = 1, Y = 2;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> writeXY()
{
    X = 10;
    __threadfence();
    Y = 20;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> readXY()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> B = Y;
    __threadfence();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> A = X;
}</pre>
                              For this code, the following outcomes can be observed:
                              <ul class="ul">
                                 <li class="li"><samp class="ph codeph">A</samp> equal to 1 and <samp class="ph codeph">B</samp> equal to 2,
                                 </li>
                                 <li class="li"><samp class="ph codeph">A</samp> equal to 10 and <samp class="ph codeph">B</samp> equal to 2,
                                 </li>
                                 <li class="li"><samp class="ph codeph">A</samp> equal to 10 and <samp class="ph codeph">B</samp> equal to 20.
                                 </li>
                              </ul>
                              
                              The fourth outcome is not possible, because the frist write must be visible
                              before the second write.
                              If thread 1 and 2 belong to the same block, it is enough to use <samp class="ph codeph">__threadfence_block()</samp>.
                              If thread 1 and 2 do not belong to the same block, <samp class="ph codeph">__threadfence()</samp> must be used if they are CUDA threads from the same device
                              and <samp class="ph codeph">__threadfence_system()</samp> must be used if they are CUDA threads from two different devices.
                              
                           </div>
                           <p class="p">A common use case is when threads consume some data produced by other threads as illustrated by
                              the following code sample of a kernel that computes the sum of an array of N numbers in
                              one call. Each block first sums a subset of the array and stores the result in global
                              memory. When all blocks are done, the last block done reads each of these partial sums
                              from global memory and sums them to obtain the final result. In order to determine which
                              block is finished last, each block atomically increments a counter to signal that it is
                              done with computing and storing its partial sum (see <a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a> about atomic
                              functions). The last block is the one that receives the counter value equal to
                              <samp class="ph codeph">gridDim.x-1</samp>. If no fence is placed between storing the partial sum
                              and incrementing the counter, the counter might increment before the partial sum is
                              stored and therefore, might reach <samp class="ph codeph">gridDim.x-1</samp> and let the last block
                              start reading partial sums before they have been actually updated in memory.
                              
                           </p>
                           <p class="p">Memory fence functions only affect the ordering of memory operations by a thread; they do not ensure that these memory operations
                              are visible to other threads
                              (like <samp class="ph codeph">__syncthreads()</samp> does for threads within a block (see <a class="xref" href="index.html#synchronization-functions" shape="rect">Synchronization Functions</a>)).
                              In the code sample below, the visibility of memory operations on the <samp class="ph codeph">result</samp> variable is ensured by declaring it as volatile (see <a class="xref" href="index.html#volatile-qualifier" shape="rect">Volatile Qualifier</a>).
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> count = 0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> bool isLastBlockDone;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> sum(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* array, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N,
                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">volatile</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* result)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each block sums a subset of the input array.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> partialSum = calculatePartialSum(array, N);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0) {

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread 0 of each block stores the partial sum</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// to global memory. The compiler will use </span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// a store operation that bypasses the L1 cache</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// since the "result" variable is declared as</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// volatile. This ensures that the threads of</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the last block will read the correct partial</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// sums computed by all other blocks.</span>
        result[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x] = partialSum;

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread 0 makes sure that the incrementation</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// of the "count" variable is only performed after</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the partial sum has been written to global memory.</span>
        __threadfence();

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread 0 signals that it is done.</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value = atomicInc(&amp;count, gridDim.x);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread 0 determines if its block is the last</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// block to be done.</span>
        isLastBlockDone = (value == (gridDim.x - 1));
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Synchronize to make sure that each thread reads</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the correct value of isLastBlockDone.</span>
    __syncthreads();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (isLastBlockDone) {

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The last block sums the partial sums</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// stored in result[0 .. gridDim.x-1]</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> totalSum = calculateTotalSum(result);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0) {

            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Thread 0 of last block stores the total sum</span>
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// to global memory and resets the count</span>
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// varialble, so that the next kernel call</span>
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// works properly.</span>
            result[0] = totalSum;
            count = 0;
        }
    }
}</pre></div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="synchronization-functions"><a name="synchronization-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#synchronization-functions" name="synchronization-functions" shape="rect">B.6.&nbsp;Synchronization Functions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __syncthreads();</pre><p class="p">waits until all threads in the thread block have reached this point and all global and shared
                              memory accesses made by these threads prior to <samp class="ph codeph">__syncthreads()</samp> are
                              visible to all threads in the block.
                              
                           </p>
                           <p class="p"><samp class="ph codeph">__syncthreads()</samp> is used to coordinate communication between the threads of
                              the same block. When some threads within a block access the same addresses in shared or
                              global memory, there are potential read-after-write, write-after-read, or
                              write-after-write hazards for some of these memory accesses. These data hazards can be
                              avoided by synchronizing threads in-between these accesses.
                              
                           </p>
                           <p class="p"><samp class="ph codeph">__syncthreads()</samp> is allowed in conditional code but only if the conditional
                              evaluates identically across the entire thread block, otherwise the code execution is
                              likely to hang or produce unintended side effects.
                              
                           </p>
                           <p class="p">Devices of compute capability 2.x and higher support three variations of
                              <samp class="ph codeph">__syncthreads()</samp> described below.
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __syncthreads_count(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> predicate);</pre><p class="p">
                              is identical to <samp class="ph codeph">__syncthreads()</samp> with the additional feature that it
                              evaluates predicate for all threads of the block and returns the number of threads for
                              which predicate evaluates to non-zero.
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __syncthreads_and(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> predicate);</pre><p class="p">
                              is identical to <samp class="ph codeph">__syncthreads()</samp> with the additional feature that it
                              evaluates predicate for all threads of the block and returns non-zero if and only if
                              predicate evaluates to non-zero for all of them.
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __syncthreads_or(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> predicate);</pre><p class="p">
                              is identical to <samp class="ph codeph">__syncthreads()</samp> with the additional feature that it
                              evaluates predicate for all threads of the block and returns non-zero if and only if
                              predicate evaluates to non-zero for any of them.
                              
                           </p><pre xml:space="preserve"> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __syncwarp(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask=0xffffffff);</pre><p class="p">
                              will cause the executing thread to wait until all warp lanes named in mask have
                              executed a <samp class="ph codeph">__syncwarp()</samp> (with the same mask) before resuming
                              execution. All non-exited threads named in mask must execute a corresponding
                              <samp class="ph codeph">__syncwarp()</samp> with the same mask, or the result is undefined.
                              
                           </p>
                           <p class="p">
                              Executing <samp class="ph codeph">__syncwarp()</samp> guarantees memory ordering among threads
                              participating in the barrier. Thus, threads within a warp that wish to communicate via
                              memory can store to memory, execute <samp class="ph codeph">__syncwarp()</samp>, and then safely read
                              values stored by other threads in the warp.
                              
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span> 
                              For .target sm_6x or below, all threads in mask must execute the same
                              <samp class="ph codeph">__syncwarp()</samp> in convergence, and the union of all values in mask must
                              be equal to the active mask. Otherwise, the behavior is undefined.
                              
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="mathematical-functions"><a name="mathematical-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#mathematical-functions" name="mathematical-functions" shape="rect">B.7.&nbsp;Mathematical Functions</a></h3>
                     <div class="body conbody">
                        <p class="p">The reference manual lists all C/C++ standard library mathematical functions that are supported in device code and all intrinsic
                           functions that are only supported in device code.
                        </p>
                        <p class="p"><a class="xref" href="index.html#mathematical-functions-appendix" shape="rect">Mathematical Functions</a> provides accuracy information for some of these functions when relevant.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="texture-functions"><a name="texture-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#texture-functions" name="texture-functions" shape="rect">B.8.&nbsp;Texture Functions</a></h3>
                     <div class="body conbody">
                        <p class="p">Texture objects are described in <a class="xref" href="index.html#texture-object-api" shape="rect">Texture Object API</a></p>
                        <p class="p">Texture references are described in <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a></p>
                        <p class="p">Texture fetching is described in <a class="xref" href="index.html#texture-fetching" shape="rect">Texture Fetching</a>.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="texture-object-api-appendix"><a name="texture-object-api-appendix" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#texture-object-api-appendix" name="texture-object-api-appendix" shape="rect">B.8.1.&nbsp;Texture Object API</a></h3>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dfetch-object"><a name="tex1dfetch-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dfetch-object" name="tex1dfetch-object" shape="rect">B.8.1.1.&nbsp;tex1Dfetch()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1Dfetch(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);
</pre><p class="p">fetches from the region of linear memory specified by the one-dimensional texture object <samp class="ph codeph">texObj</samp> using integer texture coordinate <samp class="ph codeph">x</samp>.
                                    			<samp class="ph codeph">tex1Dfetch()</samp> only works with non-normalized coordinates, so only the border and clamp addressing modes are supported. 
                                    			It does not perform any texture filtering. For integer types, it may optionally promote the integer to single-precision
                                    floating point.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1d-object"><a name="tex1d-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1d-object" name="tex1d-object" shape="rect">B.8.1.2.&nbsp;tex1D()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1D(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x);
</pre><p class="p">fetches from the CUDA array specified by the one-dimensional texture object <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">x</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlod-object"><a name="tex1dlod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlod-object" name="tex1dlod-object" shape="rect">B.8.1.3.&nbsp;tex1DLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1DLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array specified by the one-dimensional texture object
                                    <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">x</samp> at the
                                    level-of-detail <samp class="ph codeph">level</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dgrad-object"><a name="tex1dgrad-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dgrad-object" name="tex1dgrad-object" shape="rect">B.8.1.4.&nbsp;tex1DGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1DGrad(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dy);
</pre><p class="p">fetches from the CUDA array specified by the one-dimensional texture object
                                    <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">x</samp>.  The
                                    level-of-detail is derived from the X-gradient <samp class="ph codeph">dx</samp> and
                                    Y-gradient <samp class="ph codeph">dy</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2d-object"><a name="tex2d-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2d-object" name="tex2d-object" shape="rect">B.8.1.5.&nbsp;tex2D()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex2D(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y);
</pre><p class="p">fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object <samp class="ph codeph">texObj</samp>
                                    		using texture coordinate <samp class="ph codeph">(x,y)</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlod-object"><a name="tex2dlod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlod-object" name="tex2dlod-object" shape="rect">B.8.1.6.&nbsp;tex2DLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
tex2DLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array or the region of linear memory specified by the
                                    two-dimensional texture object <samp class="ph codeph">texObj</samp> using texture
                                    coordinate <samp class="ph codeph">(x,y)</samp> at level-of-detail
                                    <samp class="ph codeph">level</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dgrad-object"><a name="tex2dgrad-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dgrad-object" name="tex2dgrad-object" shape="rect">B.8.1.7.&nbsp;tex2DGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex2DGrad(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y,
            float2 dx, float2 dy);
</pre><p class="p">fetches from the CUDA array specified by the two-dimensional texture
                                    object <samp class="ph codeph">texObj</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp>.  The level-of-detail is derived from the
                                    <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> gradients.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex3d-object"><a name="tex3d-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex3d-object" name="tex3d-object" shape="rect">B.8.1.8.&nbsp;tex3D()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex3D(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z);
</pre><p class="p">fetches from the CUDA array specified by the three-dimensional texture object <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">(x,y,z)</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex3dlod-object"><a name="tex3dlod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex3dlod-object" name="tex3dlod-object" shape="rect">B.8.1.9.&nbsp;tex3DLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex3DLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array or the region of linear memory specified by the
                                    three-dimensional texture object <samp class="ph codeph">texObj</samp> using texture
                                    coordinate <samp class="ph codeph">(x,y,z)</samp> at level-of-detail
                                    <samp class="ph codeph">level</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex3dgrad-object"><a name="tex3dgrad-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex3dgrad-object" name="tex3dgrad-object" shape="rect">B.8.1.10.&nbsp;tex3DGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex3DGrad(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z,
            float4 dx, float4 dy);
</pre><p class="p">fetches from the CUDA array specified by the three-dimensional texture
                                    object <samp class="ph codeph">texObj</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp> at a level-of-detail derived from the X and Y
                                    gradients <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlayered-object"><a name="tex1dlayered-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlayered-object" name="tex1dlayered-object" shape="rect">B.8.1.11.&nbsp;tex1DLayered()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1DLayered(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer);
</pre><p class="p">fetches from the CUDA array specified by the
                                    		one-dimensional texture object <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">x</samp> and 
                                    		index <samp class="ph codeph">layer</samp>, as described in
                                    		<a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a></p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlayeredlod-object"><a name="tex1dlayeredlod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlayeredlod-object" name="tex1dlayeredlod-object" shape="rect">B.8.1.12.&nbsp;tex1DLayeredLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1DLayeredLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array specified by the one-dimensional
                                    <a class="xref" href="index.html#layered-textures" shape="rect">layered
                                       texture</a> at layer <samp class="ph codeph">layer</samp> using texture coordinate
                                    <samp class="ph codeph">x</samp> and level-of-detail <samp class="ph codeph">level</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlayeredgrad-object"><a name="tex1dlayeredgrad-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlayeredgrad-object" name="tex1dlayeredgrad-object" shape="rect">B.8.1.13.&nbsp;tex1DLayeredGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex1DLayeredGrad(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dy);
</pre><p class="p">fetches from the CUDA array specified by the one-dimensional
                                    <a class="xref" href="index.html#layered-textures" shape="rect">layered
                                       texture</a> at layer <samp class="ph codeph">layer</samp> using texture coordinate
                                    <samp class="ph codeph">x</samp> and a level-of-detail derived from the
                                    <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> gradients.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlayered-object"><a name="tex2dlayered-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlayered-object" name="tex2dlayered-object" shape="rect">B.8.1.14.&nbsp;tex2DLayered()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex2DLayered(cudaTextureObject_t texObj,
               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer);
</pre><p class="p">fetches from the CUDA array specified by the two-dimensional texture object <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">(x,y)</samp> and index <samp class="ph codeph">layer</samp>, as described in <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlayeredlod-object"><a name="tex2dlayeredlod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlayeredlod-object" name="tex2dlayeredlod-object" shape="rect">B.8.1.15.&nbsp;tex2DLayeredLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex2DLayeredLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array specified by the two-dimensional
                                    <a class="xref" href="index.html#layered-textures" shape="rect">layered
                                       texture</a> at layer <samp class="ph codeph">layer</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlayeredgrad-object"><a name="tex2dlayeredgrad-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlayeredgrad-object" name="tex2dlayeredgrad-object" shape="rect">B.8.1.16.&nbsp;tex2DLayeredGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex2DLayeredGrad(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                   float2 dx, float2 dy);
</pre><p class="p">fetches from the CUDA array specified by the two-dimensional
                                    <a class="xref" href="index.html#layered-textures" shape="rect">layered
                                       texture</a> at layer <samp class="ph codeph">layer</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp> and a level-of-detail derived from the
                                    <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> X and Y gradients.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemap-object"><a name="texcubemap-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemap-object" name="texcubemap-object" shape="rect">B.8.1.17.&nbsp;texCubemap()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T texCubemap(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z);
</pre><p class="p">fetches the CUDA array specified by the cubemap texture object <samp class="ph codeph">texObj</samp> using texture coordinate <samp class="ph codeph">(x,y,z)</samp>, as described in <a class="xref" href="index.html#cubemap-textures" shape="rect">Cubemap Textures</a>.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemaplod-object"><a name="texcubemaplod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemaplod-object" name="texcubemaplod-object" shape="rect">B.8.1.18.&nbsp;texCubemapLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T texCubemapLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>, y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array specified by the cubemap texture
                                    object <samp class="ph codeph">texObj</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp> as described in <a class="xref" href="index.html#cubemap-textures" shape="rect">Cubemap Textures</a>.  The level-of-detail
                                    used is given by <samp class="ph codeph">level</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemaplayered-object"><a name="texcubemaplayered-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemaplayered-object" name="texcubemaplayered-object" shape="rect">B.8.1.19.&nbsp;texCubemapLayered()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T texCubemapLayered(cudaTextureObject_t texObj,
                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer);
</pre><p class="p">fetches from the CUDA array specified by the cubemap layered texture object <samp class="ph codeph">texObj</samp> using texture coordinates <samp class="ph codeph">(x,y,z)</samp>, and index <samp class="ph codeph">layer</samp>, as described in <a class="xref" href="index.html#cubemap-layered-textures" shape="rect">Cubemap Layered Textures</a>.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemaplayeredlod-object"><a name="texcubemaplayeredlod-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemaplayeredlod-object" name="texcubemaplayeredlod-object" shape="rect">B.8.1.20.&nbsp;texCubemapLayeredLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T texCubemapLayeredLod(cudaTextureObject_t texObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array specified by the cubemap layered texture
                                    object <samp class="ph codeph">texObj</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp> and index <samp class="ph codeph">layer</samp>, as described in
                                    <a class="xref" href="index.html#cubemap-layered-textures" shape="rect">Cubemap Layered Textures</a>, at
                                    level-of-detail level <samp class="ph codeph">level</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dgather-object"><a name="tex2dgather-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dgather-object" name="tex2dgather-object" shape="rect">B.8.1.21.&nbsp;tex2Dgather()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class T&gt;
T tex2Dgather(cudaTextureObject_t texObj,
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> comp = 0);
</pre><p class="p">fetches from the CUDA array specified by the 2D texture object <samp class="ph codeph">texObj</samp> using texture coordinates <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> and the <samp class="ph codeph">comp</samp> parameter as described in <a class="xref" href="index.html#texture-gather" shape="rect">Texture Gather</a>.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="texture-reference-api-appendix"><a name="texture-reference-api-appendix" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#texture-reference-api-appendix" name="texture-reference-api-appendix" shape="rect">B.8.2.&nbsp;Texture Reference API</a></h3>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dfetch"><a name="tex1dfetch" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dfetch" name="tex1dfetch" shape="rect">B.8.2.1.&nbsp;tex1Dfetch()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType&gt;
Type tex1Dfetch(
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1D,
           cudaReadModeElementType&gt; texRef,
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tex1Dfetch(
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>, cudaTextureType1D,
           cudaReadModeNormalizedFloat&gt; texRef,
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tex1Dfetch(
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">signed</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>, cudaTextureType1D,
           cudaReadModeNormalizedFloat&gt; texRef,
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tex1Dfetch(
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span>, cudaTextureType1D,
           cudaReadModeNormalizedFloat&gt; texRef,
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> tex1Dfetch(
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">signed</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span>, cudaTextureType1D,
           cudaReadModeNormalizedFloat&gt; texRef,
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);</pre><p class="p">fetches from the region of linear memory bound to the one-dimensional texture reference <samp class="ph codeph">texRef</samp> using integer texture coordinate <samp class="ph codeph">x</samp>. <samp class="ph codeph">tex1Dfetch()</samp> only works with non-normalized coordinates, so only the border and clamp addressing modes are supported. It does not perform
                                    any texture filtering. For integer types, it may optionally promote the integer to single-precision floating point.
                                    
                                 </p>
                                 <p class="p">Besides the functions shown above, 2-, and 4-tuples are supported; for example:
                                    
                                 </p><pre xml:space="preserve">float4 tex1Dfetch(
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;uchar4, cudaTextureType1D,
           cudaReadModeNormalizedFloat&gt; texRef,
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x);</pre><p class="p">fetches from the region of linear memory bound to texture reference <samp class="ph codeph">texRef</samp> using texture coordinate <samp class="ph codeph">x</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1d"><a name="tex1d" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1d" name="tex1d" shape="rect">B.8.2.2.&nbsp;tex1D()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex1D(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1D, readMode&gt; texRef,
           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x);</pre><p class="p">fetches from the CUDA array bound to the one-dimensional texture reference <samp class="ph codeph">texRef</samp>
                                    using texture coordinate <samp class="ph codeph">x</samp>. <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlod"><a name="tex1dlod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlod" name="tex1dlod" shape="rect">B.8.2.3.&nbsp;tex1DLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span>
cudaTextureReadMode readMode&gt;
Type tex1DLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1D, readMode&gt; texRef, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x,
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the one-dimensional
                                    	texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    	<samp class="ph codeph">x</samp>. The level-of-detail is given by <samp class="ph codeph">level</samp>.
                                    	<samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    	except when <samp class="ph codeph">readMode</samp> is
                                    	<samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    	<a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    	in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    	floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dgrad"><a name="tex1dgrad" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dgrad" name="tex1dgrad" shape="rect">B.8.2.4.&nbsp;tex1DGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span>
cudaTextureReadMode readMode&gt;
Type tex1DGrad(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1D, readMode&gt; texRef, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x,
						 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dy);
</pre><p class="p">fetches from the CUDA array bound to the one-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">x</samp>. The level-of-detail is derived from the
                                    <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> X- and Y-gradients.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2d"><a name="tex2d" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2d" name="tex2d" shape="rect">B.8.2.5.&nbsp;tex2D()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex2D(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2D, readMode&gt; texRef,
           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y);</pre><p class="p">fetches from the CUDA array or the region of linear memory bound to the two-dimensional texture
                                    reference <samp class="ph codeph">texRef</samp> using texture coordinates <samp class="ph codeph">x</samp> and
                                    <samp class="ph codeph">y</samp>. <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlod"><a name="tex2dlod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlod" name="tex2dlod" shape="rect">B.8.2.6.&nbsp;tex2DLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span>
cudaTextureReadMode readMode&gt;
Type tex2DLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2D, readMode&gt; texRef,
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the two-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp>. The level-of-detail is given by
                                    <samp class="ph codeph">level</samp>.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dgrad"><a name="tex2dgrad" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dgrad" name="tex2dgrad" shape="rect">B.8.2.7.&nbsp;tex2DGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span>
cudaTextureReadMode readMode&gt;
Type tex2DGrad(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2D, readMode&gt; texRef,
               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, float2 dx, float2 dy);
</pre><p class="p">fetches from the CUDA array bound to the two-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp>. The level-of-detail is derived from the
                                    <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> X- and Y-gradients.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex3d"><a name="tex3d" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex3d" name="tex3d" shape="rect">B.8.2.8.&nbsp;tex3D()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex3D(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType3D, readMode&gt; texRef,
           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z);</pre><p class="p">fetches from the CUDA array bound to the three-dimensional texture reference <samp class="ph codeph">texRef</samp>
                                    using texture coordinates <samp class="ph codeph">x</samp>, <samp class="ph codeph">y</samp>, and
                                    <samp class="ph codeph">z</samp>. <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex3dlod"><a name="tex3dlod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex3dlod" name="tex3dlod" shape="rect">B.8.2.9.&nbsp;tex3DLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span>
cudaTextureReadMode readMode&gt;
Type tex3DLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType3D, readMode&gt; texRef,
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the two-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp>. The level-of-detail is
                                    given by <samp class="ph codeph">level</samp>.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex3dgrad"><a name="tex3dgrad" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex3dgrad" name="tex3dgrad" shape="rect">B.8.2.10.&nbsp;tex3DGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span>
cudaTextureReadMode readMode&gt;
Type tex3DGrad(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType3D, readMode&gt; texRef,
               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, float4 dx, float4 dy);
</pre><p class="p">fetches from the CUDA array bound to the two-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp>. The level-of-detail is
                                    derived from the <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> X- and
                                    Y-gradients.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlayered"><a name="tex1dlayered" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlayered" name="tex1dlayered" shape="rect">B.8.2.11.&nbsp;tex1DLayered()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex1DLayered(
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1DLayered, readMode&gt; texRef,
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer);</pre><p class="p">fetches from the CUDA array bound to the one-dimensional layered texture reference
                                    <samp class="ph codeph">texRef</samp> using texture coordinate <samp class="ph codeph">x</samp> and
                                    index <samp class="ph codeph">layer</samp>, as described in <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>. <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlayeredlod"><a name="tex1dlayeredlod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlayeredlod" name="tex1dlayeredlod" shape="rect">B.8.2.12.&nbsp;tex1DLayeredLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex1DLayeredLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1D, readMode&gt; texRef,
                     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the one-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">x</samp> and index <samp class="ph codeph">layer</samp> as described in
                                    <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                                    The level-of-detail is
                                    given by <samp class="ph codeph">level</samp>.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex1dlayeredgrad"><a name="tex1dlayeredgrad" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex1dlayeredgrad" name="tex1dlayeredgrad" shape="rect">B.8.2.13.&nbsp;tex1DLayeredGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex1DLayeredGrad(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType1D, readMode&gt; texRef,
                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> dy);
</pre><p class="p">fetches from the CUDA array bound to the one-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">x</samp> and index <samp class="ph codeph">layer</samp> as described in
                                    <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                                    The level-of-detail is
                                    derived from the <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> X- and
                                    Y-gradients.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlayered"><a name="tex2dlayered" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlayered" name="tex2dlayered" shape="rect">B.8.2.14.&nbsp;tex2DLayered()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex2DLayered(
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2DLayered, readMode&gt; texRef,
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer);</pre><p class="p">fetches from the CUDA array bound to the two-dimensional layered texture
                                    reference <samp class="ph codeph">texRef</samp> using texture coordinates
                                    <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and index
                                    <samp class="ph codeph">layer</samp>, as described in <a class="xref" href="index.html#texture-memory" shape="rect">Texture Memory</a>. <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlayeredlod"><a name="tex2dlayeredlod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlayeredlod" name="tex2dlayeredlod" shape="rect">B.8.2.15.&nbsp;tex2DLayeredLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex2DLayeredLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2D, readMode&gt; texRef,
                     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the two-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp> and index <samp class="ph codeph">layer</samp> as described in
                                    <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                                    The level-of-detail is
                                    given by <samp class="ph codeph">level</samp>.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dlayeredgrad"><a name="tex2dlayeredgrad" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dlayeredgrad" name="tex2dlayeredgrad" shape="rect">B.8.2.16.&nbsp;tex2DLayeredGrad()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex2DLayeredGrad(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2D, readMode&gt; texRef,
                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, float2 dx, float2 dy);
</pre><p class="p">fetches from the CUDA array bound to the two-dimensional
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y)</samp> and index <samp class="ph codeph">layer</samp> as described in
                                    <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                                    The level-of-detail is
                                    derived from the <samp class="ph codeph">dx</samp> and <samp class="ph codeph">dy</samp> X- and Y-gradients.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemap"><a name="texcubemap" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemap" name="texcubemap" shape="rect">B.8.2.17.&nbsp;texCubemap()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type texCubemap(
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureTypeCubemap, readMode&gt; texRef,
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z);</pre><p class="p">fetches from the CUDA array bound to the cubemap texture reference <samp class="ph codeph">texRef</samp> using
                                    texture coordinates <samp class="ph codeph">x</samp>, <samp class="ph codeph">y</samp>, and
                                    <samp class="ph codeph">z</samp>, as described in <a class="xref" href="index.html#cubemap-textures" shape="rect">Cubemap Textures</a>.
                                    <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemaplod"><a name="texcubemaplod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemaplod" name="texcubemaplod" shape="rect">B.8.2.18.&nbsp;texCubemapLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type texCubemapLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureTypeCubemap, readMode&gt; texRef,
                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the cubemap
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp>.
                                    The level-of-detail is
                                    given by <samp class="ph codeph">level</samp>.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemaplayered"><a name="texcubemaplayered" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemaplayered" name="texcubemaplayered" shape="rect">B.8.2.19.&nbsp;texCubemapLayered()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type texCubemapLayered(
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureTypeCubemapLayered, readMode&gt; texRef,
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer);</pre><p class="p">fetches from the CUDA array bound to the cubemap layered texture reference <samp class="ph codeph">texRef</samp>
                                    using texture coordinates <samp class="ph codeph">x</samp>, <samp class="ph codeph">y</samp>,
                                    and <samp class="ph codeph">z</samp>, and index <samp class="ph codeph">layer</samp>, as described in <a class="xref" href="index.html#cubemap-layered-textures" shape="rect">Cubemap Layered Textures</a>. <samp class="ph codeph">Type</samp> is equal to <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is equal to the matching floating-point type.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="texcubemaplayeredlod"><a name="texcubemaplayeredlod" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#texcubemaplayeredlod" name="texcubemaplayeredlod" shape="rect">B.8.2.20.&nbsp;texCubemapLayeredLod()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type texCubemapLayeredLod(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureTypeCubemapLayered, readMode&gt; texRef, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> z, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> level);
</pre><p class="p">fetches from the CUDA array bound to the cubemap layered 
                                    texture reference <samp class="ph codeph">texRef</samp> using texture coordinate
                                    <samp class="ph codeph">(x,y,z)</samp> and index <samp class="ph codeph">layer</samp> as described in
                                    <a class="xref" href="index.html#layered-textures" shape="rect">Layered Textures</a>.
                                    The level-of-detail is
                                    given by <samp class="ph codeph">level</samp>.
                                    <samp class="ph codeph">Type</samp> is the same as <samp class="ph codeph">DataType</samp>
                                    except when <samp class="ph codeph">readMode</samp> is
                                    <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see
                                    <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case <samp class="ph codeph">Type</samp> is the corresponding
                                    floating-point type.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="tex2dgather"><a name="tex2dgather" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#tex2dgather" name="tex2dgather" shape="rect">B.8.2.21.&nbsp;tex2Dgather()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">template&lt;class DataType, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> cudaTextureReadMode readMode&gt;
Type tex2Dgather(
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;DataType, cudaTextureType2D, readMode&gt; texRef,
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> comp = 0);</pre><p class="p">fetches from the CUDA array bound to the 2D texture reference <samp class="ph codeph">texRef</samp> using
                                    texture coordinates <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> and the <samp class="ph codeph">comp</samp> parameter as
                                    described in <a class="xref" href="index.html#texture-gather" shape="rect">Texture Gather</a>. <samp class="ph codeph">Type</samp> is a 4-component vector type. It is based on the base type of <samp class="ph codeph">DataType</samp> except when <samp class="ph codeph">readMode</samp> is equal to <samp class="ph codeph">cudaReadModeNormalizedFloat</samp> (see <a class="xref" href="index.html#texture-reference-api" shape="rect">[[DEPRECATED]] Texture Reference API</a>),
                                    in which case it is always <samp class="ph codeph">float4</samp>.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="surface-functions"><a name="surface-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#surface-functions" name="surface-functions" shape="rect">B.9.&nbsp;Surface Functions</a></h3>
                     <div class="body conbody">
                        <p class="p">Surface functions are only supported by devices of compute capability
                           2.0 and higher.
                        </p>
                        <p class="p">Surface objects are described in described in <a class="xref" href="index.html#surface-object-api-appendix" shape="rect">Surface Object API</a></p>
                        <p class="p">Surface references are described in <a class="xref" href="index.html#surface-reference-api-appendix" shape="rect">Surface Reference API</a>.
                        </p>
                        <p class="p">In the sections below, <samp class="ph codeph">boundaryMode</samp> specifies the
                           boundary mode, that is how out-of-range surface coordinates are handled;
                           it is equal to either <samp class="ph codeph">cudaBoundaryModeClamp</samp>, in which
                           case out-of-range coordinates are clamped to the valid range, or
                           <samp class="ph codeph">cudaBoundaryModeZero</samp>, in which case out-of-range reads
                           return zero and out-of-range writes are ignored, or
                           <samp class="ph codeph">cudaBoundaryModeTrap</samp>, in which case out-of-range
                           accesses cause the kernel execution to fail.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="surface-object-api-appendix"><a name="surface-object-api-appendix" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#surface-object-api-appendix" name="surface-object-api-appendix" shape="rect">B.9.1.&nbsp;Surface Object API</a></h3>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dread-object"><a name="surf1dread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dread-object" name="surf1dread-object" shape="rect">B.9.1.1.&nbsp;surf1Dread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surf1Dread(cudaSurfaceObject_t surfObj, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x,
               boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the one-dimensional surface object <samp class="ph codeph">surfObj</samp> using coordinate x. 
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dwrite-object"><a name="surf1dwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dwrite-object" name="surf1dwrite-object" shape="rect">B.9.1.2.&nbsp;surf1Dwrite</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1Dwrite(T data,
                  cudaSurfaceObject_t surfObj,
                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x,
                  boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    writes value data to the CUDA array specified by the one-dimensional surface object <samp class="ph codeph">surfObj</samp> at coordinate x.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dread-object"><a name="surf2dread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dread-object" name="surf2dread-object" shape="rect">B.9.1.3.&nbsp;surf2Dread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surf2Dread(cudaSurfaceObject_t surfObj,
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y,
              boundaryMode = cudaBoundaryModeTrap);
template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2Dread(T* data,
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y,
                 boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the two-dimensional surface object <samp class="ph codeph">surfObj</samp> using coordinates x and y.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dwrite-object"><a name="surf2dwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dwrite-object" name="surf2dwrite-object" shape="rect">B.9.1.4.&nbsp;surf2Dwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2Dwrite(T data,
                  cudaSurfaceObject_t surfObj,
                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y,
                  boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    writes value data to the CUDA array specified by the two-dimensional surface object <samp class="ph codeph">surfObj</samp> at coordinate x and y.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf3dread-object"><a name="surf3dread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf3dread-object" name="surf3dread-object" shape="rect">B.9.1.5.&nbsp;surf3Dread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surf3Dread(cudaSurfaceObject_t surfObj,
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
              boundaryMode = cudaBoundaryModeTrap);
template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf3Dread(T* data,
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
                 boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the three-dimensional surface object <samp class="ph codeph">surfObj</samp>  using coordinates x, y, and z. 
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf3dwrite-object"><a name="surf3dwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf3dwrite-object" name="surf3dwrite-object" shape="rect">B.9.1.6.&nbsp;surf3Dwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf3Dwrite(T data,
                  cudaSurfaceObject_t surfObj,
                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
                  boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    writes value data to the CUDA array specified by the three-dimensional object <samp class="ph codeph">surfObj</samp> at coordinate x, y, and z. 
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dlayeredread-object"><a name="surf1dlayeredread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dlayeredread-object" name="surf1dlayeredread-object" shape="rect">B.9.1.7.&nbsp;surf1DLayeredread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surf1DLayeredread(
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                 boundaryMode = cudaBoundaryModeTrap);
template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1DLayeredread(T data,
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                 boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the one-dimensional layered surface object <samp class="ph codeph">surfObj</samp> using coordinate x and index <samp class="ph codeph">layer</samp>.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dlayeredwrite-object"><a name="surf1dlayeredwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dlayeredwrite-object" name="surf1dlayeredwrite-object" shape="rect">B.9.1.8.&nbsp;surf1DLayeredwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1DLayeredwrite(T data,
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                 boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    writes value data to the CUDA array specified by the two-dimensional layered surface object <samp class="ph codeph">surfObj</samp> at coordinate x and index <samp class="ph codeph">layer</samp>.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dlayeredread-object"><a name="surf2dlayeredread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dlayeredread-object" name="surf2dlayeredread-object" shape="rect">B.9.1.9.&nbsp;surf2DLayeredread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surf2DLayeredread(
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                 boundaryMode = cudaBoundaryModeTrap);
template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2DLayeredread(T data,
                         cudaSurfaceObject_t surfObj,
                         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,	
                         boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the two-dimensional layered surface object <samp class="ph codeph">surfObj</samp> using coordinate x and y, and index <samp class="ph codeph">layer</samp>.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dlayeredwrite-object"><a name="surf2dlayeredwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dlayeredwrite-object" name="surf2dlayeredwrite-object" shape="rect">B.9.1.10.&nbsp;surf2DLayeredwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2DLayeredwrite(T data,
                          cudaSurfaceObject_t surfObj,
                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                          boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    writes value data to the CUDA array specified by the one-dimensional layered surface object <samp class="ph codeph">surfObj</samp> at coordinate x and y, and index <samp class="ph codeph">layer</samp>.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemapread-object"><a name="surfcubemapread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemapread-object" name="surfcubemapread-object" shape="rect">B.9.1.11.&nbsp;surfCubemapread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surfCubemapread(
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> face,
                 boundaryMode = cudaBoundaryModeTrap);
template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapread(T data,
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> face,
                 boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the cubemap surface object <samp class="ph codeph">surfObj</samp> using coordinate x and y, and face index face.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemapwrite-object"><a name="surfcubemapwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemapwrite-object" name="surfcubemapwrite-object" shape="rect">B.9.1.12.&nbsp;surfCubemapwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapwrite(T data,
                 cudaSurfaceObject_t surfObj,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> face,
                 boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    
                                    		   writes value data to the CUDA array specified by the cubemap object <samp class="ph codeph">surfObj</samp> at coordinate x and y, and face index face.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemaplayeredread-object"><a name="surfcubemaplayeredread-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemaplayeredread-object" name="surfcubemaplayeredread-object" shape="rect">B.9.1.13.&nbsp;surfCubemapLayeredread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
T surfCubemapLayeredread(
             cudaSurfaceObject_t surfObj,
             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layerFace,
             boundaryMode = cudaBoundaryModeTrap);
template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapLayeredread(T data,
             cudaSurfaceObject_t surfObj,
             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layerFace,
             boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    reads the CUDA array specified by the cubemap layered surface object <samp class="ph codeph">surfObj</samp> using coordinate x and y, and index <samp class="ph codeph">layerFace.</samp></div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemaplayeredwrite-object"><a name="surfcubemaplayeredwrite-object" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemaplayeredwrite-object" name="surfcubemaplayeredwrite-object" shape="rect">B.9.1.14.&nbsp;surfCubemapLayeredwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapLayeredwrite(T data,
             cudaSurfaceObject_t surfObj,
             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layerFace,
             boundaryMode = cudaBoundaryModeTrap);
</pre>
                                    writes value data to the CUDA array specified by the cubemap layered object surfObj at coordinate x and y, and index <samp class="ph codeph">layerFace</samp>.
                                 </div>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="surface-reference-api-appendix"><a name="surface-reference-api-appendix" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#surface-reference-api-appendix" name="surface-reference-api-appendix" shape="rect">B.9.2.&nbsp;Surface Reference API</a></h3>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dread"><a name="surf1dread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dread" name="surf1dread" shape="rect">B.9.2.1.&nbsp;surf1Dread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surf1Dread(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType1D&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x,
                boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1Dread(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType1D&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the one-dimensional surface reference <samp class="ph codeph">surfRef</samp> using coordinate <samp class="ph codeph">x</samp>.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dwrite"><a name="surf1dwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dwrite" name="surf1dwrite" shape="rect">B.9.2.2.&nbsp;surf1Dwrite</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1Dwrite(Type data,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType1D&gt; surfRef,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x,
                 boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the one-dimensional surface reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dread"><a name="surf2dread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dread" name="surf2dread" shape="rect">B.9.2.3.&nbsp;surf2Dread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surf2Dread(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2D&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y,
                boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2Dread(Type* data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2D&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the two-dimensional surface reference <samp class="ph codeph">surfRef</samp> using coordinates <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dwrite"><a name="surf2dwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dwrite" name="surf2dwrite" shape="rect">B.9.2.4.&nbsp;surf2Dwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf3Dwrite(Type data,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType3D&gt; surfRef,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
                 boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the two-dimensional surface reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf3dread"><a name="surf3dread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf3dread" name="surf3dread" shape="rect">B.9.2.5.&nbsp;surf3Dread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surf3Dread(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType3D&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
                boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf3Dread(Type* data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType3D&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the three-dimensional surface reference <samp class="ph codeph">surfRef</samp> using coordinates <samp class="ph codeph">x</samp>, <samp class="ph codeph">y</samp>, and <samp class="ph codeph">z</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf3dwrite"><a name="surf3dwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf3dwrite" name="surf3dwrite" shape="rect">B.9.2.6.&nbsp;surf3Dwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf3Dwrite(Type data,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType3D&gt; surfRef,
                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z,
                 boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the three-dimensional surface reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp>, <samp class="ph codeph">y</samp>, and <samp class="ph codeph">z</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dlayeredread"><a name="surf1dlayeredread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dlayeredread" name="surf1dlayeredread" shape="rect">B.9.2.7.&nbsp;surf1DLayeredread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surf1DLayeredread(
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType1DLayered&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1DLayeredread(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType1DLayered&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the one-dimensional layered surface reference <samp class="ph codeph">surfRef</samp> using coordinate <samp class="ph codeph">x</samp> and index <samp class="ph codeph">layer</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf1dlayeredwrite"><a name="surf1dlayeredwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf1dlayeredwrite" name="surf1dlayeredwrite" shape="rect">B.9.2.8.&nbsp;surf1DLayeredwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf1DLayeredwrite(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType1DLayered&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the two-dimensional layered surface reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp> and index <samp class="ph codeph">layer</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dlayeredread"><a name="surf2dlayeredread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dlayeredread" name="surf2dlayeredread" shape="rect">B.9.2.9.&nbsp;surf2DLayeredread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surf2DLayeredread(
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2DLayered&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2DLayeredread(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2DLayered&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the two-dimensional layered surface reference <samp class="ph codeph">surfRef</samp> using coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and index <samp class="ph codeph">layer</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surf2dlayeredwrite"><a name="surf2dlayeredwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surf2dlayeredwrite" name="surf2dlayeredwrite" shape="rect">B.9.2.10.&nbsp;surf2DLayeredwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surf2DLayeredwrite(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceType2DLayered&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layer,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the one-dimensional layered surface reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and index <samp class="ph codeph">layer</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemapread"><a name="surfcubemapread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemapread" name="surfcubemapread" shape="rect">B.9.2.11.&nbsp;surfCubemapread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surfCubemapread(
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceTypeCubemap&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> face,
                boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapread(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceTypeCubemap&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> face,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the cubemap surface reference <samp class="ph codeph">surfRef</samp> using coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and face index <samp class="ph codeph">face</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemapwrite"><a name="surfcubemapwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemapwrite" name="surfcubemapwrite" shape="rect">B.9.2.12.&nbsp;surfCubemapwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapwrite(Type data,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceTypeCubemap&gt; surfRef,
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> face,
                boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the cubemap reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and face index <samp class="ph codeph">face</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemaplayeredread"><a name="surfcubemaplayeredread" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemaplayeredread" name="surfcubemaplayeredread" shape="rect">B.9.2.13.&nbsp;surfCubemapLayeredread()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
Type surfCubemapLayeredread(
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceTypeCubemapLayered&gt; surfRef,
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layerFace,
            boundaryMode = cudaBoundaryModeTrap);
template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapLayeredread(Type data,
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceTypeCubemapLayered&gt; surfRef,
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layerFace,
            boundaryMode = cudaBoundaryModeTrap);</pre>
                                    reads the CUDA array bound to the cubemap layered surface reference <samp class="ph codeph">surfRef</samp> using coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and index <samp class="ph codeph">layerFace</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="surfcubemaplayeredwrite"><a name="surfcubemaplayeredwrite" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#surfcubemaplayeredwrite" name="surfcubemaplayeredwrite" shape="rect">B.9.2.14.&nbsp;surfCubemapLayeredwrite()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve">template&lt;class Type&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> surfCubemapLayeredwrite(Type data,
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>, cudaSurfaceTypeCubemapLayered&gt; surfRef,
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> layerFace,
            boundaryMode = cudaBoundaryModeTrap);</pre>
                                    writes value <samp class="ph codeph">data</samp> to the CUDA array bound to the cubemap layered reference <samp class="ph codeph">surfRef</samp> at coordinate <samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp>, and index <samp class="ph codeph">layerFace</samp>. 
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="ldg-function"><a name="ldg-function" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#ldg-function" name="ldg-function" shape="rect">B.10.&nbsp;Read-Only Data Cache Load Function</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">The read-only data cache load function is only supported by devices of compute
                              capability 3.5 and higher.
                           </p>
                           <div class="p"><pre xml:space="preserve">T __ldg(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* address);</pre> returns the
                              data of type <samp class="ph codeph">T</samp> located at address <samp class="ph codeph">address</samp>,
                              where <samp class="ph codeph">T</samp> is <samp class="ph codeph">char</samp>, <samp class="ph codeph">signed char</samp>,
                              <samp class="ph codeph">short</samp>, <samp class="ph codeph">int</samp>, <samp class="ph codeph">long</samp>,
                              <samp class="ph codeph">long long</samp><samp class="ph codeph">unsigned char</samp>, <samp class="ph codeph">unsigned short</samp>,
                              <samp class="ph codeph">unsigned int</samp>, <samp class="ph codeph">unsigned long</samp>,
                              <samp class="ph codeph">unsigned long long</samp>, <samp class="ph codeph">char2</samp>,
                              <samp class="ph codeph">char4</samp>, <samp class="ph codeph">short2</samp>, <samp class="ph codeph">short4</samp>,
                              <samp class="ph codeph">int2</samp>, <samp class="ph codeph">int4</samp>, <samp class="ph codeph">longlong2</samp><samp class="ph codeph">uchar2</samp>, <samp class="ph codeph">uchar4</samp>, <samp class="ph codeph">ushort2</samp>,
                              <samp class="ph codeph">ushort4</samp>, <samp class="ph codeph">uint2</samp>, <samp class="ph codeph">uint4</samp>,
                              <samp class="ph codeph">ulonglong2</samp><samp class="ph codeph">float</samp>, <samp class="ph codeph">float2</samp>, <samp class="ph codeph">float4</samp>,
                              <samp class="ph codeph">double</samp>, or <samp class="ph codeph">double2</samp>. With the
                              <samp class="ph codeph">cuda_fp16.h</samp> header included, <samp class="ph codeph">T</samp> can be
                              <samp class="ph codeph">__half</samp> or <samp class="ph codeph">__half2</samp>. Similarly, with the
                              <samp class="ph codeph">cuda_bf16.h</samp> header included, <samp class="ph codeph">T</samp> can also be
                              <samp class="ph codeph">__nv_bfloat16</samp> or <samp class="ph codeph">__nv_bfloat162</samp>. The
                              operation is cached in the read-only data cache (see <a class="xref" href="index.html#global-memory-3-0" shape="rect">Global Memory</a>). 
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="ldx-functions"><a name="ldx-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#ldx-functions" name="ldx-functions" shape="rect">B.11.&nbsp;Load Functions Using Cache Hints</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">These load functions are only supported by devices of compute
                              capability 3.5 and higher.
                           </p>
                           <div class="p"><pre xml:space="preserve">
T __ldcg(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* address);
T __ldca(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* address);
T __ldcs(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* address);
T __ldlu(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* address);
T __ldcv(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* address);</pre> returns the data of type <samp class="ph codeph">T</samp> located at
                              address <samp class="ph codeph">address</samp>, where <samp class="ph codeph">T</samp> is
                              <samp class="ph codeph">char</samp>, <samp class="ph codeph">signed char</samp>, <samp class="ph codeph">short</samp>,
                              <samp class="ph codeph">int</samp>, <samp class="ph codeph">long</samp>, <samp class="ph codeph">long long</samp><samp class="ph codeph">unsigned char</samp>, <samp class="ph codeph">unsigned short</samp>,
                              <samp class="ph codeph">unsigned int</samp>, <samp class="ph codeph">unsigned long</samp>,
                              <samp class="ph codeph">unsigned long long</samp>, <samp class="ph codeph">char2</samp>,
                              <samp class="ph codeph">char4</samp>, <samp class="ph codeph">short2</samp>, <samp class="ph codeph">short4</samp>,
                              <samp class="ph codeph">int2</samp>, <samp class="ph codeph">int4</samp>, <samp class="ph codeph">longlong2</samp><samp class="ph codeph">uchar2</samp>, <samp class="ph codeph">uchar4</samp>, <samp class="ph codeph">ushort2</samp>,
                              <samp class="ph codeph">ushort4</samp>, <samp class="ph codeph">uint2</samp>, <samp class="ph codeph">uint4</samp>,
                              <samp class="ph codeph">ulonglong2</samp><samp class="ph codeph">float</samp>, <samp class="ph codeph">float2</samp>, <samp class="ph codeph">float4</samp>,
                              <samp class="ph codeph">double</samp>, or <samp class="ph codeph">double2</samp>. With the
                              <samp class="ph codeph">cuda_fp16.h</samp> header included, <samp class="ph codeph">T</samp> can be
                              <samp class="ph codeph">__half</samp> or <samp class="ph codeph">__half2</samp>. Similarly, with the
                              <samp class="ph codeph">cuda_bf16.h</samp> header included, <samp class="ph codeph">T</samp> can also be
                              <samp class="ph codeph">__nv_bfloat16</samp> or <samp class="ph codeph">__nv_bfloat162</samp>. The
                              operation is using the corresponding cache operator (see <a class="xref" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#cache-operators" target="_blank" shape="rect">PTX ISA</a>) 
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="stx-functions"><a name="stx-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stx-functions" name="stx-functions" shape="rect">B.12.&nbsp;Store Functions Using Cache Hints</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">These store functions are only supported by devices of compute
                              capability 3.5 and higher.
                           </p>
                           <div class="p"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __stwb(T* address, T value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __stcg(T* address, T value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __stcs(T* address, T value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __stwt(T* address, T value);</pre> stores the <samp class="ph codeph">value</samp> argument of type
                              <samp class="ph codeph">T</samp> to the location at address <samp class="ph codeph">address</samp>,
                              where <samp class="ph codeph">T</samp> is <samp class="ph codeph">char</samp>, <samp class="ph codeph">signed char</samp>,
                              <samp class="ph codeph">short</samp>, <samp class="ph codeph">int</samp>, <samp class="ph codeph">long</samp>,
                              <samp class="ph codeph">long long</samp><samp class="ph codeph">unsigned char</samp>, <samp class="ph codeph">unsigned short</samp>,
                              <samp class="ph codeph">unsigned int</samp>, <samp class="ph codeph">unsigned long</samp>,
                              <samp class="ph codeph">unsigned long long</samp>, <samp class="ph codeph">char2</samp>,
                              <samp class="ph codeph">char4</samp>, <samp class="ph codeph">short2</samp>, <samp class="ph codeph">short4</samp>,
                              <samp class="ph codeph">int2</samp>, <samp class="ph codeph">int4</samp>, <samp class="ph codeph">longlong2</samp><samp class="ph codeph">uchar2</samp>, <samp class="ph codeph">uchar4</samp>, <samp class="ph codeph">ushort2</samp>,
                              <samp class="ph codeph">ushort4</samp>, <samp class="ph codeph">uint2</samp>, <samp class="ph codeph">uint4</samp>,
                              <samp class="ph codeph">ulonglong2</samp><samp class="ph codeph">float</samp>, <samp class="ph codeph">float2</samp>, <samp class="ph codeph">float4</samp>,
                              <samp class="ph codeph">double</samp>, or <samp class="ph codeph">double2</samp>. With the
                              <samp class="ph codeph">cuda_fp16.h</samp> header included, <samp class="ph codeph">T</samp> can be
                              <samp class="ph codeph">__half</samp> or <samp class="ph codeph">__half2</samp>. Similarly, with the
                              <samp class="ph codeph">cuda_bf16.h</samp> header included, <samp class="ph codeph">T</samp> can also be
                              <samp class="ph codeph">__nv_bfloat16</samp> or <samp class="ph codeph">__nv_bfloat162</samp>. The
                              operation is using the corresponding cache operator (see <a class="xref" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#cache-operators" target="_blank" shape="rect">PTX ISA </a>) 
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="time-function"><a name="time-function" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#time-function" name="time-function" shape="rect">B.13.&nbsp;Time Function</a></h3>
                     <div class="body conbody">
                        <div class="p"><pre xml:space="preserve">clock_t clock();
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> clock64();</pre>
                           when executed in device code, returns the value of a per-multiprocessor counter that is incremented every clock cycle. Sampling
                           this counter at the beginning and at the end of a kernel, taking the difference of the two samples, and recording the result
                           per thread provides a measure for each thread of the number of clock cycles taken by the device to completely execute the
                           thread, but not of the number of clock cycles the device actually spent executing thread instructions. The former number is
                           greater than the latter since threads are time sliced.
                           </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="atomic-functions"><a name="atomic-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#atomic-functions" name="atomic-functions" shape="rect">B.14.&nbsp;Atomic Functions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">
                              An atomic function performs a read-modify-write atomic operation on one 32-bit or 64-bit word residing in global or shared
                              memory.
                              For example, <samp class="ph codeph">atomicAdd()</samp> reads a word at some address in global or shared memory, adds a number to it, and writes the result back to the same address.
                              The operation is atomic in the sense that it is guaranteed to be performed without interference from other threads.
                              In other words, no other thread can access this address until the operation is complete.
                              Atomic&nbsp;functions do&nbsp;not act as memory fences and do not imply synchronization or ordering constraints for memory operations
                              (see <a class="xref" href="index.html#memory-fence-functions" shape="rect">Memory Fence Functions</a> for more details on memory fences).
                              Atomic functions can only be used in device functions.
                           </p>
                           <p class="p">Atomic functions are only atomic with respect to other operations performed by threads of a particular set: </p>
                           <ul class="ul">
                              <li class="li">System-wide atomics: atomic for all threads in the current
                                 program including other CPUs and GPUs in the system. These are
                                 suffixed with <samp class="ph codeph">_system</samp>, e.g.,
                                 <samp class="ph codeph">atomicAdd_system</samp>.
                              </li>
                              <li class="li">Device-wide atomics: atomic for all CUDA threads in the current
                                 program executing in the same compute device as the current thread.
                                 These are not suffixed and just named after the operation instead,
                                 e.g., <samp class="ph codeph">atomicAdd</samp>.
                              </li>
                              <li class="li">Block-wide atomics: atomic for all CUDA threads in the current
                                 program executing in the same thread block as the current thread.
                                 These are suffixed with <samp class="ph codeph">_block</samp>, e.g.,
                                 <samp class="ph codeph">atomicAdd_block</samp>.
                              </li>
                           </ul>
                           <div class="p">In the following example both the CPU and the GPU atomically update an integer value at address <samp class="ph codeph">addr</samp>:
                              <pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> mykernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *addr) {
  atomicAdd_system(addr, 10);       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// only available on devices with compute capability 6.x</span>
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *addr;
  cudaMallocManaged(&amp;addr, 4);
  *addr = 0;

   mykernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>...<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(addr);
   __sync_fetch_and_add(addr, 10);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// CPU atomic operation</span>
}</pre></div>
                           <p class="p">Note that any atomic operation can be implemented based on <samp class="ph codeph">atomicCAS()</samp> (Compare And Swap). For example, <samp class="ph codeph">atomicAdd()</samp> for double-precision floating-point numbers is not available on devices with compute capability lower than 6.0 but it can
                              be implemented as follows:
                              
                           </p><pre xml:space="preserve">#<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> __CUDA_ARCH__ &lt; 600
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span> atomicAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span> val)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address_as_ull =
                              (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>*)address;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> old = *address_as_ull, assumed;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">do</span> {
        assumed = old;
        old = atomicCAS(address_as_ull, assumed,
                        __double_as_longlong(val +
                               __longlong_as_double(assumed)));

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN)</span>
    } <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (assumed != old);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> __longlong_as_double(old);
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span></pre><p class="p">There are system-wide and block-wide variants of the following device-wide
                              atomic APIs, with the following exceptions:
                           </p>
                           <ul class="ul">
                              <li class="li">Devices with compute capability less than 6.0 only support device-wide
                                 atomic operations,
                              </li>
                              <li class="li">Tegra devices with compute capability less than 7.2 do not support
                                 system-wide atomic operations.
                              </li>
                           </ul>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="arithmetic-functions"><a name="arithmetic-functions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#arithmetic-functions" name="arithmetic-functions" shape="rect">B.14.1.&nbsp;Arithmetic Functions</a></h3>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicadd"><a name="atomicadd" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicadd" name="atomicadd" shape="rect">B.14.1.1.&nbsp;atomicAdd()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> atomicAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span> atomicAdd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span> val);
__half2 atomicAdd(__half2 *address, __half2 val);
__half atomicAdd(__half *address, __half val);
__nv_bfloat162 atomicAdd(__nv_bfloat162 *address, __nv_bfloat162 val);
__nv_bfloat16 atomicAdd(__nv_bfloat16 *address, __nv_bfloat16 val);</pre><p class="p">
                                    reads the 16-bit, 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(old + val)</samp>,
                                    and stores the result back to memory at the same address. These three operations are
                                    performed in one atomic transaction. The function returns <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                                 <p class="p">
                                    The 32-bit floating-point version of <samp class="ph codeph">atomicAdd()</samp> is only supported by devices of
                                    compute capability 2.x and higher.
                                    
                                 </p>
                                 <p class="p">
                                    The 64-bit floating-point version of <samp class="ph codeph">atomicAdd()</samp> is only supported by devices of
                                    compute capability 6.x and higher.
                                    
                                 </p>
                                 <p class="p">
                                    The 32-bit <samp class="ph codeph">__half2</samp> floating-point version of <samp class="ph codeph">atomicAdd()</samp> is only supported by devices of
                                    compute capability 6.x and higher.
                                    The atomicity of the <samp class="ph codeph">__half2</samp> or <samp class="ph codeph">__nv_bfloat162</samp> add operation is guaranteed separately for 
                                    each of the two <samp class="ph codeph">__half</samp> or <samp class="ph codeph">__nv_bfloat16</samp> elements; the entire <samp class="ph codeph">__half2</samp> or 
                                    <samp class="ph codeph">__nv_bfloat162</samp> is not guaranteed to be atomic as a single 32-bit access.
                                    
                                 </p>
                                 <p class="p">
                                    The 16-bit <samp class="ph codeph">__half</samp> floating-point version of
                                    <samp class="ph codeph">atomicAdd()</samp> is only supported by devices of compute
                                    capability 7.x and higher.
                                    
                                 </p>
                                 <p class="p">
                                    The 16-bit <samp class="ph codeph">__nv_bfloat16</samp> floating-point version of
                                    <samp class="ph codeph">atomicAdd()</samp> is only supported by devices of compute
                                    capability 8.x and higher.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicsub"><a name="atomicsub" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicsub" name="atomicsub" shape="rect">B.14.1.2.&nbsp;atomicSub()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicSub(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicSub(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
</pre>
                                    reads the 32-bit word <samp class="ph codeph">old</samp> located at the address <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(old - val)</samp>, and stores the result back to memory at the same address. These three operations are performed in one atomic transaction.
                                    The function returns <samp class="ph codeph">old</samp>.
                                    
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicexch"><a name="atomicexch" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicexch" name="atomicexch" shape="rect">B.14.1.3.&nbsp;atomicExch()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicExch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicExch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicExch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> atomicExch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> val);</pre><p class="p">
                                    reads the 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory and stores <samp class="ph codeph">val</samp>
                                    back to memory at the same address. These two operations are performed in one atomic
                                    transaction. The function returns <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicmin"><a name="atomicmin" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicmin" name="atomicmin" shape="rect">B.14.1.4.&nbsp;atomicMin()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicMin(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicMin(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicMin(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">
                                    reads the 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address <samp class="ph codeph">address</samp> in
                                    global or shared memory, computes the minimum of <samp class="ph codeph">old</samp> and <samp class="ph codeph">val</samp>,
                                    and stores the result back to memory at the same address. These three operations are performed in
                                    one atomic transaction. The function returns <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                                 <p class="p">
                                    The 64-bit version of <samp class="ph codeph">atomicMin()</samp> is only supported by devices of
                                    compute capability 3.5 and higher.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicmax"><a name="atomicmax" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicmax" name="atomicmax" shape="rect">B.14.1.5.&nbsp;atomicMax()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicMax(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicMax(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicMax(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">
                                    reads the 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address <samp class="ph codeph">address</samp> in
                                    global or shared memory, computes the maximum of <samp class="ph codeph">old</samp> and
                                    <samp class="ph codeph">val</samp>, and stores the result back to memory at the same address.
                                    These three operations are performed in one atomic transaction. The function returns
                                    <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                                 <p class="p">
                                    The 64-bit version of <samp class="ph codeph">atomicMax()</samp> is only supported by devices of
                                    compute capability 3.5 and higher.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicinc"><a name="atomicinc" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicinc" name="atomicinc" shape="rect">B.14.1.6.&nbsp;atomicInc()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicInc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">reads the 32-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">((old &gt;=
                                       val) ? 0 : (old+1))</samp>, and stores the result back to memory at the same
                                    address. These three operations are performed in one atomic transaction. The
                                    function returns <samp class="ph codeph">old</samp>. 
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicdec"><a name="atomicdec" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicdec" name="atomicdec" shape="rect">B.14.1.7.&nbsp;atomicDec()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicDec(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">reads the 32-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(((old ==
                                       0) || (old &gt; val)) ? val : (old-1) </samp> ), and stores the result back to
                                    memory at the same address. These three operations are performed in one atomic
                                    transaction. The function returns <samp class="ph codeph">old</samp>. 
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomiccas"><a name="atomiccas" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomiccas" name="atomiccas" shape="rect">B.14.1.8.&nbsp;atomicCAS()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicCAS(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> compare, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicCAS(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> compare,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicCAS(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> compare,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicCAS(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *address, 
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> compare, 
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">short</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">
                                    reads the 16-bit, 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(old ==
                                       compare ? val : old) </samp>, and stores the result back to memory at the same
                                    address. These three operations are performed in one atomic transaction. The
                                    function returns <samp class="ph codeph">old</samp> (Compare And Swap).
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="bitwise-functions"><a name="bitwise-functions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#bitwise-functions" name="bitwise-functions" shape="rect">B.14.2.&nbsp;Bitwise Functions</a></h3>
                        <div class="body conbody">
                           <p class="p"></p>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicand"><a name="atomicand" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicand" name="atomicand" shape="rect">B.14.2.1.&nbsp;atomicAnd()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicAnd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicAnd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicAnd(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">
                                    reads the 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(old
                                       &amp; val</samp>), and stores the result back to memory at the same
                                    address. These three operations are performed in one atomic transaction. The
                                    function returns <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                                 <p class="p">
                                    The 64-bit version of <samp class="ph codeph">atomicAnd()</samp> is only supported by devices of
                                    compute capability 3.5 and higher.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicor"><a name="atomicor" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicor" name="atomicor" shape="rect">B.14.2.2.&nbsp;atomicOr()</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicOr(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicOr(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicOr(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">
                                    reads the 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(old |
                                       val)</samp>, and stores the result back to memory at the same address. These
                                    three operations are performed in one atomic transaction. The function returns
                                    <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                                 <p class="p">
                                    The 64-bit version of <samp class="ph codeph">atomicOr()</samp> is only supported by devices of
                                    compute capability 3.5 and higher.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="atomicxor"><a name="atomicxor" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#atomicxor" name="atomicxor" shape="rect">B.14.2.3.&nbsp;atomicXor()</a></h3>
                           <div class="body refbody">
                              <div class="example"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicXor(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicXor(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> atomicXor(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* address,
                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val);</pre><p class="p">
                                    reads the 32-bit or 64-bit word <samp class="ph codeph">old</samp> located at the address
                                    <samp class="ph codeph">address</samp> in global or shared memory, computes <samp class="ph codeph">(old ^
                                       val)</samp>, and stores the result back to memory at the same address. These
                                    three operations are performed in one atomic transaction. The function returns
                                    <samp class="ph codeph">old</samp>.
                                    
                                 </p>
                                 <p class="p">
                                    The 64-bit version of <samp class="ph codeph">atomicXor()</samp> is only supported by devices of
                                    compute capability 3.5 and higher.
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="address-space-predicate-functions"><a name="address-space-predicate-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#address-space-predicate-functions" name="address-space-predicate-functions" shape="rect">B.15.&nbsp;Address Space Predicate Functions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">
                              The functions described in this section have unspecified behavior if the argument is a null pointer.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="isGlobal"><a name="isGlobal" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#isGlobal" name="isGlobal" shape="rect">B.15.1.&nbsp;__isGlobal()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __isGlobal(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> Returns 1 if <samp class="ph codeph">ptr</samp> contains the generic address of an object in global memory space, otherwise returns 0. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="isShared"><a name="isShared" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#isShared" name="isShared" shape="rect">B.15.2.&nbsp;__isShared()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __isShared(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> Returns 1 if <samp class="ph codeph">ptr</samp> contains the generic address of an object in shared memory space, otherwise returns 0. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="isConstant"><a name="isConstant" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#isConstant" name="isConstant" shape="rect">B.15.3.&nbsp;__isConstant()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __isConstant(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> Returns 1 if <samp class="ph codeph">ptr</samp> contains the generic address of an object in constant memory space, otherwise returns 0. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="isLocal"><a name="isLocal" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#isLocal" name="isLocal" shape="rect">B.15.4.&nbsp;__isLocal()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __isLocal(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> Returns 1 if <samp class="ph codeph">ptr</samp> contains the generic address of an object in local memory space, otherwise returns 0. 
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="address-space-conversion-functions"><a name="address-space-conversion-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#address-space-conversion-functions" name="address-space-conversion-functions" shape="rect">B.16.&nbsp;Address Space Conversion Functions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn"></div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_generic_to_global"><a name="__cvta_generic_to_global" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_generic_to_global" name="__cvta_generic_to_global" shape="rect">B.16.1.&nbsp;__cvta_generic_to_global()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> size_t __cvta_generic_to_global(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> 
                                 Returns the result of executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.to.global</samp> instruction 
                                 on the generic address denoted by <samp class="ph codeph">ptr</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_generic_to_shared"><a name="__cvta_generic_to_shared" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_generic_to_shared" name="__cvta_generic_to_shared" shape="rect">B.16.2.&nbsp;__cvta_generic_to_shared()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> size_t __cvta_generic_to_shared(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> 
                                 Returns the result of executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.to.shared</samp> instruction 
                                 on the generic address denoted by <samp class="ph codeph">ptr</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_generic_to_constant"><a name="__cvta_generic_to_constant" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_generic_to_constant" name="__cvta_generic_to_constant" shape="rect">B.16.3.&nbsp;__cvta_generic_to_constant()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> size_t __cvta_generic_to_constant(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> 
                                 Returns the result of executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.to.const</samp> instruction 
                                 on the generic address denoted by <samp class="ph codeph">ptr</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_generic_to_local"><a name="__cvta_generic_to_local" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_generic_to_local" name="__cvta_generic_to_local" shape="rect">B.16.4.&nbsp;__cvta_generic_to_local()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> size_t __cvta_generic_to_local(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *ptr);</pre><p class="p"> 
                                 Returns the result of executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.to.local</samp> instruction 
                                 on the generic address denoted by <samp class="ph codeph">ptr</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_global_to_generic"><a name="__cvta_global_to_generic" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_global_to_generic" name="__cvta_global_to_generic" shape="rect">B.16.5.&nbsp;__cvta_global_to_generic()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * __cvta_global_to_generic(size_t rawbits);</pre><p class="p"> 
                                 Returns the generic pointer obtained by executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.global</samp> instruction 
                                 on the value provided by <samp class="ph codeph">rawbits</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_shared_to_generic"><a name="__cvta_shared_to_generic" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_shared_to_generic" name="__cvta_shared_to_generic" shape="rect">B.16.6.&nbsp;__cvta_shared_to_generic()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * __cvta_shared_to_generic(size_t rawbits);</pre><p class="p"> 
                                 Returns the generic pointer obtained by executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.shared</samp> instruction 
                                 on the value provided by <samp class="ph codeph">rawbits</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_constant_to_generic"><a name="__cvta_constant_to_generic" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_constant_to_generic" name="__cvta_constant_to_generic" shape="rect">B.16.7.&nbsp;__cvta_constant_to_generic()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * __cvta_constant_to_generic(size_t rawbits);</pre><p class="p"> 
                                 Returns the generic pointer obtained by executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.const</samp> instruction 
                                 on the value provided by <samp class="ph codeph">rawbits</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__cvta_local_to_generic"><a name="__cvta_local_to_generic" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__cvta_local_to_generic" name="__cvta_local_to_generic" shape="rect">B.16.8.&nbsp;__cvta_local_to_generic()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * __cvta_local_to_generic(size_t rawbits);</pre><p class="p"> 
                                 Returns the generic pointer obtained by executing the <dfn class="term">PTX</dfn><samp class="ph codeph">cvta.local</samp> instruction 
                                 on the value provided by <samp class="ph codeph">rawbits</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="alloca-function"><a name="alloca-function" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#alloca-function" name="alloca-function" shape="rect">B.17.&nbsp;Alloca Function</a></h3>
                     <div class="topic reference nested2" xml:lang="en-US" id="alloca-synopsis"><a name="alloca-synopsis" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#alloca-synopsis" name="alloca-synopsis" shape="rect">B.17.1.&nbsp;Synopsis</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">
              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * alloca(size_t size);</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="alloca-description"><a name="alloca-description" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#alloca-description" name="alloca-description" shape="rect">B.17.2.&nbsp;Description</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The <samp class="ph codeph">alloca()</samp> function allocates <samp class="ph codeph">size</samp> bytes of memory in the stack frame of the caller. The retured value is a pointer to allocated memory, the beginning of the
                                 memory is 16 bytes aligned when the function is invoked from device code. The allocated memory is automatically freed when
                                 the caller to <samp class="ph codeph">alloca()</samp> is returned.
                              </p>
                              <p class="p">It is supported with compute capability 5.2 or higher.</p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="alloca-example"><a name="alloca-example" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#alloca-example" name="alloca-example" shape="rect">B.17.3.&nbsp;Example</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> num) {
	int4 *ptr = (int4 *)alloca(num * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(int4));
	<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// use of ptr</span>
	...
}
</pre></div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="compiler-optimization-hint-functions"><a name="compiler-optimization-hint-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compiler-optimization-hint-functions" name="compiler-optimization-hint-functions" shape="rect">B.18.&nbsp;Compiler Optimization Hint Functions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">
                              The functions described in this section can be used to provide additional information to the compiler
                              optimizer.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__builtin_assume_aligned"><a name="__builtin_assume_aligned" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__builtin_assume_aligned" name="__builtin_assume_aligned" shape="rect">B.18.1.&nbsp;__builtin_assume_aligned()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * __builtin_assume_aligned (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *exp, size_t align)</pre><p class="p"> Allows the compiler to assume that the argument pointer is aligned to at least <samp class="ph codeph">align</samp> bytes, and returns the argument pointer.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *res = __builtin_assume_aligned(ptr, 32); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compiler can assume 'res' is</span>
                                               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// at least 32-byte aligned</span>
     </pre><p class="p">Three parameter version: </p><pre xml:space="preserve">
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> * __builtin_assume_aligned (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *exp, size_t align, 
                                       &lt;integral type&gt; offset)</pre><p class="p"> Allows the compiler to assume that <samp class="ph codeph">(char *)exp - offset</samp> is aligned to at least <samp class="ph codeph">align</samp> bytes, and returns the argument pointer.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *res = __builtin_assume_aligned(ptr, 32, 8); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compiler can assume </span>
                                                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// '(char *)res - 8' is</span>
                                                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// at least 32-byte aligned.</span></pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__builtin_assume"><a name="__builtin_assume" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__builtin_assume" name="__builtin_assume" shape="rect">B.18.2.&nbsp;__builtin_assume()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __builtin_assume(bool exp)</pre><p class="p"> Allows the compiler to assume that the boolean argument is true. If the argument is not true at run time, then the
                                 behavior is undefined. The argument is not evaluated, so any side-effects will be discarded.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> get(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> idx) {
       __builtin_assume(idx &lt;= 2);
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> ptr[idx];
    }</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__assume"><a name="__assume" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__assume" name="__assume" shape="rect">B.18.3.&nbsp;__assume()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __assume(bool exp)</pre><p class="p"> Allows the compiler to assume that the boolean argument is true. If the argument is not true at run time, then the
                                 behavior is undefined. The argument is not evaluated, so any side-effects will be discarded.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> get(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> idx) {
       __assume(idx &lt;= 2);
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> ptr[idx];
    }</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__builtin_expect"><a name="__builtin_expect" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__builtin_expect" name="__builtin_expect" shape="rect">B.18.4.&nbsp;__builtin_expect()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> __builtin_expect (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> exp, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">long</span> c)</pre><p class="p"> Indicates to the compiler that it is expected that <samp class="ph codeph">exp == c</samp>, and returns 
                                 the value of <samp class="ph codeph">exp</samp>. Typically used to indicate branch prediction information to the compiler.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// indicate to the compiler that likely "var == 0", </span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// so the body of the if-block is unlikely to be</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// executed at run time.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (__builtin_expect (var, 0))
      doit ();
    </pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="__builtin_unreachable"><a name="__builtin_unreachable" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#__builtin_unreachable" name="__builtin_unreachable" shape="rect">B.18.5.&nbsp;__builtin_unreachable()</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __builtin_unreachable(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)</pre><p class="p"> Indicates to the compiler that control flow never reaches the point where this function is 
                                 being called from. The program has undefined behavior if the control flow does actually
                                 reach this point at run time.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// indicates to the compiler that the default case label is never reached.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">switch</span> (in) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">case</span> 1: <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 4;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">case</span> 2: <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">default</span>: __builtin_unreachable();
    }</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="compiler-optimization-hint-functions-restrictions"><a name="compiler-optimization-hint-functions-restrictions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#compiler-optimization-hint-functions-restrictions" name="compiler-optimization-hint-functions-restrictions" shape="rect">B.18.6.&nbsp;Restrictions</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p"><samp class="ph codeph">__assume()</samp> is only supported when using <samp class="ph codeph">cl.exe</samp> host compiler. 
                                 The other functions are supported on all platforms, subject to the following restrictions:
                              </p>
                              <ul class="ul">
                                 <li class="li">If the host compiler supports the function, the function can be invoked from anywhere in translation
                                    unit.
                                 </li>
                                 <li class="li">Otherwise, the function must be invoked from within the body of a <samp class="ph codeph">__device__</samp>/
                                    <samp class="ph codeph">__global__</samp>function, or only when the <samp class="ph codeph">__CUDA_ARCH__</samp>
                                    macro is defined<a name="fnsrc_12" href="#fntarg_12" shape="rect"><sup>12</sup></a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="warp-vote-functions"><a name="warp-vote-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#warp-vote-functions" name="warp-vote-functions" shape="rect">B.19.&nbsp;Warp Vote Functions</a></h3>
                     <div class="body conbody">
                        <div class="p"><pre xml:space="preserve">
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __all_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> predicate);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __any_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> predicate);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __ballot_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> predicate);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __activemask();
        </pre></div>
                        <p class="p">Deprecation notice: <samp class="ph codeph">__any</samp>, <samp class="ph codeph">__all</samp>, and <samp class="ph codeph">__ballot</samp>
                           have been deprecated in CUDA 9.0 for all devices.
                        </p>
                        <p class="p">Removal notice: When targeting devices with compute capability 7.x or higher,
                           <samp class="ph codeph">__any</samp>, <samp class="ph codeph">__all</samp>, and <samp class="ph codeph">__ballot</samp> are no longer available and
                           their sync variants should be used instead.
                        </p>
                        <p class="p">The warp vote functions allow the threads of a given <a class="xref" href="index.html#simt-architecture" shape="rect">warp</a> to perform a
                           reduction-and-broadcast operation. These functions take as input an
                           integer <samp class="ph codeph">predicate</samp> from each thread in the warp and
                           compare those values with zero. The results of the comparisons are
                           combined (reduced) across the <a class="xref" href="index.html#simt-architecture__notes" shape="rect">active</a> threads of the warp
                           in one of the following ways, broadcasting a single return value to
                           each participating thread:
                        </p>
                        <dl class="dl">
                           <dt class="dt dlterm"><samp class="ph codeph">__all_sync(unsigned mask, predicate)</samp>:
                           </dt>
                           <dd class="dd">Evaluate <samp class="ph codeph">predicate</samp> for all non-exited threads in <samp class="ph codeph">mask</samp>
                              and return non-zero if and only if <samp class="ph codeph">predicate</samp>
                              evaluates to non-zero for all of them.
                           </dd>
                           <dt class="dt dlterm"><samp class="ph codeph">__any_sync(unsigned mask, predicate)</samp>:
                           </dt>
                           <dd class="dd">Evaluate <samp class="ph codeph">predicate</samp> for all non-exited threads in <samp class="ph codeph">mask</samp>
                              and return non-zero if and only if <samp class="ph codeph">predicate</samp>
                              evaluates to non-zero for any of them.
                           </dd>
                           <dt class="dt dlterm"><samp class="ph codeph">__ballot_sync(unsigned mask, predicate)</samp>:
                           </dt>
                           <dd class="dd">Evaluate <samp class="ph codeph">predicate</samp> for all non-exited threads in <samp class="ph codeph">mask</samp>
                              and return an integer whose Nth bit is set if and only if
                              <samp class="ph codeph">predicate</samp> evaluates to non-zero for the Nth thread
                              of the warp and the Nth thread is active.
                           </dd>
                           <dt class="dt dlterm"><samp class="ph codeph">__activemask()</samp>:
                           </dt>
                           <dd class="dd"> Returns a 32-bit integer mask of all currently active threads in
                              the calling warp. The Nth bit is set if the Nth lane in the warp is active
                              when <samp class="ph codeph">__activemask()</samp> is called. <a class="xref" href="index.html#simt-architecture__notes" shape="rect">Inactive</a>
                              threads are represented by 0 bits in the returned mask. Threads which have
                              exited the program are always marked as inactive.
                              Note that threads that are convergent at an <samp class="ph codeph">__activemask()</samp>
                              call are not guaranteed to be convergent at subsequent instructions unless
                              those instructions are synchronizing warp-builtin functions.
                           </dd>
                        </dl>
                        <div class="section" id="warp-vote-functions__notes"><a name="warp-vote-functions__notes" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Notes</h3>
                           <p class="p">For <samp class="ph codeph">__all_sync</samp>, <samp class="ph codeph">__any_sync</samp>, and
                              <samp class="ph codeph">__ballot_sync</samp>, a mask must be passed that specifies the threads
                              participating in the call. A bit, representing the thread's lane ID, must be
                              set for each participating thread to ensure they are properly converged before
                              the intrinsic is executed by the hardware.
                              All active threads named in mask must execute the same intrinsic with the same mask,
                              or the result is undefined.
                              
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="warp-match-functions"><a name="warp-match-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#warp-match-functions" name="warp-match-functions" shape="rect">B.20.&nbsp;Warp Match Functions</a></h3>
                     <div class="body conbody">
                        <p class="p"><samp class="ph codeph">__match_any_sync</samp> and <samp class="ph codeph">__match_all_sync</samp> perform a broadcast-and-compare operation
                           of a variable between threads within a <a class="xref" href="index.html#simt-architecture" shape="rect">warp</a>.
                           
                        </p>
                        <p class="p">Supported by devices of compute capability 7.x or higher.</p>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="synopsis-match"><a name="synopsis-match" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#synopsis-match" name="synopsis-match" shape="rect">B.20.1.&nbsp;Synopsys</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">
        
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __match_any_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, T value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __match_all_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, T value, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *pred);
        
      </pre><p class="p"><samp class="ph codeph">T</samp> can be <samp class="ph codeph">int</samp>, <samp class="ph codeph">unsigned int</samp>,
                                 <samp class="ph codeph">long</samp>, <samp class="ph codeph">unsigned long</samp>, <samp class="ph codeph">long long</samp>,
                                 <samp class="ph codeph">unsigned long long</samp>, <samp class="ph codeph">float</samp> or <samp class="ph codeph">double</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="warp-description-match"><a name="warp-description-match" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp-description-match" name="warp-description-match" shape="rect">B.20.2.&nbsp;Description</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The <samp class="ph codeph">__match_sync()</samp> intrinsics permit a broadcast-and-compare of
                                 a value <samp class="ph codeph">value</samp> across threads in a warp after synchronizing threads
                                 named in <samp class="ph codeph">mask</samp>.
                                 
                              </p>
                              <dl class="dl">
                                 <dt class="dt dlterm"><samp class="ph codeph">__match_any_sync</samp></dt>
                                 <dd class="dd">Returns mask of threads that have same value of <samp class="ph codeph">value</samp> in <samp class="ph codeph">mask</samp></dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">__match_all_sync</samp></dt>
                                 <dd class="dd">Returns <samp class="ph codeph">mask</samp> if all threads in <samp class="ph codeph">mask</samp> have the same value
                                    for <samp class="ph codeph">value</samp>; otherwise 0 is returned.
                                    Predicate <samp class="ph codeph">pred</samp> is set to true if all threads in <samp class="ph codeph">mask</samp> have
                                    the same value of <samp class="ph codeph">value</samp>; otherwise the predicate is set to false.
                                 </dd>
                              </dl>
                              <p class="p">The new <samp class="ph codeph">*_sync</samp> match intrinsics take in a mask indicating the
                                 threads participating in the call. A bit, representing the thread's lane id, must
                                 be set for each participating thread to ensure they are properly converged before
                                 the intrinsic is executed by the hardware.
                                 All non-exited threads named in mask must execute the same intrinsic with the same mask,
                                 or the result is undefined.
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="warp-reduce-functions"><a name="warp-reduce-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#warp-reduce-functions" name="warp-reduce-functions" shape="rect">B.21.&nbsp;Warp Reduce Functions</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The <samp class="ph codeph">__reduce_sync(unsigned mask, T value)</samp> intrinsics perform a reduction operation on the data provided in <samp class="ph codeph">value</samp> after synchronizing threads named in <samp class="ph codeph">mask</samp>. T can be unsigned or signed for {add, min, max} and unsigned only for {and, or, xor} operations.
                           
                        </p>
                        <p class="p">Supported by devices of compute capability 8.x or higher.</p>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="warp-reduce-synopsis"><a name="warp-reduce-synopsis" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp-reduce-synopsis" name="warp-reduce-synopsis" shape="rect">B.21.1.&nbsp;Synopsys</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">
      
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// add/min/max</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __reduce_add_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __reduce_min_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __reduce_max_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __reduce_add_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __reduce_min_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> __reduce_max_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// and/or/xor</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __reduce_and_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __reduce_or_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> value);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> __reduce_xor_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> value);
      
      </pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="warp-reduce-description"><a name="warp-reduce-description" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp-reduce-description" name="warp-reduce-description" shape="rect">B.21.2.&nbsp;Description</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <dl class="dl">
                                 <dt class="dt dlterm"><samp class="ph codeph">__reduce_add_sync</samp>, <samp class="ph codeph">__reduce_min_sync</samp>, <samp class="ph codeph">__reduce_max_sync</samp></dt>
                                 <dd class="dd">
                                    Returns the result of applying an arithmetic add, min, or max reduction operation on the values provided in <samp class="ph codeph">value</samp> by each thread named in <samp class="ph codeph">mask</samp>.
                                    
                                 </dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">__reduce_and_sync</samp>, <samp class="ph codeph">__reduce_or_sync</samp>, <samp class="ph codeph">__reduce_xor_sync</samp></dt>
                                 <dd class="dd">
                                    Returns the result of applying a logical AND, OR, or XOR reduction operation on the values provided in <samp class="ph codeph">value</samp> by each thread named in <samp class="ph codeph">mask</samp>.
                                    
                                 </dd>
                              </dl>
                              <p class="p">The <samp class="ph codeph">mask</samp> indicates the threads participating in the call. A bit, representing the thread's lane id, must be set for each participating
                                 thread to ensure they are properly converged before the intrinsic is executed by the hardware. All non-exited threads named
                                 in mask must execute the same intrinsic with the same mask, or the result is undefined.
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="warp-shuffle-functions"><a name="warp-shuffle-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#warp-shuffle-functions" name="warp-shuffle-functions" shape="rect">B.22.&nbsp;Warp Shuffle Functions</a></h3>
                     <div class="body conbody">
                        <p class="p"><samp class="ph codeph">__shfl_sync</samp>, <samp class="ph codeph">__shfl_up_sync</samp>, <samp class="ph codeph">__shfl_down_sync</samp>, and <samp class="ph codeph">__shfl_xor_sync</samp>
                           exchange a variable between threads within a <a class="xref" href="index.html#simt-architecture" shape="rect">warp</a>.
                           
                        </p>
                        <p class="p">Supported by devices of compute capability 3.x or higher.</p>
                        <p class="p">Deprecation Notice:
                           <samp class="ph codeph">__shfl</samp>, <samp class="ph codeph">__shfl_up</samp>, <samp class="ph codeph">__shfl_down</samp>, and <samp class="ph codeph">__shfl_xor</samp>
                           have been deprecated in CUDA 9.0 for all devices.
                        </p>
                        <p class="p">Removal Notice:
                           When targeting devices with compute capability 7.x or higher,
                           <samp class="ph codeph">__shfl</samp>, <samp class="ph codeph">__shfl_up</samp>, <samp class="ph codeph">__shfl_down</samp>, and <samp class="ph codeph">__shfl_xor</samp>
                           are no longer available and their sync variants should be used instead.
                        </p>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="synopsis"><a name="synopsis" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#synopsis" name="synopsis" shape="rect">B.22.1.&nbsp;Synopsis</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">
              
T __shfl_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, T var, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> srcLane, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width=warpSize);
T __shfl_up_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, T var, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> delta, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width=warpSize);
T __shfl_down_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, T var, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> delta, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width=warpSize);
T __shfl_xor_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> mask, T var, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> laneMask, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> width=warpSize);
              </pre><p class="p"><samp class="ph codeph">T</samp> can be <samp class="ph codeph">int</samp>, <samp class="ph codeph">unsigned int</samp>,
                                 <samp class="ph codeph">long</samp>, <samp class="ph codeph">unsigned long</samp>, <samp class="ph codeph">long long</samp>,
                                 <samp class="ph codeph">unsigned long long</samp>, <samp class="ph codeph">float</samp> or <samp class="ph codeph">double</samp>.
                                 With the <samp class="ph codeph">cuda_fp16.h</samp> header included,
                                 <samp class="ph codeph">T</samp> can also be <samp class="ph codeph">__half</samp> or <samp class="ph codeph">__half2</samp>.
                                 Similarly, with the <samp class="ph codeph">cuda_bf16.h</samp> header included,
                                 <samp class="ph codeph">T</samp> can also be <samp class="ph codeph">__nv_bfloat16</samp> or <samp class="ph codeph">__nv_bfloat162</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="warp-description"><a name="warp-description" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp-description" name="warp-description" shape="rect">B.22.2.&nbsp;Description</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The <samp class="ph codeph">__shfl_sync()</samp> intrinsics permit exchanging of a
                                 variable between threads within a warp without use of shared memory.
                                 The exchange occurs simultaneously for all <a class="xref" href="index.html#simt-architecture__notes" shape="rect">active</a> threads within the
                                 warp (and named in <samp class="ph codeph">mask</samp>), moving 4 or 8 bytes of data per thread depending on the type.
                              </p>
                              <p class="p">Threads within a warp are referred to as <dfn class="term">lanes</dfn>, and may have an index between 0 and
                                 <samp class="ph codeph">warpSize-1</samp> (inclusive). Four source-lane addressing
                                 modes are supported:
                              </p>
                              <dl class="dl">
                                 <dt class="dt dlterm"><samp class="ph codeph">__shfl_sync()</samp></dt>
                                 <dd class="dd">Direct copy from indexed lane</dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">__shfl_up_sync()</samp></dt>
                                 <dd class="dd">Copy from a lane with lower ID relative to caller</dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">__shfl_down_sync()</samp></dt>
                                 <dd class="dd">Copy from a lane with higher ID relative to caller</dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">__shfl_xor_sync()</samp></dt>
                                 <dd class="dd">Copy from a lane based on bitwise XOR of own lane ID</dd>
                              </dl>
                              <p class="p">Threads may only read data from another thread which is actively
                                 participating in the <samp class="ph codeph">__shfl_sync()</samp> command. If the target
                                 thread is <a class="xref" href="index.html#simt-architecture__notes" shape="rect">inactive</a>, the
                                 retrieved value is undefined.
                              </p>
                              <p class="p">All of the <samp class="ph codeph">__shfl_sync()</samp> intrinsics take an optional <samp class="ph codeph">width</samp>
                                 parameter which alters the behavior of the intrinsic. <samp class="ph codeph">width</samp>
                                 must have a value which is a power of 2; results are undefined if width is not a power of 2,
                                 or is a number greater than <samp class="ph codeph">warpSize</samp>.
                              </p>
                              <p class="p"><samp class="ph codeph">__shfl_sync()</samp> returns the value of <samp class="ph codeph">var</samp> held
                                 by the thread whose ID is given by <samp class="ph codeph">srcLane</samp>. If
                                 width is less than <samp class="ph codeph">warpSize</samp> then each subsection of the
                                 warp behaves as a separate entity with a starting logical lane ID of 0.
                                 If <samp class="ph codeph">srcLane</samp> is outside the range
                                 <samp class="ph codeph">[0:width-1]</samp>, the value returned corresponds to the value of var held by
                                 the <samp class="ph codeph">srcLane modulo width</samp> (i.e. within the same subsection).
                              </p>
                              <p class="p"><samp class="ph codeph">__shfl_up_sync()</samp> calculates a source lane ID by subtracting
                                 <samp class="ph codeph">delta</samp> from the caller's lane ID. The value of
                                 <samp class="ph codeph">var</samp> held by the resulting lane ID is returned: in
                                 effect, <samp class="ph codeph">var</samp> is shifted up the warp by
                                 <samp class="ph codeph">delta</samp> lanes. 
                                 If
                                 width is less than <samp class="ph codeph">warpSize</samp> then each subsection of the
                                 warp behaves as a separate entity with a starting logical lane ID of 0.
                                 The source lane index will not wrap around
                                 the value of <samp class="ph codeph">width</samp>, so effectively the lower
                                 <samp class="ph codeph">delta</samp> lanes will be unchanged.
                              </p>
                              <p class="p"><samp class="ph codeph">__shfl_down_sync()</samp> calculates a source lane ID by adding
                                 <samp class="ph codeph">delta</samp> to the caller's lane ID. The value of
                                 <samp class="ph codeph">var</samp> held by the resulting lane ID is returned: this has
                                 the effect of shifting <samp class="ph codeph">var</samp> down the warp by
                                 <samp class="ph codeph">delta</samp> lanes. 
                                 If
                                 width is less than <samp class="ph codeph">warpSize</samp> then each subsection of the
                                 warp behaves as a separate entity with a starting logical lane ID of 0.
                                 As for <samp class="ph codeph">__shfl_up_sync()</samp>, the ID
                                 number of the source lane will not wrap around the value of width and so
                                 the upper <samp class="ph codeph">delta</samp> lanes will remain unchanged.
                              </p>
                              <p class="p"><samp class="ph codeph">__shfl_xor_sync()</samp> calculates a source line ID by performing
                                 a bitwise XOR of the caller's lane ID with <samp class="ph codeph">laneMask</samp>: the
                                 value of <samp class="ph codeph">var</samp> held by the resulting lane ID is returned.
                                 If <samp class="ph codeph">width</samp> is less than <samp class="ph codeph">warpSize</samp> then each group of <samp class="ph codeph">width</samp> consecutive 
                                 threads are able to access elements from earlier groups of threads, 
                                 however if they attempt to access elements from later groups of threads their own value
                                 of <samp class="ph codeph">var</samp> will be returned.
                                 This mode implements a butterfly addressing pattern such as is
                                 used in tree reduction and broadcast.
                              </p>
                              <p class="p">The new <samp class="ph codeph">*_sync</samp> shfl intrinsics take in a mask indicating the
                                 threads participating in the call. A bit, representing the thread's lane id, must
                                 be set for each participating thread to ensure they are properly converged before
                                 the intrinsic is executed by the hardware.
                                 All non-exited threads named in mask must execute the same intrinsic with the same mask,
                                 or the result is undefined.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="warp-notes"><a name="warp-notes" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp-notes" name="warp-notes" shape="rect">B.22.3.&nbsp;Notes</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Threads may only read data from another thread which is actively
                                 participating in the <samp class="ph codeph">__shfl_sync()</samp> command. If the target
                                 thread is inactive, the retrieved value is undefined.
                              </p>
                              <p class="p"><samp class="ph codeph">width</samp> must be a power-of-2 (i.e., 2, 4, 8, 16 or 32).
                                 Results are unspecified for other values.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="warp-examples"><a name="warp-examples" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp-examples" name="warp-examples" shape="rect">B.22.4.&nbsp;Examples</a></h3>
                        <div class="topic reference nested3" xml:lang="en-US" id="warp-examples-broadcast"><a name="warp-examples-broadcast" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#warp-examples-broadcast" name="warp-examples-broadcast" shape="rect">B.22.4.1.&nbsp;Broadcast of a single value across a warp</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">#include &lt;stdio.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bcast(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> arg) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> laneId = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x &amp; 0x1f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (laneId == 0)        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note unused variable for</span>
        value = arg;        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// all threads except lane 0</span>
    value = __shfl_sync(0xffffffff, value, 0);   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Synchronize all threads in warp, and get "value" from lane 0</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (value != arg)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Thread %d failed.\n"</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    bcast<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 32 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(1234);
    cudaDeviceSynchronize();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" id="warp-examples-inclusive"><a name="warp-examples-inclusive" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#warp-examples-inclusive" name="warp-examples-inclusive" shape="rect">B.22.4.2.&nbsp;Inclusive plus-scan across sub-partitions of 8 threads</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">#include &lt;stdio.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> scan4() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> laneId = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x &amp; 0x1f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Seed sample starting value (inverse of lane ID)</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value = 31 - laneId;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Loop to accumulate scan within my partition.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Scan requires log2(n) == 3 steps for 8 threads</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// It works by an accumulated sum up the warp</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// by 1, 2, 4, 8 etc. steps.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i=1; i&lt;=4; i*=2) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// We do the __shfl_sync unconditionally so that we</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// can read even from threads which won't do a</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// sum, and then conditionally assign the result.</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> n = __shfl_up_sync(0xffffffff, value, i, 8);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> ((laneId &amp; 7) &gt;= i)
            value += n;
    }

    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Thread %d final value = %d\n"</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, value);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    scan4<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 32 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}
</pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" id="warp-examples-reduction"><a name="warp-examples-reduction" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#warp-examples-reduction" name="warp-examples-reduction" shape="rect">B.22.4.3.&nbsp;Reduction across a warp</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">#include &lt;stdio.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> warpReduce() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> laneId = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x &amp; 0x1f;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Seed starting value as inverse lane ID</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value = 31 - laneId;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Use XOR mode to perform butterfly reduction</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i=16; i&gt;=1; i/=2)
        value += __shfl_xor_sync(0xffffffff, value, i, 32);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// "value" now contains the sum across all threads</span>
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Thread %d final value = %d\n"</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, value);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    warpReduce<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 32 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}
</pre></div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="scheduling"><a name="scheduling" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#scheduling" name="scheduling" shape="rect">B.23.&nbsp;Nanosleep Function</a></h3>
                     <div class="topic reference nested2" xml:lang="en-US" id="scheduling-synopsis"><a name="scheduling-synopsis" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#scheduling-synopsis" name="scheduling-synopsis" shape="rect">B.23.1.&nbsp;Synopsis</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">
              
T __nanosleep(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> ns);
              </pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="nanosleep-description"><a name="nanosleep-description" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#nanosleep-description" name="nanosleep-description" shape="rect">B.23.2.&nbsp;Description</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p"><samp class="ph codeph">__nanosleep(ns)</samp> suspends the thread for a sleep duration
                                 					approximately close to the delay <samp class="ph codeph">ns</samp>, specified in nanoseconds. 
                              </p>
                              <p class="p">It is supported with compute capability 7.0 or higher.</p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="scheduling-example"><a name="scheduling-example" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#scheduling-example" name="scheduling-example" shape="rect">B.23.3.&nbsp;Example</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The following code implements a mutex with exponential back-off.</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> mutex_lock(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *mutex) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> ns = 8;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (atomicCAS(mutex, 0, 1) == 1) {
        __nanosleep(ns);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (ns &lt; 256) {
            ns *= 2;
        }
    }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> mutex_unlock(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *mutex) {
    atomicExch(mutex, 0);
}
</pre></div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="wmma"><a name="wmma" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#wmma" name="wmma" shape="rect">B.24.&nbsp;Warp matrix functions</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           C++ warp matrix operations leverage Tensor Cores to accelerate matrix problems of the form <samp class="ph codeph">D=A*B+C</samp>. These operations are supported on mixed-precision floating point data for devices of compute capability 7.0 or higher.
                           This requires co-operation from all threads in a <a class="xref" href="index.html#simt-architecture" shape="rect">warp</a>.
                           In addition, these operations are allowed in conditional code only if the condition evaluates identically across the entire
                           <a class="xref" href="index.html#simt-architecture" shape="rect">warp</a>, otherwise the code execution is likely to hang.
                           
                        </p>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="wmma-description"><a name="wmma-description" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-description" name="wmma-description" shape="rect">B.24.1.&nbsp;Description</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">All following functions and types are defined in the namespace <samp class="ph codeph">nvcuda::wmma</samp>. Sub-byte operations are considered preview, i.e. the data structures and APIs for them are subject to change and may not
                                 be compatible with future releases. This extra functionality is defined in the <samp class="ph codeph">nvcuda::wmma::experimental</samp> namespace.
                              </p><pre xml:space="preserve">

template&lt;typename Use, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> m, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> n, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> k, typename T, typename Layout=<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>&gt; class fragment;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> load_matrix_sync(fragment&lt;...&gt; &amp;a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* mptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> ldm);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> load_matrix_sync(fragment&lt;...&gt; &amp;a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T* mptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> ldm, layout_t layout);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> store_matrix_sync(T* mptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> fragment&lt;...&gt; &amp;a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> ldm, layout_t layout);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> fill_fragment(fragment&lt;...&gt; &amp;a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T&amp; v);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> mma_sync(fragment&lt;...&gt; &amp;d, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> fragment&lt;...&gt; &amp;a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> fragment&lt;...&gt; &amp;b, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> fragment&lt;...&gt; &amp;c, bool satf=false);              </pre><dl class="dl">
                                 <dt class="dt dlterm"><samp class="ph codeph">fragment</samp></dt>
                                 <dd class="dd">
                                    <p class="p">An overloaded class containing a section of a matrix distributed across all threads in the warp. The mapping of matrix elements
                                       into <samp class="ph codeph">fragment</samp> internal storage is unspecified and subject to change in future architectures.
                                    </p>
                                    <div class="p">Only certain combinations of template arguments are allowed. The first template parameter specifies how the fragment will
                                       participate in the matrix operation. Acceptable values for <samp class="ph codeph">Use</samp> are:
                                       
                                       <ul class="ul">
                                          <li class="li"><samp class="ph codeph">matrix_a</samp> when the fragment is used as the first multiplicand, <samp class="ph codeph">A</samp>,
                                          </li>
                                          <li class="li"><samp class="ph codeph">matrix_b</samp> when the fragment is used as the second multiplicand, <samp class="ph codeph">B</samp>, or
                                          </li>
                                          <li class="li"><samp class="ph codeph">accumulator</samp> when the fragment is used as the source or destination accumulators
                                             (<samp class="ph codeph">C</samp> or <samp class="ph codeph">D</samp>, respectively).
                                          </li>
                                       </ul>
                                    </div>
                                    <p class="p">The <samp class="ph codeph">m</samp>, <samp class="ph codeph">n</samp> and <samp class="ph codeph">k</samp> sizes describe 
                                       the shape of the warp-wide matrix tiles participating in the multiply-accumulate operation. The dimension of each tile depends
                                       on its role.
                                       For <samp class="ph codeph">matrix_a</samp> the tile takes dimension <samp class="ph codeph">m x k</samp>; for <samp class="ph codeph">matrix_b</samp> the dimension is <samp class="ph codeph">k x n</samp>,
                                       and <samp class="ph codeph">accumulator</samp> tiles are <samp class="ph codeph">m x n</samp>.
                                    </p>
                                    <p class="p">The data type, <samp class="ph codeph">T</samp>, may be <samp class="ph codeph">double</samp>, <samp class="ph codeph">float</samp>, <samp class="ph codeph">__half</samp>, <samp class="ph codeph">__nv_bfloat16</samp>, <samp class="ph codeph">char</samp>, or <samp class="ph codeph">unsigned char</samp> for multiplicands
                                       and <samp class="ph codeph">double</samp>, <samp class="ph codeph">float</samp>, <samp class="ph codeph">int</samp>, or <samp class="ph codeph">__half</samp> for accumulators.
                                       As documented in <a class="xref" href="index.html#wmma-type-sizes" shape="rect">Element Types &amp; Matrix Sizes</a>, limited combinations of accumulator and multiplicand types are supported.
                                       The Layout parameter must be specified for <samp class="ph codeph">matrix_a</samp> and <samp class="ph codeph">matrix_b</samp> fragments.
                                       <samp class="ph codeph">row_major</samp> or <samp class="ph codeph">col_major</samp> indicate that elements within a matrix row or column are contiguous in memory, respectively.
                                       The <samp class="ph codeph">Layout</samp> parameter for an <samp class="ph codeph">accumulator</samp> matrix should retain the default value of <samp class="ph codeph">void</samp>.
                                       A row or column layout is specified only when the accumulator is loaded or stored as described below.
                                    </p>
                                 </dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">load_matrix_sync</samp></dt>
                                 <dd class="dd">
                                    <p class="p">Waits until all warp lanes have arrived at load_matrix_sync and then loads the matrix fragment a from memory.
                                       <samp class="ph codeph">mptr</samp> must be a 256-bit aligned pointer pointing to the first element of the matrix in memory.
                                       <samp class="ph codeph">ldm</samp> describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout)
                                       and must be a multiple of 8 for <samp class="ph codeph">__half</samp> element type or multiple of 4 for <samp class="ph codeph">float</samp> element type. (i.e., multiple of 16 bytes in both cases).
                                       If the fragment is an <samp class="ph codeph">accumulator</samp>, the <samp class="ph codeph">layout</samp> argument must be specified
                                       as either <samp class="ph codeph">mem_row_major</samp> or <samp class="ph codeph">mem_col_major</samp>.
                                       For <samp class="ph codeph">matrix_a</samp> and <samp class="ph codeph">matrix_b</samp> fragments, the layout is inferred from the fragment's <samp class="ph codeph">layout</samp> parameter.
                                       The values of <samp class="ph codeph">mptr</samp>, <samp class="ph codeph">ldm</samp>, <samp class="ph codeph">layout</samp> and all template parameters for <samp class="ph codeph">a</samp> must be the same for all threads in the warp.
                                       This function must be called by all threads in the warp, or the result is undefined.
                                    </p>
                                 </dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">store_matrix_sync</samp></dt>
                                 <dd class="dd">
                                    <p class="p">Waits until all warp lanes have arrived at store_matrix_sync and then stores the matrix fragment a to memory.
                                       <samp class="ph codeph">mptr</samp> must be a 256-bit aligned pointer pointing to the first element of the matrix in memory.
                                       <samp class="ph codeph">ldm</samp> describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout) and
                                       must be 
                                       a multiple of 8 for <samp class="ph codeph">__half</samp> element type or multiple of 4 for <samp class="ph codeph">float</samp> element type. (i.e., multiple of 16 bytes in both cases).
                                       The layout of the output matrix must be specified as either <samp class="ph codeph">mem_row_major</samp> or <samp class="ph codeph">mem_col_major</samp>.
                                       The values of <samp class="ph codeph">mptr</samp>, <samp class="ph codeph">ldm</samp>, <samp class="ph codeph">layout</samp> and all template parameters for a must be the same for all threads in the warp.
                                    </p>
                                 </dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">fill_fragment</samp></dt>
                                 <dd class="dd">
                                    <p class="p">Fill a matrix fragment with a constant value <samp class="ph codeph">v</samp>.
                                       Because the mapping of matrix elements to each fragment is unspecified,
                                       this function is ordinarily called by all threads in the warp with a common value for <samp class="ph codeph">v</samp>.
                                    </p>
                                 </dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">mma_sync</samp></dt>
                                 <dd class="dd">
                                    <p class="p">Waits until all warp lanes have arrived at mma_sync, and then performs the warp-synchronous matrix multiply-accumulate
                                       operation <samp class="ph codeph">D=A*B+C</samp>. The in-place operation, <samp class="ph codeph">C=A*B+C</samp>, is also supported.
                                       The value of <samp class="ph codeph">satf</samp> and template parameters for each matrix fragment must be the same for all threads in the warp.
                                       Also, the template parameters <samp class="ph codeph">m</samp>, <samp class="ph codeph">n</samp> and <samp class="ph codeph">k</samp> must match between
                                       fragments <samp class="ph codeph">A</samp>, <samp class="ph codeph">B</samp>, <samp class="ph codeph">C</samp> and <samp class="ph codeph">D</samp>.
                                       This function must be called by all threads in the warp, or the result is undefined.
                                    </p>
                                    <div class="p">If <samp class="ph codeph">satf</samp> (saturate to finite value) mode is <samp class="ph codeph">true</samp>,
                                       the following additional numerical properties apply for the destination accumulator:
                                       
                                       <ul class="ul">
                                          <li class="li">
                                             If an element result is +Infinity, the corresponding accumulator will contain <samp class="ph codeph">+MAX_NORM</samp></li>
                                          <li class="li">
                                             If an element result is -Infinity, the corresponding accumulator will contain <samp class="ph codeph">-MAX_NORM</samp></li>
                                          <li class="li">
                                             If an element result is NaN, the corresponding accumulator will contain <samp class="ph codeph">+0</samp></li>
                                       </ul>
                                    </div>
                                 </dd>
                              </dl>
                              <p class="p">Because the map of matrix elements into each thread's <samp class="ph codeph">fragment</samp> is unspecified,
                                 individual matrix elements must be accessed from memory (shared or global) after calling
                                 <samp class="ph codeph">store_matrix_sync</samp>. In the special case where all threads in the warp will apply an
                                 element-wise operation uniformly to all fragment elements, direct element access can be implemented using the following <samp class="ph codeph">fragment</samp> class members.
                              </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> fragment&lt;Use, m, n, k, T, Layout&gt;::num_elements;
T fragment&lt;Use, m, n, k, T, Layout&gt;::x[num_elements];</pre><p class="p">As an example, the following code scales an <samp class="ph codeph">accumulator</samp> matrix tile by half.
                              </p><pre xml:space="preserve">
wmma::fragment&lt;wmma::accumulator, 16, 16, 16, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt; frag;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> alpha = 0.5f; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Same value for all threads in warp</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/*...*/</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> t=0; t&lt;frag.num_elements; t++)
frag.x[t] *= alpha;      </pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="wmma-altfp"><a name="wmma-altfp" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-altfp" name="wmma-altfp" shape="rect">B.24.2.&nbsp;Alternate Floating Point</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 Tensor Cores support alternate types of floating point operations on devices with compute capability 8.0 and higher.
                                 
                              </p>
                              <dl class="dl">
                                 <dt class="dt dlterm"><samp class="ph codeph">__nv_bfloat16</samp></dt>
                                 <dd class="dd">
                                    <p class="p">This data format is an alternate fp16 format that has the same range as f32 but reduced precision (7 bits). You can use this
                                       data format directly with the <samp class="ph codeph">__nv_bfloat16</samp> type available in <samp class="ph codeph">cuda_bf16.h</samp>. Matrix fragments with <samp class="ph codeph">__nv_bfloat16</samp> data types are required to be composed with accumulators of <samp class="ph codeph">float</samp> type. The shapes and operations supported are the same as with <samp class="ph codeph">__half</samp>.
                                    </p>
                                 </dd>
                                 <dt class="dt dlterm"><samp class="ph codeph">tf32</samp></dt>
                                 <dd class="dd">
                                    <p class="p">This data format is a special floating point format supported by Tensor Cores, with the same range as f32 and reduced precision
                                       (&gt;=10 bits). The internal layout of this format is implementation defined. In order to use this floating point format with
                                       WMMA operations, the input matrices must be manually converted to tf32 precision.
                                    </p>
                                    <p class="p">To facilitate conversion, a new intrinsic <samp class="ph codeph">__float_to_tf32</samp> is provided. While the input and output arguments to the intrinsic are of <samp class="ph codeph">float</samp> type, the output will be <samp class="ph codeph">tf32</samp> numerically. This new precision is intended to be used with Tensor Cores only, and if mixed with other <samp class="ph codeph">float</samp>type operations, the precision and range of the result will be undefined.
                                    </p>
                                    <p class="p">Once an input matrix (<samp class="ph codeph">matrix_a</samp> or <samp class="ph codeph">matrix_b</samp>) is converted to tf32 precision, the combination of a <samp class="ph codeph">fragment</samp> with <samp class="ph codeph">precision::tf32</samp> precision, and a data type of <samp class="ph codeph">float</samp> to <samp class="ph codeph">load_matrix_sync</samp> will take advantage of this new capability. Both the accumulator fragments must have <samp class="ph codeph">float</samp> data types. The only supported matrix size is 16x16x8 (m-n-k).
                                    </p>
                                    <p class="p">The elements of the fragment are represented as <samp class="ph codeph">float</samp>, hence the mapping from <samp class="ph codeph">element_type&lt;T&gt;</samp> to <samp class="ph codeph">storage_element_type&lt;T&gt;</samp> is:
                                    </p><pre xml:space="preserve">precision::tf32 -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span></pre></dd>
                              </dl>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="wmma-double"><a name="wmma-double" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-double" name="wmma-double" shape="rect">B.24.3.&nbsp;Double Precision</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Tensor Cores support double-precision floating point operations on devices with compute capability 8.0 and higher. To use
                                 this new functionality, a <samp class="ph codeph">fragment</samp> with the <samp class="ph codeph">double</samp> type must be used. The <samp class="ph codeph">mma_sync</samp> operation will be performed with the .rn (rounds to nearest even) rounding modifier.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="wmma-subbyte"><a name="wmma-subbyte" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-subbyte" name="wmma-subbyte" shape="rect">B.24.4.&nbsp;Sub-byte Operations</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Sub-byte WMMA operations provide a way to access the low-precision capabilities of Tensor Cores.
                                 They are considered a preview feature i.e. the data structures and APIs for them are subject to change and may not be compatible
                                 with future releases.
                                 This functionality is available via the <samp class="ph codeph">nvcuda::wmma::experimental</samp> namespace:
                              </p><pre xml:space="preserve">
namespace experimental { 
    namespace precision { 
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> u4; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 4-bit unsigned </span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> s4; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 4-bit signed </span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> b1; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 1-bit </span>
   } 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> bmmaBitOp {
        bmmaBitOpXOR = 1, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compute_75 minimum</span>
        bmmaBitOpAND = 2  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compute_80 minimum</span>
    };
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">enum</span> bmmaAccumulateOp { bmmaAccumulateOpPOPC = 1 }; 
}      </pre><p class="p">For 4 bit precision, the APIs available remain the same, but you must specify <samp class="ph codeph">experimental::precision::u4</samp>
                                 or <samp class="ph codeph">experimental::precision::s4</samp> as the fragment data type.
                                 Since the elements of the fragment are packed together, <samp class="ph codeph">num_storage_elements</samp> will be smaller than <samp class="ph codeph">num_elements</samp> for that fragment.
                                 The <samp class="ph codeph">num_elements</samp> variable for a sub-byte fragment, hence returns the number of elements of sub-byte type <samp class="ph codeph">element_type&lt;T&gt;</samp>.
                                 This is true for single bit precision as well, in which case, the mapping from <samp class="ph codeph">element_type&lt;T&gt;</samp> to <samp class="ph codeph">storage_element_type&lt;T&gt;</samp> is as follows:
                              </p><pre xml:space="preserve">
experimental::precision::u4 -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> (8 elements in 1 storage element) 
experimental::precision::s4 -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (8 elements in 1 storage element) 
experimental::precision::b1 -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> (32 elements in 1 storage element) 
T -&gt; T  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//all other types</span></pre><p class="p">The allowed layouts for sub-byte fragments is always <samp class="ph codeph">row_major</samp> for <samp class="ph codeph">matrix_a</samp> and <samp class="ph codeph">col_major</samp> for <samp class="ph codeph">matrix_b</samp>.
                              </p>
                              <p class="p">For sub-byte operations the value of <samp class="ph codeph">ldm</samp> in <samp class="ph codeph">load_matrix_sync</samp> should be a multiple of 32 for element type
                                 <samp class="ph codeph">experimental::precision::u4</samp> and <samp class="ph codeph">experimental::precision::s4</samp> or a multiple of 128 for element type
                                 <samp class="ph codeph">experimental::precision::b1</samp> (i.e., multiple of 16 bytes in both cases).
                              </p>
                              <dl class="dl">
                                 <dt class="dt dlterm"><samp class="ph codeph">bmma_sync</samp></dt>
                                 <dd class="dd">Waits until all warp lanes have executed bmma_sync, and then performs the warp-synchronous bit matrix multiply-accumulate
                                    operation <samp class="ph codeph">D = (A op B) + C</samp>,
                                    where <samp class="ph codeph">op</samp> consists of a logical operation <samp class="ph codeph">bmmaBitOp</samp> followed by the accumulation defined by <samp class="ph codeph">bmmaAccumulateOp</samp>.
                                    The available operations are:
                                    
                                    <p class="p"><samp class="ph codeph">bmmaBitOpXOR</samp>, a 128-bit XOR of a row in <samp class="ph codeph">matrix_a</samp> with the 128-bit column of <samp class="ph codeph">matrix_b</samp></p>
                                    <p class="p"><samp class="ph codeph">bmmaBitOpAND</samp>, a 128-bit AND of a row in <samp class="ph codeph">matrix_a</samp> with the 128-bit column of <samp class="ph codeph">matrix_b</samp>, available on devices with compute capability 8.0 and higher.
                                       
                                    </p>
                                    <p class="p">The accumulate op is always <samp class="ph codeph">bmmaAccumulateOpPOPC</samp> which counts the number of set bits.
                                    </p>
                                 </dd>
                              </dl>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="wmma-restrictions"><a name="wmma-restrictions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-restrictions" name="wmma-restrictions" shape="rect">B.24.5.&nbsp;Restrictions</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The special format required by tensor cores may be different for each major and minor device architecture.
                                 This is further complicated by threads holding only a fragment (opaque architecture-specific ABI data structure) of the overall
                                 matrix,
                                 with the developer not allowed to make assumptions on how the individual parameters are mapped to the registers participating
                                 in the matrix multiply-accumulate.
                              </p>
                              <p class="p">Since fragments are architecture-specific, it is unsafe to pass them from function A to function B if the functions have been
                                 compiled
                                 for different link-compatible architectures and linked together into the same device executable.
                                 In this case, the size and layout of the fragment will be specific to one architecture and using WMMA APIs in the other will
                                 lead to incorrect results or potentially, corruption.
                              </p>
                              <p class="p">An example of two link-compatible architectures, where the layout of the fragment differs, is sm_70 and sm_75.</p>
                              <div class="p"><pre xml:space="preserve">
fragA.cu: <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo() { wmma::fragment&lt;...&gt; mat_a; bar(&amp;mat_a); }
fragB.cu: <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(wmma::fragment&lt;...&gt; *mat_a) { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// operate on mat_a }              </span></pre></div>
                              <div class="p"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// sm_70 fragment layout</span>
$&gt; nvcc -dc -arch=compute_70 -code=sm_70 fragA.cu -o fragA.o
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// sm_75 fragment layout</span>
$&gt; nvcc -dc -arch=compute_75 -code=sm_75 fragB.cu -o fragB.o
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Linking the two together</span>
$&gt; nvcc -dlink -arch=sm_75 fragA.o fragB.o -o frag.o              </pre></div>
                              <p class="p">This undefined behavior might also be undetectable at compilation time and by tools at runtime, so extra care is needed to
                                 make sure the layout of the fragments is consistent. This linking hazard is most likely to appear when linking with a legacy
                                 library that is both built for a different link-compatible architecture and expecting to be passed a WMMA fragment.
                              </p>
                              <p class="p">Note that in the case of weak linkages (for example, a CUDA C++ inline function), the linker may choose any available function
                                 definition which may result in implicit passes between compilation units.
                              </p>
                              <p class="p">To avoid these sorts of problems, the matrix should always be stored out to memory for transit through external interfaces
                                 (e.g. <samp class="ph codeph">wmma::store_matrix_sync(dst, );</samp>) and then it can be safely passed to <samp class="ph codeph">bar()</samp> as a pointer type [e.g. <samp class="ph codeph">float *dst</samp>].
                              </p>
                              <p class="p">Note that since sm_70 can run on sm_75, the above example sm_75 code can be changed to sm_70 and correctly work on sm_75.
                                 However, it is recommended to have sm_75 native code in your application when linking with other sm_75 separately compiled
                                 binaries.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="wmma-type-sizes"><a name="wmma-type-sizes" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-type-sizes" name="wmma-type-sizes" shape="rect">B.24.6.&nbsp;Element Types &amp; Matrix Sizes</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Tensor Cores support a variety of element types and matrix sizes. The following table presents the various combinations
                                 of <samp class="ph codeph">matrix_a</samp>, <samp class="ph codeph">matrix_b</samp> and <samp class="ph codeph">accumulator</samp> matrix supported:
                              </p>
                              <div class="tablenoborder"><a name="wmma-type-sizes__wmma-type-table" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="wmma-type-sizes__wmma-type-table" class="table" frame="border" border="1" rules="all">
                                    <thead class="thead" align="left">
                                       <tr class="row" valign="middle">
                                          <th class="entry" align="left" valign="middle" width="25%" id="d225e15653" rowspan="1" colspan="1">Matrix A</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15656" rowspan="1" colspan="1">Matrix B</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15659" rowspan="1" colspan="1">Accumulator</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15662" rowspan="1" colspan="1">Matrix Size (m-n-k)</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">16x16x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">32x8x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">8x32x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">16x16x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">32x8x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">__half</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">8x32x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">unsigned char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">unsigned char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">16x16x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">unsigned char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">unsigned char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">32x8x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">unsigned char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">unsigned char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">8x32x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">signed char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">signed char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">16x16x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">signed char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">signed char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">32x8x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15653" rowspan="1" colspan="1">signed char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15656" rowspan="1" colspan="1">signed char</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15659" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15662" rowspan="1" colspan="1">8x32x16</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                              <p class="p">Alternate Floating Point support:</p>
                              <div class="tablenoborder"><a name="wmma-type-sizes__wmma-type-table-alternatefp" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="wmma-type-sizes__wmma-type-table-alternatefp" class="table" frame="border" border="1" rules="all">
                                    <thead class="thead" align="left">
                                       <tr class="row" valign="middle">
                                          <th class="entry" align="left" valign="middle" width="25%" id="d225e15874" rowspan="1" colspan="1">Matrix A</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15877" rowspan="1" colspan="1">Matrix B</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15880" rowspan="1" colspan="1">Accumulator</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15883" rowspan="1" colspan="1">Matrix Size (m-n-k)</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15874" rowspan="1" colspan="1">__nv_bfloat16</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15877" rowspan="1" colspan="1">__nv_bfloat16</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15880" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15883" rowspan="1" colspan="1">16x16x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15874" rowspan="1" colspan="1">__nv_bfloat16</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15877" rowspan="1" colspan="1">__nv_bfloat16</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15880" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15883" rowspan="1" colspan="1">32x8x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15874" rowspan="1" colspan="1">__nv_bfloat16</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15877" rowspan="1" colspan="1">__nv_bfloat16</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15880" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15883" rowspan="1" colspan="1">8x32x16</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15874" rowspan="1" colspan="1">precision::tf32</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15877" rowspan="1" colspan="1">precision::tf32</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15880" rowspan="1" colspan="1">float</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15883" rowspan="1" colspan="1">16x16x8</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                              <p class="p">Double Precision Support:</p>
                              <div class="tablenoborder"><a name="wmma-type-sizes__wmma-type-table-double" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="wmma-type-sizes__wmma-type-table-double" class="table" frame="border" border="1" rules="all">
                                    <thead class="thead" align="left">
                                       <tr class="row" valign="middle">
                                          <th class="entry" align="left" valign="middle" width="25%" id="d225e15974" rowspan="1" colspan="1">Matrix A</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15977" rowspan="1" colspan="1">Matrix B</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15980" rowspan="1" colspan="1">Accumulator</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e15983" rowspan="1" colspan="1">Matrix Size (m-n-k)</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e15974" rowspan="1" colspan="1">double</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15977" rowspan="1" colspan="1">double</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15980" rowspan="1" colspan="1">double</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e15983" rowspan="1" colspan="1">8x8x4</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                              <p class="p">Experimental support for sub-byte operations:</p>
                              <div class="tablenoborder"><a name="wmma-type-sizes__wmma-type-table-subbyte" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="wmma-type-sizes__wmma-type-table-subbyte" class="table" frame="border" border="1" rules="all">
                                    <thead class="thead" align="left">
                                       <tr class="row" valign="middle">
                                          <th class="entry" align="left" valign="middle" width="25%" id="d225e16028" rowspan="1" colspan="1">Matrix A</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e16031" rowspan="1" colspan="1">Matrix B</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e16034" rowspan="1" colspan="1">Accumulator</th>
                                          <th class="entry" align="center" valign="middle" width="25%" id="d225e16037" rowspan="1" colspan="1">Matrix Size (m-n-k)</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e16028" rowspan="1" colspan="1">precision::u4</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16031" rowspan="1" colspan="1">precision::u4</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16034" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16037" rowspan="1" colspan="1">8x8x32</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e16028" rowspan="1" colspan="1">precision::s4</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16031" rowspan="1" colspan="1">precision::s4</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16034" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16037" rowspan="1" colspan="1">8x8x32</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="25%" headers="d225e16028" rowspan="1" colspan="1">precision::b1</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16031" rowspan="1" colspan="1">precision::b1</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16034" rowspan="1" colspan="1">int</td>
                                          <td class="entry" align="center" valign="middle" width="25%" headers="d225e16037" rowspan="1" colspan="1">8x8x128</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="wmma-example"><a name="wmma-example" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#wmma-example" name="wmma-example" shape="rect">B.24.7.&nbsp;Example</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The following code implements a 16x16x16 matrix multiplication in a single warp.</p><pre xml:space="preserve">
#include &lt;mma.h&gt;
using namespace nvcuda;
      
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> wmma_ker(half *a, half *b, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> *c) {
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Declare the fragments</span>
   wmma::fragment&lt;wmma::matrix_a, 16, 16, 16, half, wmma::col_major&gt; a_frag;
   wmma::fragment&lt;wmma::matrix_b, 16, 16, 16, half, wmma::row_major&gt; b_frag;
   wmma::fragment&lt;wmma::accumulator, 16, 16, 16, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt; c_frag;

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize the output to zero</span>
   wmma::fill_fragment(c_frag, 0.0f);

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Load the inputs</span>
   wmma::load_matrix_sync(a_frag, a, 16);
   wmma::load_matrix_sync(b_frag, b, 16);

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Perform the matrix multiplication</span>
   wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Store the output</span>
   wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);
}      </pre></div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="aw-barrier"><a name="aw-barrier" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#aw-barrier" name="aw-barrier" shape="rect">B.25.&nbsp;Asynchronous Barrier</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">The NVIDIA C++ standard library introduces a GPU implementation of <a class="xref" href="https://nvidia.github.io/libcudacxx/extended_api/synchronization_primitives/barrier.html" target="_blank" shape="rect">std::barrier</a>.
                              Along with the implementation of <samp class="ph codeph">std::barrier</samp> the library provides extensions that allow users to specify the scope of barrier objects.
                              The barrier API scopes are documented under <a class="xref" href="https://nvidia.github.io/libcudacxx/extended_api/thread_scopes.html" target="_blank" shape="rect">Thread Scopes</a>.
                              Devices of compute capability 8.0 or higher provide hardware acceleration for barrier operations
                              and integration of these barriers with the <a class="xref" href="index.html#memcpy_async" shape="rect">memcpy_async</a> feature.
                              On devices with compute capability below 8.0 but starting 7.0, these barriers are available without hardware acceleration.
                           </p>
                           <p class="p"><samp class="ph codeph">nvcuda::experimental::awbarrier</samp> is deprecated in favor of <samp class="ph codeph">cuda::barrier</samp>.
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="split_arrive_then_wait"><a name="split_arrive_then_wait" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#split_arrive_then_wait" name="split_arrive_then_wait" shape="rect">B.25.1.&nbsp;Simple Synchronization Pattern</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Without the arrive/wait barrier, synchronization is achieved using <samp class="ph codeph">__syncthreads()</samp> (to synchronize all threads in a block) 
                                 or <samp class="ph codeph">group.sync()</samp> when using <a class="xref" href="index.html#cooperative-groups" shape="rect">Cooperative Groups</a>.
                              </p><pre xml:space="preserve">
#include &lt;cooperative_groups.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> simple_sync(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> iteration_count) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; iteration_count; ++i) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* code before arrive */</span>
        block.sync(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* wait for all threads to arrive here */</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* code after wait */</span>
    }
}</pre><p class="p">Threads are blocked at the synchronization point (<samp class="ph codeph">block.sync()</samp>) until all threads have reached the synchronization point.
                                 In addition, memory updates that happened before the synchronization point are guaranteed to be visible to all threads 
                                 in the block after the synchronization point, i.e., equivalent to <samp class="ph codeph">__threadfence_block()</samp> as well as the <samp class="ph codeph">sync</samp>.
                              </p>
                              <p class="p">This pattern has three stages:</p>
                              <ul class="ul">
                                 <li class="li">Code <strong class="ph b">before</strong> sync performs memory updates that will be read <strong class="ph b">after</strong> the sync.
                                 </li>
                                 <li class="li">Synchronization point</li>
                                 <li class="li">Code <strong class="ph b">after</strong> sync point with visibility of memory updates that happened <strong class="ph b">before</strong> sync point.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="temporal_split_5_stage_sync"><a name="temporal_split_5_stage_sync" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#temporal_split_5_stage_sync" name="temporal_split_5_stage_sync" shape="rect">B.25.2.&nbsp;Temporal Splitting and Five Stages of Synchronization</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The temporally-split synchronization pattern with the <samp class="ph codeph">std::barrier</samp> is as follows.
                              </p><pre xml:space="preserve">
#include &lt;cuda/barrier&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups.h&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> compute(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* data, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> curr_iteration);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> split_arrive_wait(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> iteration_count, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> *data) {
    using barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span>  barrier bar;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (block.thread_rank() == 0) {
        init(&amp;bar, block.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize the barrier with expected arrival count</span>
    }
    block.sync();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> curr_iter = 0; curr_iter &lt; iteration_count; ++curr_iter) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* code before arrive */</span>
       barrier::arrival_token token = bar.arrive(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* this thread arrives. Arrival does not block a thread */</span>
       compute(data, curr_iter); 
       bar.wait(std::move(token)); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* wait for all threads participating in the barrier to complete bar.arrive()*/</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* code after wait */</span>
    }
}         </pre><p class="p">In this pattern, the synchronization point (<samp class="ph codeph">block.sync()</samp>) is split into an arrive point 
                                 (<samp class="ph codeph">bar.arrive()</samp>) and a wait point (<samp class="ph codeph">bar.wait(std::move(token))</samp>). A thread begins participating 
                                 in a <samp class="ph codeph">cuda::barrier</samp> with its first call to <samp class="ph codeph">bar.arrive()</samp>. When a thread calls 
                                 <samp class="ph codeph">bar.wait(std::move(token))</samp> it will be blocked until participating threads have completed <samp class="ph codeph">bar.arrive()</samp> 
                                 the expected number of times as specified by the expected arrival count argument passed to <samp class="ph codeph">init()</samp>. 
                                 Memory updates that happen before participating threads' call to <samp class="ph codeph">bar.arrive()</samp> are guaranteed to be 
                                 visible to participating threads after their call to <samp class="ph codeph">bar.wait(std::move(token))</samp>. Note that the call to 
                                 <samp class="ph codeph">bar.arrive()</samp> does not block a thread, it can proceed with other work that does not depend upon 
                                 memory updates that happen before other participating threads' call to <samp class="ph codeph">bar.arrive()</samp>.
                              </p>
                              <p class="p">The <em class="ph i">arrive and then wait</em> pattern has five stages which may be iteratively repeated:
                              </p>
                              <ul class="ul">
                                 <li class="li">Code <strong class="ph b">before</strong> arrive performs memory updates that will be read <strong class="ph b">after</strong> the wait.
                                 </li>
                                 <li class="li">Arrive point with implicit memory fence (i.e., equivalent to <samp class="ph codeph">__threadfence_block()</samp>). 
                                 </li>
                                 <li class="li">Code <strong class="ph b">between</strong> arrive and wait.
                                 </li>
                                 <li class="li">Wait point.</li>
                                 <li class="li">Code <strong class="ph b">after</strong> the wait, with visibility of updates that were performed <strong class="ph b">before</strong> the arrive.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="bootstrap_init_expected_arrive_count"><a name="bootstrap_init_expected_arrive_count" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#bootstrap_init_expected_arrive_count" name="bootstrap_init_expected_arrive_count" shape="rect">B.25.3.&nbsp;Bootstrap Initialization, Expected Arrival Count, and Participation</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Initialization must happen before any thread begins participating in a <samp class="ph codeph">cuda::barrier</samp>.
                              </p><pre xml:space="preserve">
#include &lt;cuda/barrier&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups.h&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> init_barrier() { 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> cuda::barrier&lt;cuda::thread_scope_block&gt; bar;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (block.thread_rank() == 0) {
        init(&amp;bar, block.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Single thread initializes the total expected arrival count.</span>
    }
    block.sync();         
}</pre><p class="p">Before any thread can participate in <samp class="ph codeph">cuda::barrier</samp>, the barrier must be initialized using <samp class="ph codeph">init()</samp> 
                                 with an <strong class="ph b">expected arrival count</strong>, <samp class="ph codeph">block.size()</samp> in this example. 
                                 Initialization must happen before any thread calls <samp class="ph codeph">bar.arrive()</samp>. This poses a bootstrapping 
                                 challenge in that threads must synchronize before participating in the <samp class="ph codeph">cuda::barrier</samp>, but threads 
                                 are creating a <samp class="ph codeph">cuda::barrier</samp> in order to synchronize. In this example, threads that will participate 
                                 are part of a cooperative group and use <samp class="ph codeph">block.sync()</samp> to bootstrap initialization. 
                                 In this example a whole thread block is participating in initialization, hence <samp class="ph codeph">__syncthreads()</samp> could 
                                 also be used.
                              </p>
                              <p class="p">The second parameter of <samp class="ph codeph">init()</samp> is the <strong class="ph b">expected arrival count</strong>, i.e., the number of times <samp class="ph codeph">bar.arrive()</samp> 
                                 will be called by participating threads before a participating thread is unblocked from its call to 
                                 <samp class="ph codeph">bar.wait(std::move(token))</samp>. In the prior example the <samp class="ph codeph">cuda::barrier</samp> is initialized with the 
                                 number of threads in the thread block i.e., <samp class="ph codeph">cooperative_groups::this_thread_block().size()</samp>,
                                 and all threads within the thread block participate in the barrier.
                              </p>
                              <p class="p">A <samp class="ph codeph">cuda::barrier</samp> is flexible in specifying how threads participate (split arrive/wait) and which threads participate. 
                                 In contrast <samp class="ph codeph">this_thread_block.sync()</samp> from cooperative groups or <samp class="ph codeph">__syncthreads()</samp> is applicable to 
                                 whole-thread-block and <samp class="ph codeph">__syncwarp(mask)</samp> is a specified subset of a warp. If the intention of the user is to synchronize
                                 a full thread block or a full warp we recommend using <samp class="ph codeph">__syncthreads()</samp> and <samp class="ph codeph">__syncwarp(mask)</samp> respectively
                                 for performance reasons.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="countdown_complete_reset_phase"><a name="countdown_complete_reset_phase" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#countdown_complete_reset_phase" name="countdown_complete_reset_phase" shape="rect">B.25.4.&nbsp;A Barrier's Phase: Arrival, Countdown, Completion, and Reset</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">A <samp class="ph codeph">cuda::barrier</samp> counts down from the expected arrival count to zero as participating threads call <samp class="ph codeph">bar.arrive()</samp>. 
                                 When the countdown reaches zero, a <samp class="ph codeph">cuda::barrier</samp> is complete for the current phase. When the last call to 
                                 <samp class="ph codeph">bar.arrive()</samp> causes the countdown to reach zero, the countdown is automatically and atomically reset. The reset assigns the 
                                 countdown to the expected arrival count, and moves the <samp class="ph codeph">cuda::barrier</samp> to the next phase.
                              </p>
                              <p class="p">A <samp class="ph codeph">token</samp> object of class <samp class="ph codeph">cuda::barrier::arrival_token</samp>, as returned from <samp class="ph codeph">token=bar.arrive()</samp>, is associated 
                                 with the current phase of the barrier. A call to <samp class="ph codeph">bar.wait(std::move(token))</samp> blocks the calling thread while the 
                                 <samp class="ph codeph">cuda::barrier</samp> is in the current phase, i.e., while the phase associated with the token matches the phase of the 
                                 <samp class="ph codeph">cuda::barrier</samp>. If the phase is advanced (because the countdown reaches zero) before the call to <samp class="ph codeph">bar.wait(std::move(token))</samp> 
                                 then the thread does not block; if the phase is advanced while the thread is blocked in <samp class="ph codeph">bar.wait(std::move(token))</samp>, the thread is unblocked.
                              </p>
                              <p class="p"><strong class="ph b">It is essential to know when a reset could or could not occur, especially in non-trivial arrive/wait synchronization patterns.</strong></p>
                              <ul class="ul">
                                 <li class="li">A thread's calls to <samp class="ph codeph">token=bar.arrive()</samp> and <samp class="ph codeph">bar.wait(std::move(token))</samp> must be sequenced such that <samp class="ph codeph">token=bar.arrive()</samp> 
                                    occurs during the <samp class="ph codeph">cuda::barrier</samp>'s current phase, and <samp class="ph codeph">bar.wait(std::move(token))</samp> occurs during the same or next phase.
                                 </li>
                                 <li class="li">A thread's call to <samp class="ph codeph">bar.arrive()</samp> must occur when the barrier's counter is non-zero. After barrier initialization, if a thread's call to 
                                    <samp class="ph codeph">bar.arrive()</samp> causes the countdown to reach zero then a call to <samp class="ph codeph">bar.wait(std::move(token))</samp> must happen before the 
                                    barrier can be reused for a subsequent call to <samp class="ph codeph">bar.arrive()</samp>.
                                 </li>
                                 <li class="li"><samp class="ph codeph">bar.wait()</samp> must only be called using a <samp class="ph codeph">token</samp> object of the current phase or the immediately preceding phase.
                                    For any other values of the <samp class="ph codeph">token</samp> object, the behavior is undefined.
                                 </li>
                              </ul>
                              <p class="p">For simple arrive/wait synchronization patterns, compliance with these usage rules is straightforward.</p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="warp_specialization"><a name="warp_specialization" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#warp_specialization" name="warp_specialization" shape="rect">B.25.5.&nbsp;Spatial Partitioning (also known as Warp Specialization)</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">A thread block can be spatially partitioned such that warps are specialized to perform independent computations. 
                                 Spatial partitioning is used in a producer or consumer pattern, where one subset of threads produces data that 
                                 is concurrently consumed by the other (disjoint) subset of threads.
                              </p>
                              <p class="p">A producer/consumer spatial partitioning pattern requires two one sided synchronizations to manage a data buffer between the
                                 producer and consumer.
                              </p>
                              <div class="tablenoborder"><a name="warp_specialization__producer_consumer_awbarrier" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="warp_specialization__producer_consumer_awbarrier" class="table" frame="border" border="1" rules="all">
                                    <thead class="thead" align="left">
                                       <tr class="row" valign="middle">
                                          <th class="entry" align="left" valign="middle" width="50%" id="d225e16555" rowspan="1" colspan="1">Producer</th>
                                          <th class="entry" align="left" valign="middle" width="50%" id="d225e16558" rowspan="1" colspan="1">Consumer</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16555" rowspan="1" colspan="1">wait for buffer to be ready to be filled</td>
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16558" rowspan="1" colspan="1">signal buffer is ready to be filled</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16555" rowspan="1" colspan="1">produce data and fill the buffer</td>
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16558" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16555" rowspan="1" colspan="1">signal buffer is filled</td>
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16558" rowspan="1" colspan="1">wait for buffer to be filled</td>
                                       </tr>
                                       <tr class="row" valign="middle">
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16555" rowspan="1" colspan="1">&nbsp;</td>
                                          <td class="entry" align="left" valign="middle" width="50%" headers="d225e16558" rowspan="1" colspan="1">consume data in filled buffer</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                              <p class="p">Producer threads wait for consumer threads to signal that the buffer is ready to be filled; however, consumer threads do not
                                 wait 
                                 for this signal. Consumer threads wait for producer threads to signal that the buffer is filled; however, producer threads
                                 do not 
                                 wait for this signal. For full producer/consumer concurrency this pattern has (at least) double buffering where each buffer
                                 requires two 
                                 <samp class="ph codeph">cuda::barrier</samp>s.
                              </p><pre xml:space="preserve">
#include &lt;cuda/barrier&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups.h&gt;</span>

using barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> producer(barrier ready[], barrier filled[], <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* buffer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* in, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> buffer_len)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; (N/buffer_len); ++i) {
        ready[i%2].arrive_and_wait(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* wait for buffer_(i%2) to be ready to be filled */</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* produce, i.e., fill in, buffer_(i%2)  */</span>
        barrier::arrival_token token = filled[i%2].arrive(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* buffer_(i%2) is filled */</span>
    }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> consumer(barrier ready[], barrier filled[], <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* buffer, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> buffer_len)
{
    barrier::arrival_token token1 = ready[0].arrive(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* buffer_0 is ready for initial fill */</span>
    barrier::arrival_token token2 = ready[1].arrive(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* buffer_1 is ready for initial fill */</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; (N/buffer_len); ++i) {
        filled[i%2].arrive_and_wait(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* wait for buffer_(i%2) to be filled */</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* consume buffer_(i%2) */</span>
        barrier::arrival_token token = ready[i%2].arrive(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* buffer_(i%2) is ready to be re-filled */</span>
    }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//N is the total number of float elements in arrays in and out</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> producer_consumer_pattern(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> buffer_len, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* in, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* out) {

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Shared memory buffer declared below is of size 2 * buffer_len</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// so that we can alternatively work between two buffers. </span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// buffer_0 = buffer and buffer_1 = buffer + buffer_len</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> buffer[];
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// bar[0] and bar[1] track if buffers buffer_0 and buffer_1 are ready to be filled, </span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// while bar[2] and bar[3] track if buffers buffer_0 and buffer_1 are filled-in respectively</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> barrier bar[4];
   

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (block.thread_rank() &lt; 4)
        init(bar + block.thread_rank(), block.size());
    block.sync();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (block.thread_rank() &lt; warpSize)
        producer(bar, bar+2, buffer, in, N, buffer_len);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">else</span>
        consumer(bar, bar+2, buffer, out, N, buffer_len);
}</pre><p class="p">In this example the first warp is specialized as the producer and the remaining warps are specialized as the consumer. All
                                 producer 
                                 and consumer threads participate (call <samp class="ph codeph">bar.arrive()</samp> or <samp class="ph codeph">bar.arrive_and_wait()</samp>) in each of the four 
                                 <samp class="ph codeph">cuda::barrier</samp>s so the expected arrival counts are equal to <samp class="ph codeph">block.size()</samp>.
                              </p>
                              <p class="p">A producer thread waits for the consumer threads to signal that the shared memory buffer can be filled. In order to wait for
                                 a <samp class="ph codeph">cuda::barrier</samp> 
                                 a producer thread must first arrive on that <samp class="ph codeph">ready[i%2].arrive()</samp> to get a token and then <samp class="ph codeph">ready[i%2].wait(token)</samp>
                                 with that token. For simplicity <samp class="ph codeph">ready[i%2].arrive_and_wait()</samp> combines these operations.
                              </p><pre xml:space="preserve">
bar.arrive_and_wait();
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* is equivalent to */</span>
bar.wait(bar.arrive());</pre><p class="p">Producer threads compute and fill the ready buffer, they then signal that the buffer is filled by arriving on the filled barrier,
                                 
                                 <samp class="ph codeph">filled[i%2].arrive()</samp>. A producer thread does not wait at this point, instead it waits until the 
                                 next iteration's buffer (double buffering) is ready to be filled.
                              </p>
                              <p class="p">A consumer thread begins by signaling that both buffers are ready to be filled. A consumer thread does not wait at this point,
                                 
                                 instead it waits for this iteration's buffer to be filled, <samp class="ph codeph">filled[i%2].arrive_and_wait()</samp>. After the
                                 consumer threads consume the buffer they signal that the buffer is ready to be filled again, <samp class="ph codeph">ready[i%2].arrive()</samp>, 
                                 and then wait for the next iteration's buffer to be filled.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="early_exit"><a name="early_exit" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#early_exit" name="early_exit" shape="rect">B.25.6.&nbsp;Early Exit (Dropping out of Participation)</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">When a thread that is participating in a sequence of synchronizations must exit early from that sequence, that thread 
                                 must explicitly drop out of participation before exiting. The remaining participating threads can proceed normally 
                                 with subsequent <samp class="ph codeph">cuda::barrier</samp> arrive and wait operations.
                              </p><pre xml:space="preserve">
#include &lt;cuda/barrier&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups.h&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> bool condition_check();

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> early_exit_kernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N) {
    using barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> barrier bar;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (block.thread_rank() == 0)
        init(&amp;bar , block.size());
    block.sync();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; N; ++i) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (condition_check()) {
          bar.arrive_and_drop();
          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>;
        }
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* other threads can proceed normally */</span>
        barrier::arrival_token token = bar.arrive();
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* code between arrive and wait */</span>
        bar.wait(std::move(token)); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* wait for all threads to arrive */</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* code after wait */</span>
    }
}          </pre><p class="p">This operation arrives on the <samp class="ph codeph">cuda::barrier</samp> to fulfill the participating thread's obligation to arrive in the <strong class="ph b">current</strong> phase, 
                                 and then decrements the expected arrival count for the <strong class="ph b">next</strong> phase so that this thread is no longer expected to arrive on the barrier.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" id="memory_barrier_primitives_interface"><a name="memory_barrier_primitives_interface" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#memory_barrier_primitives_interface" name="memory_barrier_primitives_interface" shape="rect">B.25.7.&nbsp;Memory Barrier Primitives Interface</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Memory barrier primitives are C-like interfaces to <samp class="ph codeph">cuda::barrier</samp> functionality. 
                                 These primitives are available through including the <samp class="ph codeph">&lt;cuda_awbarrier_primitives.h&gt;</samp> header.
                              </p>
                           </div>
                        </div>
                        <div class="topic reference nested3" id="memory_barrier_primitives_datatypes"><a name="memory_barrier_primitives_datatypes" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#memory_barrier_primitives_datatypes" name="memory_barrier_primitives_datatypes" shape="rect">B.25.7.1.&nbsp;Data Types</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* implementation defined */</span> __mbarrier_t;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* implementation defined */</span> __mbarrier_token_t;     </pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" id="memory_barrier_primitives_api"><a name="memory_barrier_primitives_api" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#memory_barrier_primitives_api" name="memory_barrier_primitives_api" shape="rect">B.25.7.2.&nbsp;Memory Barrier Primitives API</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
uint32_t __mbarrier_maximum_expected();
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __mbarrier_init(__mbarrier_t* bar, uint32_t expected_count); </pre><ul class="ul">
                                    <li class="li"><samp class="ph codeph">bar</samp> must be a pointer to <samp class="ph codeph">__shared__</samp> memory.
                                    </li>
                                    <li class="li"><samp class="ph codeph">expected_count &lt;= __mbarrier_maximum_expected()</samp></li>
                                    <li class="li">Initialize <samp class="ph codeph">*bar</samp> expected arrival count for the current and next phase to <samp class="ph codeph">expected_count</samp>.
                                    </li>
                                 </ul><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __mbarrier_inval(__mbarrier_t* bar); </pre><ul class="ul">
                                    <li class="li"><samp class="ph codeph">bar</samp> must be a pointer to the mbarrier object residing in shared memory.
                                    </li>
                                    <li class="li">Invalidation of <samp class="ph codeph">*bar</samp> is required before the corresponding shared memory can be repurposed.
                                    </li>
                                 </ul><pre xml:space="preserve">
__mbarrier_token_t __mbarrier_arrive(__mbarrier_t* bar);    </pre><ul class="ul">
                                    <li class="li">Initialization of <samp class="ph codeph">*bar</samp> must happen before this call.
                                    </li>
                                    <li class="li">Pending count must not be zero.</li>
                                    <li class="li">Atomically decrement the pending count for the current phase of the barrier.</li>
                                    <li class="li">Return an arrival token associated with the barrier state immediately prior to the decrement.</li>
                                 </ul><pre xml:space="preserve">
__mbarrier_token_t __mbarrier_arrive_and_drop(__mbarrier_t* bar);   </pre><ul class="ul">
                                    <li class="li">Initialization of <samp class="ph codeph">*bar</samp> must happen before this call.
                                    </li>
                                    <li class="li">Pending count must not be zero.</li>
                                    <li class="li">Atomically decrement the pending count for the current phase and expected count for the next phase of the barrier.</li>
                                    <li class="li">Return an arrival token associated with the barrier state immediately prior to the decrement.</li>
                                 </ul><pre xml:space="preserve">
bool __mbarrier_test_wait(__mbarrier_t* bar, __mbarrier_token_t token);  </pre><ul class="ul">
                                    <li class="li"><samp class="ph codeph">token</samp> must be associated with the immediately preceding phase or current phase of <samp class="ph codeph">*this</samp>.
                                    </li>
                                    <li class="li">Returns <samp class="ph codeph">true</samp> if <samp class="ph codeph">token</samp> is associated with the immediately preceding phase of <samp class="ph codeph">*bar</samp>, 
                                       otherwise returns <samp class="ph codeph">false</samp>.
                                    </li>
                                 </ul><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//Note: This API has been deprecated in CUDA 11.1</span>
uint32_t __mbarrier_pending_count(__mbarrier_token_t token);      </pre></div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="memcpy_async"><a name="memcpy_async" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#memcpy_async" name="memcpy_async" shape="rect">B.26.&nbsp;Asynchronous Data Copies</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">CUDA 11 introduces <samp class="ph codeph">memcpy_async</samp> to allow device code
                              to explicitly manage the asynchronous copying of data. The
                              <samp class="ph codeph">memcpy_async</samp> feature enables CUDA kernels to overlap
                              computation with data movement.
                              
                           </p>
                           <p class="p">The <samp class="ph codeph">memcpy_async</samp> APIs that use <a class="xref" href="index.html#aw-barrier" shape="rect"><samp class="ph codeph">cuda::barrier</samp></a>, see
                              Sections <a class="xref" href="index.html#with-memcpy_async" shape="rect">With memcpy_async</a> and <a class="xref" href="index.html#arrive-primitive" shape="rect">Arrive On Barrier Primitive</a>, require compute capability 7.0
                              or higher. On devices with compute capability 8.0 or higher,
                              <samp class="ph codeph">memcpy_async</samp> operations from global to shared memory can
                              benefit from hardware acceleration.
                              
                           </p>
                           <p class="p"><samp class="ph codeph">nvcuda::experimental::pipeline</samp> is deprecated in favor of <samp class="ph codeph">cuda::pipeline</samp>.
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="memcpy-async-api"><a name="memcpy-async-api" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#memcpy-async-api" name="memcpy-async-api" shape="rect">B.26.1.&nbsp;<samp class="ph codeph">memcpy_async</samp> API</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The <samp class="ph codeph">memcpy_async</samp> APIs are provided in the
                                 <samp class="ph codeph">cuda/barrier</samp>, <samp class="ph codeph">cuda/pipeline</samp>, and
                                 <samp class="ph codeph">cooperative_groups/memcpy_async.h</samp> header files.
                              </p>
                              <p class="p">The <samp class="ph codeph">cuda::memcpy_async</samp> APIs work with
                                 <samp class="ph codeph">cuda::barrier</samp> and <samp class="ph codeph">cuda::pipeline</samp>
                                 synchronization primitives, while the
                                 <samp class="ph codeph">cooperative_groups::memcpy_async</samp> synchronizes using
                                 <samp class="ph codeph">coopertive_groups::wait</samp>.
                              </p>
                              <p class="p">These APIs have very similar semantics: copy objects from
                                 <samp class="ph codeph">src</samp> to <samp class="ph codeph">dst</samp> as-if performed by another
                                 thread which, on completion of the copy, synchronizes with the current
                                 thread through <samp class="ph codeph">cuda::pipeline</samp>, <samp class="ph codeph">cuda::barrier</samp>, or
                                 <samp class="ph codeph">cooperative_groups::wait</samp>.
                              </p>
                              <p class="p">The complete API documentation of the
                                 <samp class="ph codeph">cuda::memcpy_async</samp> overloads for <samp class="ph codeph">cuda::barrier</samp> and
                                 <samp class="ph codeph">cuda::pipeline</samp> is provided in the <a class="xref" href="https://nvidia.github.io/libcudacxx" target="_blank" shape="rect">libcudacxx API</a> documentation along with some
                                 examples.
                              </p>
                              <p class="p">The API documentation of <a class="xref" href="index.html#collectives-cg-memcpy-async" shape="rect"><samp class="ph codeph">cooperative_groups::memcpy_async</samp></a>
                                 is provided in the <a class="xref" href="index.html#cooperative-groups" shape="rect">cooperative
                                    groups</a> Section of the documentation.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="copy-and-compute-pattern"><a name="copy-and-compute-pattern" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#copy-and-compute-pattern" name="copy-and-compute-pattern" shape="rect">B.26.2.&nbsp;Copy and Compute Pattern - Staging Data Through Shared Memory</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <div class="p"> CUDA applications often employ a <em class="ph i">copy and compute</em> pattern
                                 that:
                                 
                                 <ul class="ul">
                                    <li class="li">fetches data from global memory,</li>
                                    <li class="li">stores data to shared memory, and</li>
                                    <li class="li">performs computations on shared memory data, and potentially writes results back to global memory.</li>
                                 </ul>
                                 
                                 The following sections illustrate how this pattern can be expressed
                                 without and with the <samp class="ph codeph">memcpy_async</samp> feature:
                                 
                                 <ul class="ul">
                                    <li class="li"> The section <a class="xref" href="index.html#without-memcpy_async" shape="rect">Without
                                          <samp class="ph codeph">memcpy_async</samp></a> introduces an example that does
                                       not overlap computation with data movement and uses an intermediate
                                       register to copy data.
                                    </li>
                                    <li class="li">The section <a class="xref" href="index.html#with-memcpy_async" shape="rect">With
                                          <samp class="ph codeph">memcpy_async</samp></a> improves the previous example by
                                       introducing the <a class="xref" href="index.html#collectives-cg-memcpy-async" shape="rect"><samp class="ph codeph">cooperative_groups::memcpy_async</samp></a> and
                                       the <samp class="ph codeph">cuda::memcpy_async</samp> APIs to directly copy data
                                       from global to shared memory without using intermediate registers.
                                    </li>
                                    <li class="li">The section <a class="xref" href="index.html#with-memcpy_async-pipeline-pattern" shape="rect">Multi-Stage
                                          Asynchronous Copy Pattern</a> improves the previous example by
                                       introducing <samp class="ph codeph">cuda::pipeline</samp> to build a two-stage
                                       pipeline that overlaps computation with asynchronous data movement.
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="without-memcpy_async"><a name="without-memcpy_async" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#without-memcpy_async" name="without-memcpy_async" shape="rect">B.26.2.1.&nbsp;Without <samp class="ph codeph">memcpy_async</samp></a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">Without <samp class="ph codeph">memcpy_async</samp>, the <em class="ph i">copy</em> phase of the <em class="ph i">copy and
                                       	     compute</em> pattern is expressed as <samp class="ph codeph">shared[local_idx] =
                                       	     global[global_idx]</samp>. This global to shared memory copy is expanded
                                    	     to a read from global memory into a register, followed by a write to shared
                                    	     memory from the register.
                                 </p>
                                 <p class="p">When this pattern occurs within an iterative algorithm, each thread block
                                    	     needs to synchronize after the <samp class="ph codeph">shared[local_idx] =
                                       	     global[global_idx]</samp> assignment, to ensure all writes to shared
                                    	     memory have completed before the compute phase can begin. The thread block
                                    	     also needs to synchronize again after the compute phase, to prevent
                                    	     overwriting shared memory before all threads have completed their
                                    	     computations. This pattern is illustrated in the following code snippet.
                                 </p><pre xml:space="preserve">
#include &lt;cooperative_groups.h&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> compute(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* shared_in) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Computes using all values of current batch from shared memory.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Stores this thread's result back to global memory.</span>
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> without_memcpy_async(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* global_in, size_t size, size_t batch_sz) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> grid = cooperative_groups::this_grid();
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
  assert(size == batch_sz * grid.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Exposition: input size fits batch_sz * grid_size</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared[]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// block.size() * sizeof(int) bytes</span>

  size_t local_idx = block.thread_rank();

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t batch = 0; batch &lt; batch_sz; ++batch) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Compute the index of the current batch for this block in global memory:</span>
    size_t block_batch_idx = block.group_index().x * block.size() + grid.size() * batch;
    size_t global_idx = block_batch_idx + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    shared[local_idx] = global_in[global_idx];

    block.sync(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Wait for all copies to complete</span>

    compute(global_out + block_batch_idx, shared); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Compute and write result to global memory</span>

    block.sync(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Wait for compute using shared memory to finish</span>
  }
}
</pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="with-memcpy_async"><a name="with-memcpy_async" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#with-memcpy_async" name="with-memcpy_async" shape="rect">B.26.2.2.&nbsp;With <samp class="ph codeph">memcpy_async</samp></a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <div class="p"> With <samp class="ph codeph">memcpy_async</samp>, the assignment of shared memory from global memory
                                    <pre xml:space="preserve">shared[local_idx] = global_in[global_idx];</pre>
                                    is replaced with an asynchronous copy operation from <a class="xref" href="index.html#cooperative-groups" shape="rect">cooperative groups</a><pre xml:space="preserve">
cooperative_groups::memcpy_async(group, shared, global_in + batch_idx, sizeof(int) * block.size());
</pre></div>
                                 <p class="p"> The <a class="xref" href="index.html#collectives-cg-memcpy-async" shape="rect"><samp class="ph codeph">cooperative_groups::memcpy_async</samp></a>
                                    	  API copies <samp class="ph codeph">sizeof(int) * block.size()</samp> bytes from global
                                    	  memory starting at <samp class="ph codeph">global_in + batch_idx</samp> to the
                                    	  <samp class="ph codeph">shared</samp> data. This operation happens as-if performed by
                                    	  another thread, which synchronizes with the current thread's call to <a class="xref" href="index.html#collectives-cg-wait" shape="rect"><samp class="ph codeph">cooperative_groups::wait</samp></a>
                                    	  after the copy has completed. Until the copy operation completes, modifying
                                    	  the global data or reading or writing the shared data introduces a data
                                    	  race.
                                 </p>
                                 <p class="p">On devices with compute capability 8.0 or higher,
                                    <samp class="ph codeph">memcpy_async</samp> transfers from global to shared memory can
                                    benefit from hardware acceleration, which avoids transfering the data
                                    through an intermediate register.
                                 </p><pre xml:space="preserve">
#include &lt;cooperative_groups.h&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups/memcpy_async.h&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> compute(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* shared_in);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> with_memcpy_async(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* global_in, size_t size, size_t batch_sz) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> grid = cooperative_groups::this_grid();
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
  assert(size == batch_sz * grid.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Exposition: input size fits batch_sz * grid_size</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared[]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// block.size() * sizeof(int) bytes</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t batch = 0; batch &lt; batch_sz; ++batch) {
    size_t block_batch_idx = block.group_index().x * block.size() + grid.size() * batch;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Whole thread-group cooperatively copies whole batch to shared memory:</span>
    cooperative_groups::memcpy_async(block, shared, global_in + block_batch_idx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * block.size());

    cooperative_groups::wait(block); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Joins all threads, waits for all copies to complete</span>

    compute(global_out + block_batch_idx, shared);

    block.sync();
  }
}}
</pre><p class="p">The <samp class="ph codeph">cuda::memcpy_async</samp> overload for <a class="xref" href="index.html#aw-barrier" shape="rect"><samp class="ph codeph">cuda::barrier</samp></a>
                                    enables synchronizing asynchronous data transfers using a
                                    <samp class="ph codeph">barrier</samp>. This overloads executes the copy operation as-if performed
                                    by another thread bound to the barrier by: incrementing the expected count of
                                    the current phase on creation, and decrementing it on completion of the copy
                                    operation, such that the phase of the <samp class="ph codeph">barrier</samp> will only
                                    advance when all threads participating in the barrier have arrived, and all
                                    <samp class="ph codeph">memcpy_async</samp> bound to the current phase of the barrier have
                                    completed. The following example uses a block-wide <samp class="ph codeph">barrier</samp>,
                                    where all block threads participate, and swaps the wait operation with a
                                    barrier <samp class="ph codeph">arrive_and_wait</samp>, while providing the same
                                    functionality as the previous example:
                                 </p><pre xml:space="preserve">
#include &lt;cooperative_groups.h&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cuda/barrier&gt;</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> compute(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* shared_in);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> with_barrier(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* global_in, size_t size, size_t batch_sz) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> grid = cooperative_groups::this_grid();
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
  assert(size == batch_sz * grid.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Assume input size fits batch_sz * grid_size</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared[]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// block.size() * sizeof(int) bytes</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create a synchronization object (C++20 barrier)</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> cuda::barrier&lt;cuda::thread_scope::thread_scope_block&gt; barrier;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (block.thread_rank() == 0) {
    init(&amp;barrier, block.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Friend function initializes barrier</span>
  }
  block.sync();

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t batch = 0; batch &lt; batch_sz; ++batch) {
    size_t block_batch_idx = block.group_index().x * block.size() + grid.size() * batch;
    cuda::memcpy_async(block, shared, global_in + block_batch_idx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * block.size(), barrier);

    barrier.arrive_and_wait(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Waits for all copies to complete</span>

    compute(global_out + block_batch_idx, shared);

    block.sync();
  }
}
</pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="with-memcpy_async-pipeline-pattern"><a name="with-memcpy_async-pipeline-pattern" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#with-memcpy_async-pipeline-pattern" name="with-memcpy_async-pipeline-pattern" shape="rect">B.26.2.3.&nbsp;Multi-Stage Asynchronous Copy Pattern</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">In the previous examples with <a class="xref" href="index.html#collectives-cg-wait" shape="rect"><samp class="ph codeph">cooperative_groups::wait</samp></a>
                                    and <a class="xref" href="index.html#aw-barrier" shape="rect"><samp class="ph codeph">cuda::barrier</samp></a>, the kernel threads immediately wait for
                                    the data transfer to shared memory to complete. This avoids data transfers
                                    from global memory into registers, but does not <em class="ph i">hide</em> the latency of
                                    the <samp class="ph codeph">memcpy_async</samp> operation by overlapping computation.
                                 </p>
                                 <div class="p">For that we use the CUDA <a class="xref" href="index.html#pipeline-interface" shape="rect"><em class="ph i">pipeline</em></a> feature in
                                    the following example. It provides a mechanism for managing a sequence of
                                    <samp class="ph codeph">memcpy_async</samp> batches, enabling CUDA kernels to overlap
                                    memory transfers with computation. The following example implements a
                                    two-stage pipeline that overlaps data-transfer with computation. It:
                                    
                                    <ul class="ul">
                                       <li class="li">Initializes the pipeline shared state (more below)</li>
                                       <li class="li">Kickstarts the pipeline by scheduling a <samp class="ph codeph">memcpy_async</samp> for the first batch.
                                       </li>
                                       <li class="li">Loops over all the batches: it schedules <samp class="ph codeph">memcpy_async</samp> for the next batch, blocks all threads on the completion of the <samp class="ph codeph">memcpy_async</samp> for the previous batch, and then overlaps the computation on the previous batch with the asynchronous copy of the memory
                                          for the next batch.
                                       </li>
                                       <li class="li">Finally, it drains the pipeline by performing the computation on the last batch.</li>
                                    </ul>
                                    
                                    Note that, for interoperability with <samp class="ph codeph">cuda::pipeline</samp>,
                                    <samp class="ph codeph">cuda::memcpy_async</samp> from the <samp class="ph codeph">cuda/pipeline</samp>
                                    header is used here.
                                    
                                 </div><pre xml:space="preserve">
#include &lt;cooperative_groups/memcpy_async.h&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cuda/pipeline&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> compute(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* shared_in);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> with_staging(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* global_in, size_t size, size_t batch_sz) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> grid = cooperative_groups::this_grid();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
    assert(size == batch_sz * grid.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Assume input size fits batch_sz * grid_size</span>

    constexpr size_t stages_count = 2; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Pipeline with two stages</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Two batches must fit in shared memory:</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared[];  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// stages_count * block.size() * sizeof(int) bytes</span>
    size_t shared_offset[stages_count] = { 0, block.size() }; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Offsets to each batch</span>

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate shared storage for a two-stage cuda::pipeline:</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> cuda::pipeline_shared_state&lt;
        cuda::thread_scope::thread_scope_block,
        stages_count
    &gt; shared_state;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> pipeline = cuda::make_pipeline(block, &amp;shared_state);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread processes `batch_sz` elements.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Compute offset of the batch `batch` of this thread block in global memory:</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block_batch = [&amp;](size_t batch) -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> {
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> block.group_index().x * block.size() + grid.size() * batch;
    };

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block:</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (batch_sz == 0) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>;
    pipeline.producer_acquire();
    cuda::memcpy_async(block, shared + shared_offset[0], global_in + block_batch(0), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * block.size(), pipeline);
    pipeline.producer_commit();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Pipelined copy/compute:</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t batch = 1; batch &lt; batch_sz; ++batch) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Stage indices for the compute and copy stages:</span>
        size_t compute_stage_idx = (batch - 1) % 2;
        size_t copy_stage_idx = batch % 2;

        size_t global_idx = block_batch(batch);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Collectively acquire the pipeline head stage from all producer threads:</span>
        pipeline.producer_acquire();

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Submit async copies to the pipeline's head stage to be</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// computed in the next loop iteration</span>
        cuda::memcpy_async(block, shared + shared_offset[copy_stage_idx], global_in + global_idx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * block.size(), pipeline);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Collectively commit (advance) the pipeline's head stage</span>
        pipeline.producer_commit();

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Collectively wait for the operations commited to the</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// previous `compute` stage to complete:</span>
        pipeline.consumer_wait();

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Computation overlapped with the memcpy_async of the "copy" stage:</span>
        compute(global_out + global_idx, shared + shared_offset[compute_stage_idx]);

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Collectively release the stage resources</span>
        pipeline.consumer_release();
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Compute the data fetch by the last iteration</span>
    pipeline.consumer_wait();
    compute(global_out + block_batch(batch_sz-1), shared + shared_offset[(batch_sz - 1) % 2]);
    pipeline.consumer_release();
}
</pre><div class="p">A <a class="xref" href="index.html#pipeline-interface" shape="rect"><em class="ph i">pipeline</em>
                                       object</a> is a double-ended queue with a <em class="ph i">head</em> and a <em class="ph i">tail</em>,
                                    and is used to process work in a first-in first-out (FIFO) order. Producer
                                    threads commit work to the pipeline's head, while consumer threads pull work
                                    from the pipeline's tail. In the example above, all threads are both
                                    producer and consumer threads. The threads first <em class="ph i">commit</em><samp class="ph codeph">memcpy_async</samp> operations to fetch the <em class="ph i">next</em> batch
                                    while they <em class="ph i">wait</em> on the <em class="ph i">previous</em> batch of
                                    <samp class="ph codeph">memcpy_async</samp> operations to complete.
                                    
                                    <ul class="ul">
                                       <li class="li">Committing work to a pipeline stage involves:
                                          
                                          <ul class="ul">
                                             <li class="li">Collectively <em class="ph i">acquiring</em> the pipeline <em class="ph i">head</em> from a set of producer
                                                threads using <samp class="ph codeph">pipeline.producer_acquire()</samp>.
                                             </li>
                                             <li class="li">Submitting <samp class="ph codeph">memcpy_async</samp> operations to the pipeline head.
                                             </li>
                                             <li class="li">Collectively <em class="ph i">commiting</em> (advancing) the pipeline head
                                                using <samp class="ph codeph">pipeline.producer_commit()</samp>.
                                             </li>
                                          </ul>
                                       </li>
                                       <li class="li">Using a previously commited stage involves:
                                          <ul class="ul">
                                             <li class="li">Collectively waiting for the stage to complete, e.g., using
                                                <samp class="ph codeph">pipeline.consumer_wait()</samp> to wait on the tail (oldest) stage.
                                             </li>
                                             <li class="li">Collectively <em class="ph i">releasing</em> the stage using
                                                <samp class="ph codeph">pipeline.consumer_release()</samp>.
                                             </li>
                                          </ul>
                                       </li>
                                    </ul><samp class="ph codeph">cuda::pipeline_shared_state&lt;scope, count&gt;</samp> encapsulates the finite resources that allow a pipeline to process up to <samp class="ph codeph">count</samp> concurrent stages. If all resources are in use,
                                    <samp class="ph codeph">pipeline.producer_acquire()</samp> blocks producer threads until the resources of the next pipeline stage are released by consumer threads.
                                    
                                 </div>
                                 <p class="p">This example can be written in a more concise manner by merging the prolog and epilog of the loop with the loop itself as
                                    follows:
                                 </p><pre xml:space="preserve">
template &lt;size_t stages_count = 2 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* Pipeline with stages_count stages */</span>&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> with_staging_unified(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* global_in, size_t size, size_t batch_sz) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> grid = cooperative_groups::this_grid();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
    assert(size == batch_sz * grid.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Assume input size fits batch_sz * grid_size</span>

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared[]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// stages_count * block.size() * sizeof(int) bytes</span>
    size_t shared_offset[stages_count];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> s = 0; s &lt; stages_count; ++s) shared_offset[s] = s * block.size();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> cuda::pipeline_shared_state&lt;
        cuda::thread_scope::thread_scope_block,
        stages_count
    &gt; shared_state;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> pipeline = cuda::make_pipeline(block, &amp;shared_state);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block_batch = [&amp;](size_t batch) -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> block.group_index().x * block.size() + grid.size() * batch;
    };

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compute_batch: next batch to process</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// fetch_batch:  next batch to fetch from global memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t compute_batch = 0, fetch_batch = 0; compute_batch &lt; batch_sz; ++compute_batch) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The outer loop iterates over the computation of the batches</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (; fetch_batch &lt; batch_sz &amp;&amp; fetch_batch &lt; (compute_batch + stages_count); ++fetch_batch) {
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This inner loop iterates over the memory transfers, making sure that the pipeline is always full</span>
            pipeline.producer_acquire();
            size_t shared_idx = fetch_batch % stages_count;
            size_t batch_idx = fetch_batch;
            size_t block_batch_idx = block_batch(batch_idx);
            cuda::memcpy_async(block, shared + shared_offset[shared_idx], global_in + block_batch_idx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * block.size(), pipeline);
            pipeline.producer_commit();
        }
        pipeline.consumer_wait();
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared_idx = compute_batch % stages_count;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> batch_idx = compute_batch;
        compute(global_out + block_batch(batch_idx), shared + shared_offset[shared_idx]);
        pipeline.consumer_release();
    }
}
</pre><p class="p">The <samp class="ph codeph">pipeline&lt;thread_scope_block&gt;</samp> primitive
                                    used above is very flexible, and supports two features that our examples
                                    above are not using: any arbitrary subset of threads in the block can
                                    participate in the <samp class="ph codeph">pipeline</samp>, and from the threads that
                                    participate, any subsets can be producers, consumers, or both. There are
                                    some optimizations that <samp class="ph codeph">pipeline</samp> performs, for example,
                                    when all threads are both producers and consumers, but in general, the cost
                                    of supporting all these features cannot be fully eliminated. For example,
                                    <samp class="ph codeph">pipeline</samp> stores and uses a set of barriers in shared memory
                                    for synchronization, which is not really necessary if all threads in the
                                    block participate in the pipeline.
                                 </p>
                                 <p class="p">For the particular case in which all threads in the block participate in
                                    the <samp class="ph codeph">pipeline</samp>, we can do better than
                                    <samp class="ph codeph">pipeline&lt;thread_scope_block&gt;</samp> by using a <samp class="ph codeph">pipeline&lt;thread_scope_thread&gt;</samp> combined with <samp class="ph codeph">__syncthreads()</samp>:
                                 </p><pre xml:space="preserve">
template&lt;size_t stages_count&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> with_staging_scope_thread(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* global_out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>* global_in, size_t size, size_t batch_sz) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> grid = cooperative_groups::this_grid();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block = cooperative_groups::this_thread_block();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> thread = cooperative_groups::this_thread();
    assert(size == batch_sz * grid.size()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Assume input size fits batch_sz * grid_size</span>

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared[]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// stages_count * block.size() * sizeof(int) bytes</span>
    size_t shared_offset[stages_count];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> s = 0; s &lt; stages_count; ++s) shared_offset[s] = s * block.size();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// No pipeline::shared_state needed</span>
    cuda::pipeline&lt;cuda::thread_scope_thread&gt; pipeline = cuda::make_pipeline();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> block_batch = [&amp;](size_t batch) -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> block.group_index().x * block.size() + grid.size() * batch;
    };

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (size_t compute_batch = 0, fetch_batch = 0; compute_batch &lt; batch_sz; ++compute_batch) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (; fetch_batch &lt; batch_sz &amp;&amp; fetch_batch &lt; (compute_batch + stages_count); ++fetch_batch) {
            pipeline.producer_acquire();
            size_t shared_idx = fetch_batch % stages_count;
            size_t batch_idx = fetch_batch;
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Each thread fetches its own data:</span>
            size_t thread_batch_idx = block_batch(batch_idx) + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The copy is performed by a single `thread` and the size of the batch is now that of a single element:</span>
            cuda::memcpy_async(thread, shared + shared_offset[shared_idx] + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, global_in + thread_batch_idx, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>), pipeline);
            pipeline.producer_commit();
        }
        pipeline.consumer_wait();
        block.sync(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __syncthreads: All memcpy_async of all threads in the block for this stage have completed here</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> shared_idx = compute_batch % stages_count;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> batch_idx = compute_batch;
        compute(global_out + block_batch(batch_idx), shared + shared_offset[shared_idx]);
        pipeline.consumer_release();
    }
}
</pre><p class="p">If the <samp class="ph codeph">compute</samp> operation only reads shared memory
                                    written to by other threads in the same warp as the current thread,
                                    <samp class="ph codeph">__syncwarp()</samp> suffices.
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="performance-guidance-memcpy_async"><a name="performance-guidance-memcpy_async" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#performance-guidance-memcpy_async" name="performance-guidance-memcpy_async" shape="rect">B.26.3.&nbsp;Performance Guidance for <samp class="ph codeph">memcpy_async</samp></a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">For compute capability 8.x, the pipeline mechanism is shared among CUDA
                                 threads in the same CUDA warp. This sharing causes batches of
                                 	       <samp class="ph codeph">memcpy_async</samp> to be entangled within a warp, which can
                                 	       impact performance under certain circumstances.
                              </p>
                              <p class="p">This section highlights the warp-entanglement effect on <em class="ph i">commit</em>, <em class="ph i">wait</em>, 
                                 and <em class="ph i">arrive</em> operations. Please refer to the
                                 <a class="xref" href="index.html#pipeline-interface" shape="rect">Pipeline Interface</a>
                                 and the <a class="xref" href="index.html#pipeline-primitives-interface" shape="rect">Pipeline Primitives Interface</a> for an overview
                                 of the individual operations.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="aligned_size_t"><a name="aligned_size_t" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#aligned_size_t" name="aligned_size_t" shape="rect">B.26.3.1.&nbsp;Alignment</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">On devices with compute capability 8.0, the <a class="xref" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async" target="_blank" shape="rect"><samp class="ph codeph">cp.async</samp>
                                       	    family of instructions</a> allows copying data from global to shared
                                    	    memory asynchronously. These instructions support copying 4, 8, and 16
                                    	    bytes at a time. If the size provided to <samp class="ph codeph">memcpy_async</samp> is
                                    	    a multiple of 4, 8, or 16, and both pointers passed to
                                    	    <samp class="ph codeph">memcpy_async</samp> are aligned to a 4, 8, or 16 alignment
                                    	    boundary, then <samp class="ph codeph">memcpy_async</samp> can be implemented using
                                    	    exclusively asynchronous memory operations.
                                 </p>
                                 <p class="p">For pointers to values of types with an alignment requirement of 1 or
                                    2, it is often not possible to prove that the pointers are always aligned
                                    to a higher alignment boundary. Determining whether the
                                    <samp class="ph codeph">cp.async</samp> instructions can or cannot be used must be
                                    delayed until run-time. Performing such a runtime alignment check
                                    increases code-size and adds runtime overhead.
                                 </p>
                                 <p class="p">The <a class="xref" href="https://nvidia.github.io/libcudacxx" target="_blank" shape="rect"><samp class="ph codeph">cuda::aligned_size_t&lt;size_t Align&gt;(size_t size)</samp></a><a class="xref" href="https://nvidia.github.io/libcudacxx" target="_blank" shape="rect"><samp class="ph codeph">Shape</samp></a> can be
                                    used to supply a proof that both pointers passed to
                                    <samp class="ph codeph">memcpy_async</samp> are aligned to an <samp class="ph codeph">Align</samp>
                                    alignment boundary, by passing it as an argument where the
                                    <samp class="ph codeph">memcpy_async</samp> APIs expect a <samp class="ph codeph">Shape</samp>:
                                 </p><pre xml:space="preserve">
  cuda::memcpy_async(group, dst, src, cuda::aligned_size_t&lt;16&gt;(N * block.size()), pipeline);
  </pre><p class="p">If the proof is incorrect, the behavior is undefined.</p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="trivially-copyable"><a name="trivially-copyable" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#trivially-copyable" name="trivially-copyable" shape="rect">B.26.3.2.&nbsp;Trivially copyable</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">On devices with compute capability 8.0, the <a class="xref" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async" target="_blank" shape="rect"><samp class="ph codeph">cp.async</samp>
                                       	    family of instructions</a> allows copying data from global to shared
                                    	    memory asynchronously. If the pointer types passed to
                                    	    <samp class="ph codeph">memcpy_async</samp> do not point to <a class="xref" href="https://en.cppreference.com/w/cpp/named_req/TriviallyCopyable" target="_blank" shape="rect"><samp class="ph codeph">TriviallyCopyable</samp></a>
                                    	    types, the copy constructor of each output element needs to be invoked,
                                    	    and these instructions cannot be used to accelerate
                                    	    <samp class="ph codeph">memcpy_async</samp>.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="warp-entanglement-commit"><a name="warp-entanglement-commit" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#warp-entanglement-commit" name="warp-entanglement-commit" shape="rect">B.26.3.3.&nbsp;Warp Entanglement - Commit</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">The sequence of <samp class="ph codeph">memcpy_async</samp> batches is shared across the warp. The commit
                                    operation is coalesced such that the sequence is incremented once for all converged
                                    threads that invoke the commit operation.  If the warp is fully converged, the sequence is
                                    incremented by one; if the warp is fully diverged, the sequence is incremented by 32.
                                 </p>
                                 <ul class="ul">
                                    <li class="li">Let <em class="ph i">PB</em> be the warp-shared pipeline's <em class="ph i">actual</em> sequence of batches.
                                       
                                       <p class="p"><samp class="ph codeph">PB = {B<sub class="ph sub">P0</sub>, B<sub class="ph sub">P1</sub>, B<sub class="ph sub">P2</sub>, , B<sub class="ph sub">PL</sub>}</samp></p>
                                    </li>
                                    <li class="li"> Let <em class="ph i">TB</em> be a thread's <em class="ph i">perceived</em> sequence of batches, as if the
                                       sequence were only incremented by this thread's invocation of the commit operation.
                                       
                                       <p class="p"><samp class="ph codeph">TB = {B<sub class="ph sub">T0</sub>, B<sub class="ph sub">T1</sub>, B<sub class="ph sub">T2</sub>, , B<sub class="ph sub">TL</sub>}</samp></p>
                                       <p class="p">The <samp class="ph codeph">pipeline::producer_commit()</samp>
                                          return value is from the thread's <em class="ph i">perceived</em> batch sequence.
                                       </p>
                                    </li>
                                    <li class="li">An index in a thread's perceived sequence always aligns to an equal or larger
                                       index in the actual warp-shared sequence. The sequences are equal only when all commit
                                       operations are invoked from converged threads. 
                                       
                                       <p class="p"><samp class="ph codeph">B<sub class="ph sub">Tn</sub>  B<sub class="ph sub">Pm</sub></samp> where <samp class="ph codeph">n &lt;= m</samp></p>
                                    </li>
                                 </ul>
                                 <p class="p"> For example, when a warp is fully diverged:</p>
                                 <ul class="ul">
                                    <li class="li">The warp-shared pipeline's actual sequence would be: 
                                       <samp class="ph codeph">PB = {0, 1, 2, 3, ..., 31}</samp> (<samp class="ph codeph">PL=31</samp>).
                                    </li>
                                    <li class="li">The perceived sequence for each thread of this warp would be:
                                       
                                       <ul class="ul">
                                          <li class="li"> Thread 0: <samp class="ph codeph">TB = {0}</samp> (<samp class="ph codeph">TL=0</samp>)
                                          </li>
                                          <li class="li"> Thread 1: <samp class="ph codeph">TB = {0}</samp> (<samp class="ph codeph">TL=0</samp>)
                                          </li>
                                          <li class="li"><samp class="ph codeph"></samp></li>
                                          <li class="li"> Thread 31: <samp class="ph codeph">TB = {0}</samp> (<samp class="ph codeph">TL=0</samp>)
                                          </li>
                                       </ul>
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="warp-entanglement-wait"><a name="warp-entanglement-wait" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#warp-entanglement-wait" name="warp-entanglement-wait" shape="rect">B.26.3.4.&nbsp;Warp Entanglement - Wait</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">A CUDA thread invokes either
                                    	     <samp class="ph codeph">pipeline_consumer_wait_prior&lt;N&gt;()</samp> or
                                    	     <samp class="ph codeph">pipeline::consumer_wait()</samp> to wait for batches in the
                                    	     <em class="ph i">perceived</em> sequence <samp class="ph codeph">TB</samp> to complete. Note that
                                    	     <samp class="ph codeph">pipeline::consumer_wait()</samp> is equivalent to
                                    	     <samp class="ph codeph">pipeline_consumer_wait_prior&lt;N&gt;()</samp>, where <samp class="ph codeph">N =
                                       	     PL</samp>. 
                                 </p>
                                 <p class="p">The <samp class="ph codeph">pipeline_consumer_wait_prior&lt;N&gt;()</samp> function waits for batches in the
                                    <em class="ph i">actual</em> sequence at least up to and including <samp class="ph codeph">PL-N</samp>. Since
                                    <samp class="ph codeph">TL &lt;= PL</samp>, waiting for batch up to and including <samp class="ph codeph">PL-N</samp>
                                    includes waiting for batch <samp class="ph codeph">TL-N</samp>.  Thus, when <samp class="ph codeph">TL &lt; PL</samp>,
                                    the thread will unintentionally wait for additional, more recent batches. 
                                 </p>
                                 <p class="p">In the extreme fully-diverged warp example above, each thread could wait for all
                                    32 batches.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="warp-entanglement-arrive-on"><a name="warp-entanglement-arrive-on" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#warp-entanglement-arrive-on" name="warp-entanglement-arrive-on" shape="rect">B.26.3.5.&nbsp;Warp Entanglement - Arrive-On</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">Warp-divergence affects the number of times an
                                    	  <samp class="ph codeph">arrive_on(bar)</samp> operation updates the barrier. If the
                                    	  invoking warp is fully converged, then the barrier is updated once. If the
                                    	  invoking warp is fully diverged, then 32 individual updates are applied to
                                    	  the barrier.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="keep-commit-arrive-on-ops-converged"><a name="keep-commit-arrive-on-ops-converged" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#keep-commit-arrive-on-ops-converged" name="keep-commit-arrive-on-ops-converged" shape="rect">B.26.3.6.&nbsp;Keep Commit and Arrive-On Operations Converged</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">It is recommended that commit and arrive-on invocations are by converged threads:</p>
                                 <ul class="ul">
                                    <li class="li"> to not over-wait, by keeping threads' perceived sequence of batches aligned
                                       with the actual sequence, and
                                    </li>
                                    <li class="li"> to minimize updates to the barrier object.</li>
                                 </ul>
                                 <p class="p">When code preceding these operations diverges threads, then the warp should be
                                    re-converged, via <samp class="ph codeph">__syncwarp</samp> before invoking commit or arrive-on
                                    operations.
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="pipeline-interface"><a name="pipeline-interface" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#pipeline-interface" name="pipeline-interface" shape="rect">B.26.4.&nbsp;Pipeline Interface</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The complete API documentation for <samp class="ph codeph">cuda::memcpy_async</samp> is
                                 provided in the <a class="xref" href="https://nvidia.github.io/libcudacxx" target="_blank" shape="rect">libcudacxx API</a>
                                 documentation along with some examples.
                              </p>
                              <p class="p">The <samp class="ph codeph">pipeline</samp> interface requires
                              </p>
                              <ul class="ul">
                                 <li class="li">at least CUDA 11.0,</li>
                                 <li class="li">at least ISO C++ 2011 compatibility, e.g., to be compiled with
                                    <samp class="ph codeph">-std=c++11</samp>, and
                                 </li>
                                 <li class="li"><samp class="ph codeph">#include &lt;cuda/pipeline&gt;</samp>.
                                 </li>
                              </ul>
                              <p class="p">For a C-like interface, when compiling without ISO C++ 2011
                                 compatibility, see <a class="xref" href="index.html#pipeline-primitives-interface" shape="rect">Pipeline Primitives Interface</a>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="pipeline-primitives-interface"><a name="pipeline-primitives-interface" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#pipeline-primitives-interface" name="pipeline-primitives-interface" shape="rect">B.26.5.&nbsp;Pipeline Primitives Interface</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Pipeline primitives are a C-like interface for <samp class="ph codeph">memcpy_async</samp> functionality.  The
                                 pipeline primitives interface is available by including the
                                 <samp class="ph codeph">&lt;cuda_pipeline.h&gt;</samp> header.  When compiling without 
                                 ISO C++ 2011 compatibility, include the <samp class="ph codeph">&lt;cuda_pipeline_primitives.h&gt;</samp>
                                 header.
                              </p>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="memcpy_async-primitive"><a name="memcpy_async-primitive" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#memcpy_async-primitive" name="memcpy_async-primitive" shape="rect">B.26.5.1.&nbsp;<samp class="ph codeph">memcpy_async</samp> Primitive</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __pipeline_memcpy_async(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> dst_shared,
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> src_global,
                             size_t size_and_align,
                             size_t zfill=0);
</pre><ul class="ul">
                                    <li class="li"> Request that the following operation be submitted for asynchronous
                                       evaluation:
                                       <pre xml:space="preserve">
  size_t i = 0;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (; i &lt; size_and_align - zfill; ++i) ((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>*)dst_shared)[i] = ((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>*)src_shared)[i]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* copy */</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (; i &lt; size_and_align; ++i) ((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>*)dst_shared)[i] = 0; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* zero-fill */</span>
</pre></li>
                                    <li class="li">Requirements:
                                       
                                       <ul class="ul">
                                          <li class="li"><samp class="ph codeph">dst_shared</samp> must be a pointer to the shared memory
                                             destination for the <samp class="ph codeph">memcpy_async</samp>.
                                          </li>
                                          <li class="li"><samp class="ph codeph">src_global</samp> must be a pointer to the global memory source
                                             for the <samp class="ph codeph">memcpy_async</samp>.
                                          </li>
                                          <li class="li"><samp class="ph codeph">size_and_align</samp> must be 4, 8, or 16.
                                          </li>
                                          <li class="li"><samp class="ph codeph">zfill &lt;= size_and_align</samp>.
                                          </li>
                                          <li class="li"><samp class="ph codeph">size_and_align</samp> must be the alignment of
                                             <samp class="ph codeph">dst_shared</samp> and <samp class="ph codeph">src_global</samp>.
                                          </li>
                                       </ul>
                                    </li>
                                    <li class="li"> It is a race condition for any thread to modify the source memory or observe
                                       the destination memory prior to waiting for the <samp class="ph codeph">memcpy_async</samp> operation to complete.
                                       		     Between submitting a <samp class="ph codeph">memcpy_async</samp> operation and waiting for its completion, any of the
                                       following actions introduces a race condition:
                                       
                                       <ul class="ul">
                                          <li class="li"> Loading from <samp class="ph codeph">dst_shared</samp>.
                                          </li>
                                          <li class="li"> Storing to <samp class="ph codeph">dst_shared</samp> or <samp class="ph codeph">src_global</samp>.
                                          </li>
                                          <li class="li"> Applying an atomic update to <samp class="ph codeph">dst_shared</samp> or
                                             <samp class="ph codeph">src_global</samp>.
                                          </li>
                                       </ul>
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="commit-primitive"><a name="commit-primitive" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#commit-primitive" name="commit-primitive" shape="rect">B.26.5.2.&nbsp;Commit Primitive</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __pipeline_commit();
</pre><ul class="ul">
                                    <li class="li"> Commit submitted <samp class="ph codeph">memcpy_async</samp> to the pipeline as the current batch.
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="wait-primitive"><a name="wait-primitive" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#wait-primitive" name="wait-primitive" shape="rect">B.26.5.3.&nbsp;Wait Primitive</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __pipeline_wait_prior(size_t N);
</pre><ul class="ul">
                                    <li class="li">Let <samp class="ph codeph">{0, 1, 2, ..., L}</samp> be the sequence of indices associated
                                       with invocations of <samp class="ph codeph">__pipeline_commit()</samp> by a given thread.
                                    </li>
                                    <li class="li">Wait for completion of batches <em class="ph i">at least</em> up to and including 
                                       <samp class="ph codeph">L-N</samp>.
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="arrive-primitive"><a name="arrive-primitive" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#arrive-primitive" name="arrive-primitive" shape="rect">B.26.5.4.&nbsp;Arrive On Barrier Primitive</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __pipeline_arrive_on(__mbarrier_t* bar);
</pre><ul class="ul">
                                    <li class="li"><samp class="ph codeph">bar</samp> points to a barrier in shared memory.
                                    </li>
                                    <li class="li"> When all <samp class="ph codeph">memcpy_async</samp> operations sequenced before this call have completed,
                                       the effect is as if <samp class="ph codeph">__mbarrier_arrive(bar)</samp> was called (see 
                                       <a class="xref" href="index.html#memory_barrier_primitives_interface" shape="rect">Memory Barrier Primitives Interface</a>).
                                       
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="profiler-counter-function"><a name="profiler-counter-function" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#profiler-counter-function" name="profiler-counter-function" shape="rect">B.27.&nbsp;Profiler Counter Function</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Each multiprocessor has a set of sixteen hardware counters that an
                              application can increment with a single instruction by calling the
                              <samp class="ph codeph">__prof_trigger()</samp> function.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __prof_trigger(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> counter);</pre><p class="p">increments by one per warp the per-multiprocessor hardware counter of
                              index <samp class="ph codeph">counter</samp>. Counters 8 to 15 are reserved and
                              should not be used by applications.
                           </p>
                           <p class="p">The value of counters 0, 1, ..., 7 can be obtained via <samp class="ph codeph">nvprof</samp> by
                              <samp class="ph codeph">nvprof --events prof_trigger_0x</samp> where <samp class="ph codeph">x</samp>
                              is 0, 1, ..., 7. All counters are reset before each
                              kernel launch (note that when collecting counters, kernel launches are
                              synchronous as mentioned in <a class="xref" href="index.html#concurrent-execution-host-device" shape="rect">Concurrent Execution between Host and Device</a>).
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="assertion"><a name="assertion" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#assertion" name="assertion" shape="rect">B.28.&nbsp;Assertion</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Assertion is only supported by devices of compute capability 2.x and higher.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> assert(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> expression);</pre><p class="p">stops the kernel execution if <samp class="ph codeph">expression</samp> is equal to zero. If the
                              program is run within a debugger, this triggers a breakpoint and the debugger can be
                              used to inspect the current state of the device. Otherwise, each thread for which
                              <samp class="ph codeph">expression</samp> is equal to zero prints a message to <em class="ph i">stderr</em>
                              after synchronization with the host via <samp class="ph codeph">cudaDeviceSynchronize()</samp>,
                              <samp class="ph codeph">cudaStreamSynchronize()</samp>, or
                              <samp class="ph codeph">cudaEventSynchronize()</samp>. The format of this message is as
                              follows: 
                           </p><pre xml:space="preserve">&lt;filename&gt;:&lt;line number&gt;:&lt;function&gt;:
block: [blockId.x,blockId.x,<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.z],
thread: [<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x,<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.y,<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.z]
Assertion `&lt;expression&gt;` failed.</pre><p class="p">Any subsequent host-side synchronization calls made for the same device will return
                              <samp class="ph codeph">cudaErrorAssert</samp>. No more commands can be sent to this device until
                              <samp class="ph codeph">cudaDeviceReset()</samp> is called to reinitialize the device.
                              
                           </p>
                           <p class="p">If <samp class="ph codeph">expression</samp> is different from zero, the kernel execution is
                              unaffected. 
                           </p>
                           <p class="p">For example, the following program from source file <em class="ph i">test.cu</em></p><pre xml:space="preserve">#include &lt;assert.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> testAssert(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> is_one = 1;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> should_be_one = 0;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This will have no effect</span>
    assert(is_one);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This will halt kernel execution</span>
    assert(should_be_one);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> argc, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* argv[])
{
    testAssert<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre><p class="p">will output:</p><pre xml:space="preserve">test.cu:19: <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> testAssert(): block: [0,0,0], thread: [0,0,0] Assertion `should_be_one` failed.</pre><p class="p">Assertions are for debugging purposes. They can affect performance and it is therefore
                              recommended to disable them in production code. They can be disabled at compile time by
                              defining the <samp class="ph codeph">NDEBUG</samp> preprocessor macro before including
                              <samp class="ph codeph">assert.h</samp>. Note that <samp class="ph codeph">expression</samp> should not be an
                              expression with side effects (something like<samp class="ph codeph"> (++i &gt; 0)</samp>, for example),
                              otherwise disabling the assertion will affect the functionality of the code.
                              
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="trap-function"><a name="trap-function" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#trap-function" name="trap-function" shape="rect">B.29.&nbsp;Trap function</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">A trap operation can be initiated by calling the <samp class="ph codeph">__trap()</samp>
                              function from any device thread.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __trap();</pre><p class="p">The execution of the kernel is aborted and an interrupt is raised in the
                              host program.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="breakpoint-function"><a name="breakpoint-function" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#breakpoint-function" name="breakpoint-function" shape="rect">B.30.&nbsp;Breakpoint Function</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Execution of a kernel function can be suspended by calling the
                              <samp class="ph codeph">__brkpt()</samp> function from any device thread.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> __brkpt();</pre></div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="formatted-output"><a name="formatted-output" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#formatted-output" name="formatted-output" shape="rect">B.31.&nbsp;Formatted Output</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Formatted output is only supported by devices of compute capability 2.x and higher.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *format[, arg, ...]);</pre><p class="p">prints formatted output from a kernel to a host-side output stream.</p>
                           <p class="p">The in-kernel <samp class="ph codeph">printf()</samp> function behaves in a similar way to the standard
                              C-library printf() function, and the user is referred to the host system's manual
                              pages for a complete description of <samp class="ph codeph">printf()</samp> behavior. In essence,
                              the string passed in as <samp class="ph codeph">format</samp> is output to a stream on the host,
                              with substitutions made from the argument list wherever a format specifier is
                              encountered. Supported format specifiers are listed below. 
                           </p>
                           <p class="p">The <samp class="ph codeph">printf()</samp> command is executed as any other device-side function:
                              per-thread, and in the context of the calling thread. From a multi-threaded kernel,
                              this means that a straightforward call to <samp class="ph codeph">printf()</samp> will be executed
                              by every thread, using that thread's data as specified. Multiple versions of the
                              output string will then appear at the host stream, once for each thread which
                              encountered the <samp class="ph codeph">printf()</samp>. 
                           </p>
                           <p class="p">It is up to the programmer to limit the output to a single thread if only a single output string is desired (see <a class="xref" href="index.html#examples-per-thread" shape="rect">Examples</a> for an illustrative example).
                           </p>
                           <p class="p">Unlike the C-standard <samp class="ph codeph">printf()</samp>, which returns the number of characters
                              printed, CUDA's <samp class="ph codeph">printf()</samp> returns the number of arguments parsed. If
                              no arguments follow the format string, 0 is returned. If the format string is NULL,
                              -1 is returned. If an internal error occurs, -2 is returned. 
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="format-specifiers"><a name="format-specifiers" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#format-specifiers" name="format-specifiers" shape="rect">B.31.1.&nbsp;Format Specifiers</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">As for standard <samp class="ph codeph">printf()</samp>, format specifiers take the
                                 form: <samp class="ph codeph">%[flags][width][.precision][size]type</samp></p>
                              <p class="p">The following fields are supported (see widely-available documentation
                                 for a complete description of all behaviors):
                              </p>
                              <ul class="ul">
                                 <li class="li">Flags: <samp class="ph codeph">'#' ' ' '0' '+' '-'</samp></li>
                                 <li class="li">Width: <samp class="ph codeph">'*' '0-9'</samp></li>
                                 <li class="li">Precision: <samp class="ph codeph">'0-9'</samp></li>
                                 <li class="li">Size: <samp class="ph codeph">'h' 'l' 'll'</samp></li>
                                 <li class="li">Type: <samp class="ph codeph">"%cdiouxXpeEfgGaAs"</samp></li>
                              </ul>
                              <p class="p">Note that CUDA's <samp class="ph codeph">printf()</samp>will accept any combination
                                 of flag, width, precision, size and type, whether or not overall they
                                 form a valid format specifier.  In other words, "<samp class="ph codeph">%hd</samp>"
                                 will be accepted and printf will expect a double-precision variable in
                                 the corresponding location in the argument list. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="limitations"><a name="limitations" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#limitations" name="limitations" shape="rect">B.31.2.&nbsp;Limitations</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">Final formatting of the <samp class="ph codeph">printf() </samp>output takes place
                                 on the host system.  This means that the format string must be
                                 understood by the host-system's compiler and C library. Every effort
                                 has been made to ensure that the format specifiers supported by CUDA's
                                 printf function form a universal subset from the most common host
                                 compilers, but exact behavior will be host-OS-dependent. 
                              </p>
                              <p class="p">As described in <a class="xref" href="index.html#format-specifiers" shape="rect">Format Specifiers</a>,
                                 <samp class="ph codeph">printf()</samp> will accept <em class="ph i">all</em> combinations of valid
                                 flags and types. This is because it cannot determine what will and will
                                 not be valid on the host system where the final output is formatted.
                                 The effect of this is that output may be undefined if the program emits
                                 a format string which contains invalid combinations. 
                              </p>
                              <p class="p">The <samp class="ph codeph">printf()</samp> command can accept at most 32 arguments
                                 in addition to the format string. Additional arguments beyond this will
                                 be ignored, and the format specifier output as-is. 
                              </p>
                              <p class="p">Owing to the differing size of the <samp class="ph codeph">long</samp> type on
                                 64-bit Windows platforms (four bytes on 64-bit Windows platforms, eight
                                 bytes on other 64-bit platforms), a kernel which is compiled on a
                                 non-Windows 64-bit machine but then run on a win64 machine will see
                                 corrupted output for all format strings which include
                                 "<samp class="ph codeph">%ld</samp>". It is recommended that the compilation platform
                                 matches the execution platform to ensure safety. 
                              </p>
                              <p class="p">The output buffer for <samp class="ph codeph">printf()</samp> is set to a fixed size
                                 before kernel launch (see <a class="xref" href="index.html#associated-host-side-api" shape="rect">Associated Host-Side API</a>).
                                 It is circular and if more output is produced during kernel execution
                                 than can fit in the buffer, older output is overwritten. It is flushed
                                 only when one of these actions is performed:
                              </p>
                              <ul class="ul">
                                 <li class="li">Kernel launch via <samp class="ph codeph">&lt;&lt;&lt;&gt;&gt;&gt;</samp> or
                                    <samp class="ph codeph">cuLaunchKernel()</samp> (at the start of the launch, and if
                                    the CUDA_LAUNCH_BLOCKING environment variable is set to 1, at the end
                                    of the launch as well),
                                 </li>
                                 <li class="li">Synchronization via <samp class="ph codeph">cudaDeviceSynchronize()</samp>,
                                    <samp class="ph codeph">cuCtxSynchronize()</samp>,
                                    <samp class="ph codeph">cudaStreamSynchronize()</samp>,
                                    <samp class="ph codeph">cuStreamSynchronize()</samp>,
                                    <samp class="ph codeph">cudaEventSynchronize()</samp>, or
                                    <samp class="ph codeph">cuEventSynchronize()</samp>,
                                 </li>
                                 <li class="li">Memory copies via any blocking version of
                                    <samp class="ph codeph">cudaMemcpy*()</samp> or <samp class="ph codeph">cuMemcpy*()</samp>,
                                 </li>
                                 <li class="li">Module loading/unloading via <samp class="ph codeph">cuModuleLoad()</samp> or
                                    <samp class="ph codeph">cuModuleUnload()</samp>,
                                 </li>
                                 <li class="li">Context destruction via <samp class="ph codeph">cudaDeviceReset()</samp> or
                                    <samp class="ph codeph">cuCtxDestroy()</samp>.
                                 </li>
                                 <li class="li">Prior to executing a stream callback added by <samp class="ph codeph">cudaStreamAddCallback</samp>
                                    or <samp class="ph codeph">cuStreamAddCallback</samp>.
                                 </li>
                              </ul>
                              <p class="p">Note that the buffer is not flushed automatically when the program
                                 exits. The user must call <samp class="ph codeph">cudaDeviceReset()</samp> or
                                 <samp class="ph codeph">cuCtxDestroy()</samp> explicitly, as shown in the examples
                                 below.
                              </p>
                              <p class="p">Internally <samp class="ph codeph">printf()</samp> uses a shared data structure and so it is possible that calling <samp class="ph codeph">printf()</samp> might change the order of execution of threads.
                                 In particular, a thread which calls <samp class="ph codeph">printf()</samp> might take a longer execution path than one which does not call <samp class="ph codeph">printf()</samp>, and that path length is dependent upon the parameters of the <samp class="ph codeph">printf()</samp>.
                                 Note, however, that CUDA makes no guarantees of thread execution order except at explicit <samp class="ph codeph">__syncthreads()</samp> barriers, so it is impossible to tell whether execution order has been modified by <samp class="ph codeph">printf()</samp> or by other scheduling behaviour in the hardware.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="associated-host-side-api"><a name="associated-host-side-api" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#associated-host-side-api" name="associated-host-side-api" shape="rect">B.31.3.&nbsp;Associated Host-Side API</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The following API functions get and set the size of the buffer used to transfer the
                                 <samp class="ph codeph">printf()</samp> arguments and internal metadata to the host (default is 1
                                 megabyte):
                                 
                              </p>
                              <ul class="ul">
                                 <li class="li"><samp class="ph codeph">cudaDeviceGetLimit(size_t* size,cudaLimitPrintfFifoSize)</samp></li>
                                 <li class="li"><samp class="ph codeph">cudaDeviceSetLimit(cudaLimitPrintfFifoSize, size_t size)</samp></li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="examples"><a name="examples" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#examples" name="examples" shape="rect">B.31.4.&nbsp;Examples</a></h3>
                        <div class="body conbody">
                           <div class="p">The following code sample:
                              <pre xml:space="preserve">
#include &lt;stdio.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> helloCUDA(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> f)
{
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Hello thread %d, f=%f\n"</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, f);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    helloCUDA<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1, 5<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(1.2345f);
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           <p class="p">will output:</p>
                           <div class="p"><pre class="pre screen" xml:space="preserve">Hello thread 2, f=1.2345
Hello thread 1, f=1.2345
Hello thread 4, f=1.2345
Hello thread 0, f=1.2345
Hello thread 3, f=1.2345</pre></div>
                           <p class="p">Notice how each thread encounters the <samp class="ph codeph">printf()</samp> command, so there are as
                              many lines of output as there were threads launched in the grid. As expected, global
                              values (i.e., <samp class="ph codeph">float f</samp>) are common between all threads, and local values
                              (i.e., <samp class="ph codeph">threadIdx.x</samp>) are distinct per-thread. 
                           </p>
                           <div class="p">The following code sample:
                              
                              <pre xml:space="preserve">
#include &lt;stdio.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> helloCUDA(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> f)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Hello thread %d, f=%f\n"</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, f) ;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    helloCUDA<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1, 5<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(1.2345f);
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           <p class="p">will output:</p>
                           <div class="p"><pre class="pre screen" xml:space="preserve">Hello thread 0, f=1.2345</pre></div>
                           <p class="p">
                              Self-evidently, the <samp class="ph codeph">if()</samp> statement limits which threads will call <samp class="ph codeph">printf</samp>, so that only a single line of output is seen. 
                              
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="dynamic-global-memory-allocation-and-operations"><a name="dynamic-global-memory-allocation-and-operations" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#dynamic-global-memory-allocation-and-operations" name="dynamic-global-memory-allocation-and-operations" shape="rect">B.32.&nbsp;Dynamic Global Memory Allocation and Operations</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Dynamic global memory allocation and operations are only supported by devices of compute capability 2.x and higher.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* malloc(size_t size);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *__nv_aligned_device_malloc(size_t size, size_t align);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> free(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* ptr);</pre><p class="p">allocate and free memory dynamically from a fixed-size heap in global memory.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* memcpy(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* dest, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* src, size_t size);</pre><p class="p">copy <samp class="ph codeph">size</samp> bytes from the memory location pointed by <samp class="ph codeph">src</samp> to the memory location pointed by <samp class="ph codeph">dest</samp>.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* memset(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* ptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value, size_t size);</pre><p class="p">set <samp class="ph codeph">size</samp> bytes of memory block pointed by <samp class="ph codeph">ptr</samp> to <samp class="ph codeph">value</samp> (interpreted as an unsigned char).
                           </p>
                           <p class="p">The CUDA in-kernel <samp class="ph codeph">malloc() </samp>function allocates at least
                              <samp class="ph codeph">size</samp> bytes from the device heap and returns a pointer to the
                              allocated memory or NULL if insufficient memory exists to fulfill the request. The
                              returned pointer is guaranteed to be aligned to a 16-byte boundary. 
                           </p>
                           <p class="p">The CUDA in-kernel <samp class="ph codeph">__nv_aligned_device_malloc()</samp> function allocates at least
                              <samp class="ph codeph">size</samp> bytes from the device heap and returns a pointer to the
                              allocated memory or NULL if insufficient memory exists to fulfill the requested size or
                              alignment. The address of the allocated memory will be a multiple of <samp class="ph codeph">align</samp>.  <samp class="ph codeph">align</samp>
                              must be a non-zero power of 2.
                           </p>
                           <p class="p">The CUDA in-kernel <samp class="ph codeph">free()</samp> function deallocates the memory pointed to by
                              <samp class="ph codeph">ptr</samp>, which must have been returned by a previous call to
                              <samp class="ph codeph">malloc()</samp> or <samp class="ph codeph">__nv_aligned_device_malloc()</samp>. 
                              If <samp class="ph codeph">ptr</samp> is NULL, the call to
                              <samp class="ph codeph">free()</samp> is ignored. Repeated calls to <samp class="ph codeph">free()</samp>
                              with the same <samp class="ph codeph">ptr</samp> has undefined behavior. 
                           </p>
                           <p class="p">The memory allocated by a given CUDA thread via <samp class="ph codeph">malloc()</samp> or
                              <samp class="ph codeph">__nv_aligned_device_malloc()</samp> remains
                              allocated for the lifetime of the CUDA context, or until it is explicitly released
                              by a call to <samp class="ph codeph">free()</samp>. It can be used by any other CUDA threads even
                              from subsequent kernel launches. Any CUDA thread may free memory allocated by
                              another thread, but care should be taken to ensure that the same pointer is not
                              freed more than once. 
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="heap-memory-allocation"><a name="heap-memory-allocation" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#heap-memory-allocation" name="heap-memory-allocation" shape="rect">B.32.1.&nbsp;Heap Memory Allocation</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">The device memory heap has a fixed size that must be specified before any program using
                                 <samp class="ph codeph">malloc()</samp>, <samp class="ph codeph">__nv_aligned_device_malloc()</samp> or
                                 <samp class="ph codeph">free()</samp> is loaded into the context. A
                                 default heap of eight megabytes is allocated if any program uses
                                 <samp class="ph codeph">malloc()</samp> or <samp class="ph codeph">__nv_aligned_device_malloc()</samp>
                                 without explicitly specifying the heap size.
                                 
                              </p>
                              <p class="p">The following API functions get and set the heap size:</p>
                              <ul class="ul">
                                 <li class="li"><samp class="ph codeph">cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize)</samp></li>
                                 <li class="li"><samp class="ph codeph">cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size)</samp></li>
                              </ul>
                              <p class="p">
                                 The heap size granted will be at least <samp class="ph codeph">size</samp> bytes.
                                 <samp class="ph codeph">cuCtxGetLimit()</samp>and <samp class="ph codeph">cudaDeviceGetLimit()</samp> return the
                                 currently requested heap size.
                                 
                              </p>
                              <p class="p">The actual memory allocation for the heap occurs when a module is loaded into the context, either
                                 explicitly via the CUDA driver API (see <a class="xref" href="index.html#module" shape="rect">Module</a>), or implicitly via the CUDA runtime
                                 API (see <a class="xref" href="index.html#cuda-c-runtime" shape="rect">CUDA Runtime</a>). If the memory allocation fails, the module load will generate a
                                 <samp class="ph codeph">CUDA_ERROR_SHARED_OBJECT_INIT_FAILED</samp> error.
                                 
                              </p>
                              <p class="p">Heap size cannot be changed once a module load has occurred and it does not resize dynamically according to need.</p>
                              <p class="p">Memory reserved for the device heap is in addition to memory allocated through host-side CUDA API
                                 calls such as <samp class="ph codeph">cudaMalloc()</samp>.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="interoperability-host-memory-api"><a name="interoperability-host-memory-api" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#interoperability-host-memory-api" name="interoperability-host-memory-api" shape="rect">B.32.2.&nbsp;Interoperability with Host Memory API</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p">
                                 Memory allocated via device <samp class="ph codeph">malloc()</samp> or <samp class="ph codeph">__nv_aligned_device_malloc()</samp>
                                 cannot be freed using the runtime (i.e., by calling any of the free memory functions from <a class="xref" href="index.html#device-memory" shape="rect">Device Memory</a>).
                                 
                              </p>
                              <p class="p">Similarly, memory allocated via the runtime (i.e., by calling any of the memory allocation functions from <a class="xref" href="index.html#device-memory" shape="rect">Device Memory</a>) cannot be freed via <samp class="ph codeph">free()</samp>.
                                 
                              </p>
                              <p class="p"> In addition, memory allocated by a call to <samp class="ph codeph">malloc()</samp> or <samp class="ph codeph">__nv_aligned_device_malloc()</samp>
                                 	       in device code cannot be used in any runtime or driver API calls (i.e. cudaMemcpy, cudaMemset, etc).
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="examples-per-thread"><a name="examples-per-thread" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#examples-per-thread" name="examples-per-thread" shape="rect">B.32.3.&nbsp;Examples</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn">
                              <p class="p"></p>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="per-thread-allocation"><a name="per-thread-allocation" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#per-thread-allocation" name="per-thread-allocation" shape="rect">B.32.3.1.&nbsp;Per Thread Allocation</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn">
                                 <p class="p">The following code sample:</p><pre xml:space="preserve">
#include &lt;stdlib.h&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;stdio.h&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> mallocTest()
{
    size_t size = 123;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* ptr = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>*)malloc(size);
    memset(ptr, 0, size);
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Thread %d got pointer: %p\n"</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, ptr);
    free(ptr);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set a heap size of 128 megabytes. Note that this must</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// be done before any kernel is launched.</span>
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);
    mallocTest<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1, 5<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre><p class="p">will output:</p><pre xml:space="preserve">Thread 0 got pointer: 00057020
Thread 1 got pointer: 0005708c
Thread 2 got pointer: 000570f8
Thread 3 got pointer: 00057164
Thread 4 got pointer: 000571d0</pre><p class="p">
                                    Notice how each thread encounters the <samp class="ph codeph">malloc()</samp> and <samp class="ph codeph">memset()</samp> commands and so receives and initializes its
                                    own allocation. (Exact pointer values will vary: these are illustrative.)
                                    
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="per-thread-block-allocation"><a name="per-thread-block-allocation" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#per-thread-block-allocation" name="per-thread-block-allocation" shape="rect">B.32.3.2.&nbsp;Per Thread Block Allocation</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">
#include &lt;stdlib.h&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> mallocTest()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* data;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The first thread in the block does the allocation and then</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// shares the pointer with all other threads through shared memory,</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// so that access can easily be coalesced.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 64 bytes per thread are allocated.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0) {
        size_t size = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x * 64;
        data = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>*)malloc(size);
    }
    __syncthreads();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Check for failure</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (data == NULL)
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Threads index into the memory, ensuring coalescence</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* ptr = data;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 64; ++i)
        ptr[i * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Ensure all threads complete before freeing </span>
    __syncthreads();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Only one thread may free the memory!</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0)
        free(data);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);
    mallocTest<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>10, 128<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           </div>
                        </div>
                        <div class="topic reference nested3" xml:lang="en-US" id="allocation-persisting-kernel-launches"><a name="allocation-persisting-kernel-launches" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#allocation-persisting-kernel-launches" name="allocation-persisting-kernel-launches" shape="rect">B.32.3.3.&nbsp;Allocation Persisting Between Kernel Launches</a></h3>
                           <div class="body refbody">
                              <div class="section refsyn"><pre xml:space="preserve">#include &lt;stdlib.h&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;stdio.h&gt;</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define NUM_BLOCKS 20</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* dataptr[NUM_BLOCKS]; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Per-block pointer</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> allocmem()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Only the first thread in the block does the allocation</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// since we want only one allocation per block.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0)
        dataptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>*)malloc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x * 4);
    __syncthreads();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Check for failure</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (dataptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x] == NULL)
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Zero the data with all threads in parallel</span>
    dataptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x][<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = 0;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Simple example: store thread ID into each element</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> usemem()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* ptr = dataptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (ptr != NULL)
        ptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] += <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Print the content of the buffer before freeing it</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> freemem()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>* ptr = dataptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (ptr != NULL)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Block %d, Thread %d: final value = %d\n"</span>,
                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x, ptr[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x]);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Only free from one thread!</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0)
        free(ptr);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory</span>
    allocmem<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> NUM_BLOCKS, 10 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Use memory</span>
    usemem<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> NUM_BLOCKS, 10 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    usemem<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> NUM_BLOCKS, 10 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    usemem<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> NUM_BLOCKS, 10 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free memory</span>
    freemem<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> NUM_BLOCKS, 10 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();

    cudaDeviceSynchronize();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="execution-configuration"><a name="execution-configuration" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#execution-configuration" name="execution-configuration" shape="rect">B.33.&nbsp;Execution Configuration</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">Any call to a <samp class="ph codeph">__global__</samp> function must specify the
                              <dfn class="term">execution configuration</dfn> for that call. The execution
                              configuration defines the dimension of the grid and blocks that will be
                              used to execute the function on the device, as well as the associated
                              stream (see <a class="xref" href="index.html#cuda-c-runtime" shape="rect">CUDA Runtime</a> for a description of
                              streams).
                           </p>
                           <p class="p">The execution configuration is specified by inserting an expression of
                              the form <samp class="ph codeph">&lt;&lt;&lt; Dg, Db, Ns, S &gt;&gt;&gt;</samp> between the
                              function name and the parenthesized argument list, where:
                           </p>
                           <ul class="ul">
                              <li class="li"><samp class="ph codeph">Dg</samp> is of type <samp class="ph codeph">dim3</samp> (see <a class="xref" href="index.html#dim3" shape="rect">dim3</a>) and specifies the dimension and size of the grid,
                                 such that <samp class="ph codeph">Dg.x * Dg.y * Dg.z</samp> equals the number of
                                 blocks being launched;
                              </li>
                              <li class="li"><samp class="ph codeph">Db</samp> is of type <samp class="ph codeph">dim3</samp> (see <a class="xref" href="index.html#dim3" shape="rect">dim3</a>) and specifies the dimension and size of each
                                 block, such that <samp class="ph codeph">Db.x * Db.y * Db.z</samp> equals the
                                 number of threads per block;
                              </li>
                              <li class="li"><samp class="ph codeph">Ns</samp> is of type <samp class="ph codeph">size_t</samp> and
                                 specifies the number of bytes in shared memory that is dynamically
                                 allocated per block for this call in addition to the statically
                                 allocated memory; this dynamically allocated memory is used by any of
                                 the variables declared as an external array as mentioned in <a class="xref" href="index.html#shared" shape="rect">__shared__</a>; <samp class="ph codeph">Ns</samp> is an optional argument
                                 which defaults to 0;
                              </li>
                              <li class="li"><samp class="ph codeph">S</samp> is of type <samp class="ph codeph">cudaStream_t</samp> and
                                 specifies the associated stream; <samp class="ph codeph">S</samp> is an optional
                                 argument which defaults to 0.
                              </li>
                           </ul>
                           <p class="p">As an example, a function declared as</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* parameter);</pre><p class="p">must be called like this:</p><pre xml:space="preserve">Func<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> Dg, Db, Ns <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(parameter);</pre><p class="p">The arguments to the execution configuration are evaluated before the
                              actual function arguments.
                           </p>
                           <p class="p">The function call will fail if <samp class="ph codeph">Dg</samp> or
                              <samp class="ph codeph">Db</samp> are greater than the maximum sizes allowed for the
                              device as specified in <a class="xref" href="index.html#compute-capabilities" shape="rect">Compute Capabilities</a>, or if
                              <samp class="ph codeph">Ns</samp> is greater than the maximum amount of shared memory
                              available on the device, minus the amount of shared memory required for
                              static allocation.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="launch-bounds"><a name="launch-bounds" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#launch-bounds" name="launch-bounds" shape="rect">B.34.&nbsp;Launch Bounds</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">As discussed in detail in <a class="xref" href="index.html#multiprocessor-level" shape="rect">Multiprocessor Level</a>, the fewer registers a kernel uses, the more threads and thread blocks are likely to reside on a multiprocessor, which can
                              improve performance.
                              
                           </p>
                           <p class="p">Therefore, the compiler uses heuristics to minimize register usage while keeping register spilling (see <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>) and instruction count to a minimum. An application can optionally aid these heuristics by providing additional information
                              to the compiler in the form of launch bounds that are specified using the <samp class="ph codeph">__launch_bounds__()</samp> qualifier in the definition of a <samp class="ph codeph">__global__</samp> function:
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>
__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)
MyKernel(...)
{
    ...
}</pre><ul class="ul">
                              <li class="li"><samp class="ph codeph">maxThreadsPerBlock</samp> specifies the maximum number of threads per block with which the application will ever launch <samp class="ph codeph">MyKernel()</samp>; it compiles to the <samp class="ph codeph">.maxntid</samp><dfn class="term">PTX</dfn> directive;
                                 
                              </li>
                              <li class="li"><samp class="ph codeph">minBlocksPerMultiprocessor</samp> is optional and specifies the desired minimum number of resident blocks per multiprocessor; it compiles to the <samp class="ph codeph">.minnctapersm</samp><dfn class="term">PTX</dfn> directive.
                                 
                              </li>
                           </ul>
                           <p class="p">
                              If launch bounds are specified, the compiler first derives from them the upper limit <em class="ph i">L</em> on the number of registers the kernel should use to ensure that <samp class="ph codeph">minBlocksPerMultiprocessor</samp> blocks (or a single block if <samp class="ph codeph">minBlocksPerMultiprocessor</samp> is not specified) of <samp class="ph codeph">maxThreadsPerBlock</samp> threads can reside on the multiprocessor (see <a class="xref" href="index.html#hardware-multithreading" shape="rect">Hardware Multithreading</a> for the relationship between the number of registers used by a kernel and the number of registers allocated per block). The
                              compiler then optimizes register usage in the following way:
                              
                           </p>
                           <ul class="ul">
                              <li class="li">
                                 If the initial register usage is higher than <em class="ph i">L</em>, the compiler reduces it further until it becomes less or equal to <em class="ph i">L</em>, usually at the expense of more local memory usage and/or higher number of instructions;
                                 
                              </li>
                              <li class="li">
                                 If the initial register usage is lower than <em class="ph i">L</em><ul class="ul">
                                    <li class="li">
                                       If <samp class="ph codeph">maxThreadsPerBlock</samp> is specified and <samp class="ph codeph">minBlocksPerMultiprocessor</samp> is not, the compiler uses <samp class="ph codeph">maxThreadsPerBlock</samp> to determine the register usage thresholds for the transitions between <samp class="ph codeph">n</samp> and <samp class="ph codeph">n+1</samp> resident blocks (i.e., when using one less register makes room for an additional resident block as in the example of <a class="xref" href="index.html#multiprocessor-level" shape="rect">Multiprocessor Level</a>) and then applies similar heuristics as when no launch bounds are specified;
                                       
                                    </li>
                                    <li class="li">
                                       If both <samp class="ph codeph">minBlocksPerMultiprocessor</samp> and <samp class="ph codeph">maxThreadsPerBlock</samp> are specified, the compiler may increase register usage as high as <em class="ph i">L</em> to reduce the number of instructions and better hide single thread instruction latency.
                                       
                                    </li>
                                 </ul>
                              </li>
                           </ul>
                           <p class="p">A kernel will fail to launch if it is executed with more threads per block than its launch bound <samp class="ph codeph">maxThreadsPerBlock</samp>.
                              
                           </p>
                           <p class="p">Per thread resources required by a CUDA kernel might limit the maximum
                              block size in an unwanted way. In order to maintain forward compatibility to
                              future hardware and toolkits and to ensure that at least one thread block can
                              run on an SM, developers should include the single argument 
                              <samp class="ph codeph">__launch_bounds__(maxThreadsPerBlock)</samp> which specifies the largest block
                              size that the kernel will be launched with. Failure to do so could lead to 
                              "too many resources requested for launch" errors. Providing the two argument version 
                              of <samp class="ph codeph">__launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor)</samp> can
                              improve performance in some cases. The right value for <samp class="ph codeph">minBlocksPerMultiprocessor</samp>
                              should be determined using a detailed per kernel analysis.
                           </p>
                           <p class="p">Optimal launch bounds for a given kernel will usually differ across major architecture revisions.  The sample code below shows
                              how this is typically handled in device code using the <samp class="ph codeph">__CUDA_ARCH__</samp> macro introduced in <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a></p><pre xml:space="preserve">#define THREADS_PER_BLOCK          256
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#if __CUDA_ARCH__ &gt;= 200</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define MY_KERNEL_MAX_THREADS  (2 * THREADS_PER_BLOCK)</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define MY_KERNEL_MIN_BLOCKS   3</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define MY_KERNEL_MAX_THREADS  THREADS_PER_BLOCK</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define MY_KERNEL_MIN_BLOCKS   2</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>
__launch_bounds__(MY_KERNEL_MAX_THREADS, MY_KERNEL_MIN_BLOCKS)
MyKernel(...)
{
    ...
}</pre><p class="p">
                              In the common case where <samp class="ph codeph">MyKernel</samp> is invoked with the maximum number of threads per block (specified as the first parameter of <samp class="ph codeph">__launch_bounds__()</samp>), it is tempting to use <samp class="ph codeph">MY_KERNEL_MAX_THREADS</samp> as the number of threads per block in the execution configuration:
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>blocksPerGrid, MY_KERNEL_MAX_THREADS<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);</pre><p class="p">
                              This will not work however since <samp class="ph codeph">__CUDA_ARCH__</samp> is undefined in host code as mentioned in <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a>, so <samp class="ph codeph">MyKernel</samp> will launch with 256 threads per block even when <samp class="ph codeph">__CUDA_ARCH__</samp> is greater or equal to 200. Instead the number of threads per block should be determined:
                              
                           </p>
                           <ul class="ul">
                              <li class="li">
                                 Either at compile time using a macro that does not depend on <samp class="ph codeph">__CUDA_ARCH__</samp>, for example
                                 <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>blocksPerGrid, THREADS_PER_BLOCK<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);</pre></li>
                              <li class="li">Or at runtime based on the compute capability
                                 <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
cudaGetDeviceProperties(&amp;deviceProp, device);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> threadsPerBlock =
          (deviceProp.major &gt;= 2 ?
                    2 * THREADS_PER_BLOCK : THREADS_PER_BLOCK);
MyKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>blocksPerGrid, threadsPerBlock<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);</pre></li>
                           </ul>
                           <p class="p">
                              Register usage is reported by the <samp class="ph codeph">--ptxas options=-v</samp> compiler option. The number of resident blocks can be derived from the occupancy reported by the CUDA profiler (see <a class="xref" href="index.html#device-memory-accesses" shape="rect">Device Memory Accesses</a>for a definition of occupancy).
                              
                           </p>
                           <p class="p">
                              Register usage can also be controlled for all <samp class="ph codeph">__global__</samp> functions in a file using the <samp class="ph codeph">maxrregcount</samp> compiler option. The value of <samp class="ph codeph">maxrregcount</samp> is ignored for functions with launch bounds.
                              
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="pragma-unroll"><a name="pragma-unroll" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#pragma-unroll" name="pragma-unroll" shape="rect">B.35.&nbsp;#pragma unroll</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">
                              By default, the compiler unrolls small loops with a known trip count. The <samp class="ph codeph">#pragma
                                 unroll</samp> directive however can be used to control unrolling of any given loop. It
                              must be placed immediately before the loop and only applies to that loop. It is optionally
                              followed by an integral constant expression (ICE)<a name="fnsrc_13" href="#fntarg_13" shape="rect"><sup>13</sup></a>. If the ICE is absent, the loop will be
                              completely unrolled if its trip count is constant. If the ICE evaluates to 1, the compiler will
                              not unroll the loop. The pragma will be ignored if the ICE evaluates to a non-positive integer or
                              to an integer greater than the maximum value representable by the <samp class="ph codeph">int</samp> data type.
                              
                           </p>
                           <p class="p">Examples:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value = 4; };
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> X, typename T2&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *p1, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *p2) {

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// no argument specified, loop will be completely unrolled</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#pragma unroll</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 12; ++i) 
  p1[i] += p2[i]*2;
  
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// unroll value = 8</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#pragma unroll (X+1)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 12; ++i) 
  p1[i] += p2[i]*4;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// unroll value = 1, loop unrolling disabled</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#pragma unroll 1</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 12; ++i) 
  p1[i] += p2[i]*8;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// unroll value = 4</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#pragma unroll (T2::value)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 12; ++i) 
  p1[i] += p2[i]*16;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *p1, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *p2) {
foo&lt;7, S1_t&gt;(p1, p2);
}</pre></div>
                     </div>
                  </div>
                  <div class="topic reference nested1" xml:lang="en-US" id="simd-video"><a name="simd-video" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#simd-video" name="simd-video" shape="rect">B.36.&nbsp;SIMD Video Instructions</a></h3>
                     <div class="body refbody">
                        <div class="section refsyn">
                           <p class="p">
                              PTX ISA version 3.0 includes SIMD (Single Instruction, Multiple
                              Data) video instructions which operate on pairs of 16-bit
                              values and quads of 8-bit values.  These are available on
                              devices of compute capability 3.0.
                              
                           </p>
                           <div class="p">
                              The SIMD video instructions are:
                              
                              <ul class="ul">
                                 <li class="li">vadd2, vadd4 </li>
                                 <li class="li">vsub2, vsub4 </li>
                                 <li class="li">vavrg2, vavrg4 </li>
                                 <li class="li">vabsdiff2, vabsdiff4 </li>
                                 <li class="li">vmin2, vmin4 </li>
                                 <li class="li">vmax2, vmax4 </li>
                                 <li class="li">vset2, vset4 </li>
                              </ul>
                              
                              PTX instructions, such as the SIMD video instructions,
                              can be included in CUDA programs by way of the assembler,
                              <samp class="ph codeph">asm()</samp>, statement.
                              
                           </div>
                           <p class="p">The basic syntax of an asm() statement is:</p><pre xml:space="preserve">asm(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"template-string"</span> : <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"constraint"</span>(output) : <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"constraint"</span>(input)<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"));</span></pre><p class="p">An example of using the <samp class="ph codeph">vabsdiff4</samp> PTX instruction is:
                              
                           </p><pre xml:space="preserve">asm(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"vabsdiff4.u32.u32.u32.add"</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">" %0, %1, %2, %3;"</span>: <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"=r"</span> (result):<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"r"</span> (A), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"r"</span> (B), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"r"</span> (C));</pre><p class="p">
                              This uses the <samp class="ph codeph">vabsdiff4</samp> instruction to compute
                              an integer quad byte SIMD sum of absolute differences.    The
                              absolute difference value is computed for each byte of the
                              unsigned integers A and B in SIMD fashion.  The optional
                              accumulate operation (<samp class="ph codeph">.add</samp>) is specified to
                              sum these differences.
                              
                           </p>
                           <p class="p">
                              Refer to the document "Using Inline PTX Assembly in CUDA"
                              for details on using the assembly statement in your code.
                              Refer to the PTX ISA documentation ("Parallel Thread
                              Execution ISA Version 3.0" for example) for details on the
                              PTX instructions for the version of PTX that you are using.
                              
                           </p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="cooperative-groups"><a name="cooperative-groups" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#cooperative-groups" name="cooperative-groups" shape="rect">C.&nbsp;Cooperative Groups</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="introduction-cg"><a name="introduction-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#introduction-cg" name="introduction-cg" shape="rect">C.1.&nbsp;Introduction</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           	Cooperative Groups is an extension to the CUDA programming model,
                           	introduced in CUDA 9, for organizing groups of communicating threads.
                           Cooperative Groups allows developers to express the granularity at
                           which threads are communicating, helping them to express richer,
                           more efficient parallel decompositions.
                           
                        </p>
                        <p class="p">
                           	Historically, the CUDA programming model has provided a single, simple
                           	construct for synchronizing cooperating threads: a barrier across all
                           	threads of a thread block, as implemented with the
                           	<samp class="ph codeph">__syncthreads()</samp> intrinsic function. However,
                           	programmers would like to define and synchronize groups of threads at
                           	other granularities to enable greater performance, design flexibility,
                           	and software reuse in the form of collective group-wide function
                           	interfaces. In an effort to express broader patterns of parallel
                           	interaction, many performance-oriented programmers have resorted to
                           	writing their own ad hoc and unsafe primitives for synchronizing
                           	threads within a single warp, or across sets of thread blocks running
                           	on a single GPU. Whilst the performance improvements achieved have
                           	often been valuable, this has resulted in an ever-growing collection
                           	of brittle code that is expensive to write, tune, and maintain over
                           	time and across GPU generations. Cooperative Groups addresses this by
                           	providing a safe and future-proof mechanism to enable performant code.
                           
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="whats-new-cg"><a name="whats-new-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#whats-new-cg" name="whats-new-cg" shape="rect">C.2.&nbsp;What's New in CUDA 11.0</a></h3>
                     <div class="body conbody">
                        <ul class="ul">
                           <li class="li">
                              Separate compilation is no longer required to use the grid-scoped group
                              and synchronizing this group is now up to <strong class="ph b">30% faster</strong>.
                              Additionally we've enabled cooperative launches on latest Windows platforms,
                              and added support for them when running under MPS.
                              
                           </li>
                           <li class="li"><samp class="ph codeph">grid_group</samp> is now convertible to <samp class="ph codeph">thread_group</samp>.
                              
                           </li>
                           <li class="li">
                              New collectives for thread block tiles and coalesced groups:
                              <samp class="ph codeph">reduce</samp> and <samp class="ph codeph">memcpy_async</samp>.
                              
                           </li>
                           <li class="li">
                              New partition operations for thread block tiles and coalesced groups:
                              <samp class="ph codeph">labeled_partition</samp> and <samp class="ph codeph">binary_partition</samp>.
                              
                           </li>
                           <li class="li">
                              New APIs, <samp class="ph codeph">meta_group_rank</samp> and <samp class="ph codeph">meta_group_size</samp>
                              which provide information about the partitioning that led to the creation of this group.
                              
                           </li>
                           <li class="li">
                              Thread block tiles can now have their parent encoded in the type,
                              which allows for better compile-time optimization of emitted code.
                              
                           </li>
                           <li class="li">
                              Interface change: <samp class="ph codeph">grid_group</samp> must be constructed with
                              <samp class="ph codeph">this_grid()</samp> at declaration time. The default constructor is removed.
                              
                           </li>
                        </ul>
                        <p class="p">Notice: In this release, we are moving towards requiring C++11 for the new features.
                           This will be <strong class="ph b">required</strong> for all existing APIs in a future release.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="concept-cg"><a name="concept-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#concept-cg" name="concept-cg" shape="rect">C.3.&nbsp;Programming Model Concept</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           	The Cooperative Groups programming model describes
                           	synchronization patterns both within and across CUDA thread blocks. It
                           	provides both the means for applications to define their own groups of
                           	threads, and the interfaces to synchronize them. It also provides new
                           	launch APIs that enforce certain restrictions and therefore can guarantee
                           	the synchronization will work. These primitives enable new patterns of
                           	cooperative parallelism within CUDA, including producer-consumer
                           	parallelism, opportunistic parallelism, and global synchronization
                           	across the entire Grid.
                           
                        </p>
                        <div class="p">
                           	The Cooperative Groups programming model consists of the following elements:
                           	
                           <ul class="ul">
                              <li class="li">Data types for representing groups of cooperating threads; </li>
                              <li class="li">Operations to obtain implicit groups defined by the CUDA launch API (e.g., thread blocks); </li>
                              <li class="li">Collectives for partitioning existing groups into new groups; </li>
                              <li class="li">Collective Algorithms for data movement and manipulation (e.g. memcpy_async, reduce); </li>
                              <li class="li">An operation to synchronize all threads within the group; </li>
                              <li class="li">Operations to inspect the group properties; </li>
                              <li class="li">Collectives that expose low-level, group-specific and often HW accelerated, operations. </li>
                           </ul>
                        </div>
                        <p class="p">
                           The main concept in Cooperative Groups is that of objects naming the
                           set of threads that are part of it. This expression of groups as first-class
                           program objects improves software composition, since collective functions can
                           receive an explicit object representing the group of participating threads. This
                           object also makes programmer intent explicit, which eliminates unsound
                           architectural assumptions that result in brittle code, undesirable
                           restrictions upon compiler optimizations, and better compatibility with
                           new GPU generations.
                           
                        </p>
                        <p class="p">
                           To write efficient code, its best to use specialized groups (going generic loses
                           a lot of compile time optimizations), and pass these group objects by
                           reference to functions that intend to use these threads in some cooperative fashion.
                           
                        </p>
                        <p class="p">
                           Cooperative Groups requires CUDA 9.0 or later. To use Cooperative
                           Groups, include the header file:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Primary header is compatible with pre-C++11, collective algorithm headers require C++11</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups.h&gt;</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Optionally include for memcpy_async() collective</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups/memcpy_async.h&gt;</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Optionally include for reduce() collective</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cooperative_groups/reduce.h&gt;</span>
</pre><p class="p">
                           and use the Cooperative Groups namespace:
                           
                        </p><pre xml:space="preserve">
using namespace cooperative_groups;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Alternatively use an alias to avoid polluting the namespace with collective algorithms</span>
namespace cg = cooperative_groups;
</pre><p class="p">
                           The code can be compiled in a normal way using nvcc,
                           however if you wish to use memcpy_async or reduce functionality
                           and your host compiler's default dialect is not C++11 or higher,
                           then you must add <samp class="ph codeph">--std=c++11</samp> to the command line.
                           
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="composition-cg"><a name="composition-cg" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#composition-cg" name="composition-cg" shape="rect">C.3.1.&nbsp;Composition Example</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              To illustrate the concept of groups, this example attempts to perform a block-wide sum reduction.
                              Previously, there were hidden constraints on the implementation when
                              writing this code:
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> sum(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> n) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
    __syncthreads();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> total;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> parallel_kernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> *x) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Entire thread block must call sum</span>
    sum(x, n);
}
</pre><p class="p">
                              All threads in the thread block must arrive at the
                              <samp class="ph codeph">__syncthreads()</samp> barrier, however, this constraint is
                              hidden from the developer who might want to use
                              <samp class="ph codeph">sum()</samp>. With Cooperative Groups, a better way of
                              writing this would be:
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> sum(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> thread_block&amp; g, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *x, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> n) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
    g.sync()
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> total;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> parallel_kernel(...) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Entire thread block must call sum</span>
    thread_block tb = this_thread_block();
    sum(tb, x, n);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
}</pre></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="group-types-cg"><a name="group-types-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#group-types-cg" name="group-types-cg" shape="rect">C.4.&nbsp;Group Types</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="group-types-implicit-cg"><a name="group-types-implicit-cg" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#group-types-implicit-cg" name="group-types-implicit-cg" shape="rect">C.4.1.&nbsp;Implicit Groups</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              Implicit groups represent the launch configuration of the kernel.
                              Regardless of how your kernel is written, it always has a set number
                              of threads, blocks and block dimensions, a single grid and grid dimensions.
                              In addition, if the multi-device cooperative launch API is used, it can
                              have multiple grids (single grid per device). These groups provide a starting
                              point for decomposition into finer grained groups which are typically
                              HW accelerated and are more specialzied for the problem the developer is solving.
                              
                           </p>
                           <p class="p">
                              Although you can create an implicit group anywhere in the code, it is dangerous to do so.
                              Creating a handle for an implicit group is a collective operation - all threads in the group
                              must participate. If the group was created in a conditional branch that not all threads reach,
                              this can lead to deadlocks or data corruption. For this reason, it is recommended that you
                              create a handle for the implicit group upfront (as early as possible,
                              before any branching has occured) and use that handle throughout the kernel.
                              Group handles must be initialized at declaration time (there is no default
                              constructor) for the same reason and copy-constructing them is discouraged.
                              
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="thread-block-group-cg"><a name="thread-block-group-cg" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#thread-block-group-cg" name="thread-block-group-cg" shape="rect">C.4.1.1.&nbsp;Thread Block Group</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 Any CUDA programmer is already familiar with a certain group of
                                 threads: the thread block. The Cooperative Groups extension introduces
                                 a new datatype, <samp class="ph codeph">thread_block</samp>, to explicitly represent
                                 this concept within the kernel.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">class thread_block;</samp></p>
                              <p class="p">Constructed via:</p><pre xml:space="preserve">
thread_block g = this_thread_block();
</pre><p class="p"><strong class="ph b">Public Member Functions:</strong></p>
                              <p class="p"><samp class="ph codeph">static void sync()</samp>: Synchronize the threads named in the group
                              </p>
                              <p class="p"><samp class="ph codeph">static unsigned long long size()</samp>: Total number of threads in the group
                              </p>
                              <p class="p"><samp class="ph codeph">static unsigned long long thread_rank()</samp>: Rank of the calling thread within [0, size]
                              </p>
                              <p class="p"><samp class="ph codeph">static dim3 group_index()</samp>: 3-Dimensional index of the block within the launched grid
                              </p>
                              <p class="p"><samp class="ph codeph">static dim3 thread_index()</samp>: 3-Dimensional index of the thread within the launched block
                              </p>
                              <p class="p"><samp class="ph codeph">static dim3 group_dim()</samp>: Dimensions of the launched block
                              </p>
                              <div class="p"><strong class="ph b">Example:</strong><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// Loading an integer from global into shared memory</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *globalInput) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x;
    thread_block g = this_thread_block();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Choose a leader in the thread block</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (g.thread_rank() == 0) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// load from global into shared for all threads to work with</span>
        x = (*globalInput);
    }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// After loading data into shared memory, you want to synchronize</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// if all threads in your thread block need to see it</span>
    g.sync(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// equivalent to __syncthreads();</span>
}
</pre></div>
                              <p class="p"><strong class="ph b">Note:</strong> that all threads in the group must participate in collective operations, or the behavior is undefined.
                              </p>
                              <p class="p"><strong class="ph b">Related:</strong>
                                 The <samp class="ph codeph">thread_block</samp> datatype is derived from the more
                                 generic <samp class="ph codeph">thread_group</samp> datatype, which can be used to
                                 represent a wider class of groups.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="grid-group-cg"><a name="grid-group-cg" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#grid-group-cg" name="grid-group-cg" shape="rect">C.4.1.2.&nbsp;Grid Group</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 This group object represents all the threads launched in a single grid.
                                 APIs other than <samp class="ph codeph">sync()</samp> are available at all times,
                                 but to be able to synchronize across the grid, you need to use the
                                 cooperative launch API.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">class grid_group;</samp></p>
                              <p class="p">Constructed via:</p><pre xml:space="preserve">
grid_group g = this_grid();
</pre><p class="p"><strong class="ph b">Public Member Functions:</strong></p>
                              <p class="p"><samp class="ph codeph">bool is_valid() const</samp>: Returns whether the grid_group can synchronize
                              </p>
                              <p class="p"><samp class="ph codeph">void sync() const</samp>: Synchronize the threads named in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long size() const</samp>: Total number of threads in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long thread_rank() const</samp>: Rank of the calling thread within [0, size]
                              </p>
                              <p class="p"><samp class="ph codeph">dim3 group_dim() const</samp>: Dimensions of the launched grid
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="multi-grid-group-cg"><a name="multi-grid-group-cg" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#multi-grid-group-cg" name="multi-grid-group-cg" shape="rect">C.4.1.3.&nbsp;Multi Grid Group</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 This group object represents all the threads launched across all devices of
                                 a multi-device cooperative launch. Unlike the grid.group, all the APIs require
                                 that you have used the appropriate launch API.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">class multi_grid_group;</samp></p>
                              <p class="p">Constructed via:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel must be launched with the cooperative multi-device API</span>
multi_grid_group g = this_multi_grid();
</pre><p class="p"><strong class="ph b">Public Member Functions:</strong></p>
                              <p class="p"><samp class="ph codeph">bool is_valid() const</samp>: Returns whether the multi_grid_group can be used
                              </p>
                              <p class="p"><samp class="ph codeph">void sync() const</samp>: Synchronize the threads named in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long size() const</samp>: Total number of threads in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long thread_rank() const</samp>: Rank of the calling thread within [0, size]
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned int grid_rank() const</samp>: Rank of the grid within [0,num_grids]
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned int num_grids() const</samp>: Total number of grids launched
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="group-types-explicit-cg"><a name="group-types-explicit-cg" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#group-types-explicit-cg" name="group-types-explicit-cg" shape="rect">C.4.2.&nbsp;Explicit Groups</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="thread-block-tile-group-cg"><a name="thread-block-tile-group-cg" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#thread-block-tile-group-cg" name="thread-block-tile-group-cg" shape="rect">C.4.2.1.&nbsp;Thread Block Tile</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 An templated version of a tiled group, where a template parameter is used to specify
                                 the size of the tile - with this known at compile time there is the potential for more optimal
                                 execution.
                                 
                              </p>
                              <div class="p"><pre xml:space="preserve">
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Size, typename ParentT = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>&gt;
class thread_block_tile;
</pre></div>
                              <p class="p">Constructed via:</p><pre xml:space="preserve">
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Size, typename ParentT&gt;
_CG_QUALIFIER thread_block_tile&lt;Size, ParentT&gt; tiled_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> ParentT&amp; g)
</pre><p class="p"><samp class="ph codeph">Size</samp> must be a power of 2 and less than or equal to 32.
                              </p>
                              <p class="p"><samp class="ph codeph">ParentT</samp> is the parent-type from which this group was partitioned.
                                 It is automatically inferred, but a value of void will store this information in the
                                 group handle rather than in the type.
                                 
                              </p>
                              <p class="p"><strong class="ph b">Public Member Functions:</strong></p>
                              <p class="p"><samp class="ph codeph">void sync() const</samp>: Synchronize the threads named in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long size() const</samp>: Total number of threads in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long thread_rank() const</samp>: Rank of the calling thread within [0, size]
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long meta_group_size() const</samp>:
                                 Returns the number of groups created when the parent group was partitioned.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long meta_group_rank() const</samp>:
                                 Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size)
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">T shfl(T var, unsigned int src_rank) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T shfl_up(T var, int delta) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T shfl_down(T var, int delta) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T shfl_xor(T var, int delta) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T any(int predicate) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              <p class="p"><samp class="ph codeph">T all(int predicate) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              <p class="p"><samp class="ph codeph">T ballot(int predicate) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              <p class="p"><samp class="ph codeph">T match_any(T val) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-match-functions" shape="rect">Warp Match Functions</a></p>
                              <p class="p"><samp class="ph codeph">T match_all(T val, int &amp;pred) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-match-functions" shape="rect">Warp Match Functions</a></p>
                              <p class="p"><strong class="ph b">Notes:</strong></p>
                              <div class="p"><samp class="ph codeph">shfl, shfl_up, shfl_down, and shfl_xor</samp> functions accept objects of any type when compiled with C++11 or later. This means it's possible to shuffle non-integral
                                 types as long as they satisfy the below constraints:
                                 
                                 <ul class="ul">
                                    <li class="li">Qualifies as trivially copyable i.e., <samp class="ph codeph">is_trivially_copyable&lt;T&gt;::value == true</samp></li>
                                    <li class="li"><samp class="ph codeph">sizeof(T) &lt;= 32</samp></li>
                                 </ul>
                              </div>
                              <div class="p"><strong class="ph b">Example:</strong><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// The following code will create two sets of tiled groups, of size 32 and 4 respectively:</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// The latter has the provenance encoded in the type, while the first stores it in the handle</span>
thread_block block = this_thread_block();
thread_block_tile&lt;32&gt; tile32 = tiled_partition&lt;32&gt;(block);
thread_block_tile&lt;4, thread_block&gt; tile4 = tiled_partition&lt;4&gt;(block);
</pre></div>
                              <p class="p">
                                 Note: that the <samp class="ph codeph">thread_block_tile</samp> templated data
                                 structure is being used here, and that the size of the group is passed
                                 to the <samp class="ph codeph">tiled_partition</samp> call as a template parameter
                                 rather than an argument.
                                 
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="ws-code-pattern-cg"><a name="ws-code-pattern-cg" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#ws-code-pattern-cg" name="ws-code-pattern-cg" shape="rect">C.4.2.1.1.&nbsp;Warp-Synchronous Code Pattern</a></h3>
                              <div class="body conbody">
                                 <p class="p">
                                    Developers might have had warp-synchronous codes that they previously
                                    made implicit assumptions about the warp size and would code around
                                    that number. Now this needs to be specified explicitly.
                                    
                                 </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> cooperative_kernel(...) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// obtain default "current thread block" group</span>
    thread_block my_block = this_thread_block();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// subdivide into 32-thread, tiled subgroups</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Tiled subgroups evenly partition a parent group into</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// adjacent sets of threads - in this case each one warp in size</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> my_tile = tiled_partition&lt;32&gt;(my_block);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This operation will be performed by only the</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// first 32-thread tile of each block</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (my_tile.meta_group_rank() == 0) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
        my_tile.sync();
    }
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="thb-tiles-bigger-than-32-cg"><a name="thb-tiles-bigger-than-32-cg" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#thb-tiles-bigger-than-32-cg" name="thb-tiles-bigger-than-32-cg" shape="rect">C.4.2.1.2.&nbsp;Thread Block Tile of size larger than 32</a></h3>
                              <div class="body conbody">
                                 <p class="p">
                                    It is possible to obtain <samp class="ph codeph">thread_block_tile</samp> of size 64, 128, 256 or 512 using new API present
                                    in <samp class="ph codeph">cooperative_groups::experimental</samp> namespace. To use it, <samp class="ph codeph">_CG_ABI_EXPERIMENTAL</samp>
                                    has to be defined in the source code. Before partitioning, a small amount of memory has to be reserved for
                                    <samp class="ph codeph">thread_block_tile</samp> usage. This can be done using <samp class="ph codeph">cooperative_groups::experimental::block_tile_memory</samp>
                                    struct template that has to reside in either shared or global memory.
                                    
                                 </p><pre xml:space="preserve">
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> TileCommunicationSize = 8, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> MaxBlockSize = 1024&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> block_tile_memory;
</pre><p class="p"><samp class="ph codeph">TileCommunicationSize</samp> Determines how much memory is reserved for collective operations. If such
                                    operation is performed on type of size larger than specified communication size, the collective may involve multiple
                                    transfers and take longer to complete.
                                 </p>
                                 <p class="p"><samp class="ph codeph">MaxBlockSize</samp> Specifies the maximal number of threads in the current thread block. This parameter
                                    can be used to minimize the shared memory usage of <samp class="ph codeph">block_tile_memory</samp> in kernels launched only with
                                    smaller thread counts.
                                 </p>
                                 <p class="p">
                                    This <samp class="ph codeph">block_tile_memory</samp> needs be then passed into
                                    <samp class="ph codeph">cooperative_groups::experimental::this_thread_block</samp>, allowing the resulting <samp class="ph codeph">thread_block</samp> to
                                    be partitioned into tiles of sizes larger than 32. Overload of <samp class="ph codeph">this_thread_block</samp> accepting
                                    <samp class="ph codeph">block_tile_memory</samp> argument is a collective operation and has to be called with all threads
                                    in the <samp class="ph codeph">thread_block</samp>. Returned <samp class="ph codeph">thread_block</samp> can be partitioned using
                                    <samp class="ph codeph">experimental::tiled_partition</samp> function template, which accepts the same arguments as the
                                    regular <samp class="ph codeph">tiled_partition</samp>.
                                    
                                 </p><pre xml:space="preserve">
#define _CG_ABI_EXPERIMENTAL <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// enable experimental API</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> cooperative_kernel(...) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// reserve shared memory for thread_block_tile usage.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> experimental::block_tile_memory&lt;4, 256&gt; shared;
    thread_block thb = experimental::this_thread_block(shared);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> tile = experimental::tiled_partition&lt;128&gt;(thb);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...</span>
}
</pre><p class="p"><strong class="ph b">Public Member Functions:</strong></p>
                                 <p class="p"><samp class="ph codeph">void sync() const</samp>: Synchronize the threads named in the group
                                 </p>
                                 <p class="p"><samp class="ph codeph">unsigned long long size() const</samp>: Total number of threads in the group
                                 </p>
                                 <p class="p"><samp class="ph codeph">unsigned long long thread_rank() const</samp>: Rank of the calling thread within [0, size]
                                 </p>
                                 <p class="p"><samp class="ph codeph">unsigned long long meta_group_size() const</samp>:
                                    Returns the number of groups created when the parent group was partitioned.
                                    
                                 </p>
                                 <p class="p"><samp class="ph codeph">unsigned long long meta_group_rank() const</samp>:
                                    Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size)
                                    
                                 </p>
                                 <p class="p"><samp class="ph codeph">T shfl(T var, unsigned int src_rank) const</samp>:
                                    Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a>, <strong class="ph b">Note: All threads in the group have to specify the same src_rank,
                                       otherwise the behavior is undefined.</strong></p>
                                 <p class="p"><samp class="ph codeph">T any(int predicate) const</samp>:
                                    Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                                 <p class="p"><samp class="ph codeph">T all(int predicate) const</samp>:
                                    Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="coalesced-group-cg"><a name="coalesced-group-cg" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#coalesced-group-cg" name="coalesced-group-cg" shape="rect">C.4.2.2.&nbsp;Coalesced Groups</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 In CUDAs SIMT architecture, at the hardware level the multiprocessor
                                 executes threads in groups of 32 called warps. If there exists a
                                 data-dependent conditional branch in the application code such that
                                 threads within a warp diverge, then the warp serially executes each
                                 branch disabling threads not on that path. The threads that remain
                                 active on the path are referred to as coalesced. Cooperative Groups
                                 has functionality to discover, and create, a group containing all
                                 coalesced threads.
                                 
                              </p>
                              <p class="p">
                                 Constructing the group handle via <samp class="ph codeph">coalesced_threads()</samp> is opportunistic.
                                 It returns the set of active threads at that point in time, and makes no
                                 guarantee about which threads are returned (as long as they are active) or that
                                 they will stay coalesced throughout execution (they will be brought back together
                                 for the execution of a collective but can diverge again afterwards).
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">class coalesced_group;</samp></p>
                              <p class="p">Constructed via:</p><pre xml:space="preserve">
coalesced_group active = coalesced_threads();
</pre><p class="p"><strong class="ph b">Public Member Functions:</strong></p>
                              <p class="p"><samp class="ph codeph">void sync() const</samp>: Synchronize the threads named in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long size() const</samp>: Total number of threads in the group
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long thread_rank() const</samp>: Rank of the calling thread within [0, size]
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long meta_group_size() const</samp>:
                                 Returns the number of groups created when the parent group was partitioned.
                                 If this group was created by querying the set of active threads, e.g. <samp class="ph codeph">coalesced_threads()</samp>
                                 the value of <samp class="ph codeph">meta_group_size()</samp> will be 1.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">unsigned long long meta_group_rank() const</samp>:
                                 Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size).
                                 If this group was created by querying the set of active threads, e.g. <samp class="ph codeph">coalesced_threads()</samp>
                                 the value of <samp class="ph codeph">meta_group_rank()</samp> will always be 0.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">T shfl(T var, unsigned int src_rank) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T shfl_up(T var, int delta) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T shfl_down(T var, int delta) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a></p>
                              <p class="p"><samp class="ph codeph">T any(int predicate) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              <p class="p"><samp class="ph codeph">T all(int predicate) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              <p class="p"><samp class="ph codeph">T ballot(int predicate) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a></p>
                              <p class="p"><samp class="ph codeph">T match_any(T val) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-match-functions" shape="rect">Warp Match Functions</a></p>
                              <p class="p"><samp class="ph codeph">T match_all(T val, int &amp;pred) const</samp>:
                                 Refer to <a class="xref" href="index.html#warp-match-functions" shape="rect">Warp Match Functions</a></p>
                              <p class="p"><strong class="ph b">Notes:</strong></p>
                              <div class="p"><samp class="ph codeph">shfl, shfl_up, and shfl_down</samp> functions accept objects of any type when compiled with C++11 or later. This means it's possible to shuffle non-integral
                                 types as long as they satisfy the below constraints:
                                 
                                 <ul class="ul">
                                    <li class="li">Qualifies as trivially copyable i.e. <samp class="ph codeph">is_trivially_copyable&lt;T&gt;::value == true</samp></li>
                                    <li class="li"><samp class="ph codeph">sizeof(T) &lt;= 32</samp></li>
                                 </ul>
                              </div>
                              <div class="p"><strong class="ph b">Example:</strong><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// Consider a situation whereby there is a branch in the</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// code in which only the 2nd, 4th and 8th threads in each warp are</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// active. The coalesced_threads() call, placed in that branch, will create (for each</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// warp) a group, active, that has three threads (with</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// ranks 0-2 inclusive).</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *globalInput) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Lets say globalInput says that threads 2, 4, 8 should handle the data</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == *globalInput) {
        coalesced_group active = coalesced_threads();
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// active contains 0-2 inclusive</span>
        active.sync();
    }
}
</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="discovery-pattern-cg"><a name="discovery-pattern-cg" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#discovery-pattern-cg" name="discovery-pattern-cg" shape="rect">C.4.2.2.1.&nbsp;Discovery Pattern</a></h3>
                              <div class="body conbody">
                                 <p class="p">
                                    Commonly developers need to work with the current active set of
                                    threads. No assumption is made about the threads that are present, and
                                    instead developers work with the threads that happen to be there. This
                                    is seen in the following aggregating atomic increment across threads
                                    in a warp example (written using the correct CUDA 9.0 set of
                                    intrinsics):
                                    
                                 </p><pre xml:space="preserve">
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> writemask = __activemask();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> total = __popc(writemask);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> prefix = __popc(writemask &amp; __lanemask_lt());
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Find the lowest-numbered active lane</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> elected_lane = __ffs(writemask) - 1;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> base_offset = 0;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (prefix == 0) {
        base_offset = atomicAdd(p, total);
    }
    base_offset = __shfl_sync(writemask, base_offset, elected_lane);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> thread_offset = prefix + base_offset;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> thread_offset;
}
</pre><p class="p">
                                    This can be re-written with Cooperative Groups as follows:
                                    
                                 </p><pre xml:space="preserve">
{
    cg::coalesced_group g = cg::coalesced_threads();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> prev;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (g.thread_rank() == 0) {
        prev = atomicAdd(p, g.size());
    }
    prev = g.thread_rank() + g.shfl(prev, 0);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> prev;
}
</pre></div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="partitions-cg"><a name="partitions-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#partitions-cg" name="partitions-cg" shape="rect">C.5.&nbsp;Group Partitioning</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="partitions-cg-tiled"><a name="partitions-cg-tiled" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#partitions-cg-tiled" name="partitions-cg-tiled" shape="rect">C.5.1.&nbsp;<samp class="ph codeph">tiled_partition</samp></a></h3>
                        <div class="body conbody"><pre xml:space="preserve">
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Size, typename ParentT&gt;
thread_block_tile&lt;Size, ParentT&gt; tiled_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> ParentT&amp; g);
</pre><pre xml:space="preserve">
thread_group tiled_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> thread_group&amp; parent, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> tilesz);
</pre><p class="p">
                              The <samp class="ph codeph">tiled_partition</samp> method is a collective operation that
                              partitions the parent group into a one-dimensional, row-major, tiling of subgroups.
                              A total of ((size(parent)/tilesz) subgroups will be created,
                              therefore the parent group size must be evenly divisible by the <samp class="ph codeph">Size</samp>.
                              The allowed parent groups are <samp class="ph codeph">thread_block</samp> or <samp class="ph codeph">thread_block_tile</samp>.
                              
                           </p>
                           <p class="p">
                              The implementation may cause the calling thread to wait until all the members
                              of the parent group have invoked the operation before resuming execution.
                              Functionality is limited to native hardware sizes, 1/2/4/8/16/32 and the
                              <samp class="ph codeph">cg::size(parent)</samp> must be greater than the <samp class="ph codeph">Size</samp> parameter.
                              The experimental version in cooperative_groups::experimental namespace supports 64/128/256/512 sizes.
                              
                           </p>
                           <p class="p"><strong class="ph b">Codegen Requirements:</strong> Compute Capability 3.5 minimum, C++11 for sizes larger than 32
                           </p>
                           <p class="p"><strong class="ph b">Example:</strong></p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// The following code will create a 32-thread tile</span>
thread_block block = this_thread_block();
thread_block_tile&lt;32&gt; tile32 = tiled_partition&lt;32&gt;(block);
</pre><p class="p">
                              We can partition each of these groups into even
                              smaller groups, each of size 4 threads:
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> tile4 = tiled_partition&lt;4&gt;(tile32);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// or using a general group</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// thread_group tile4 = tiled_partition(tile32, 4);</span>
</pre><p class="p">
                              	If, for instance, if we were to then include the following line of code:
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (tile4.thread_rank()==0) printf(Hello from tile4 rank 0\n);
</pre><p class="p">
                              then the statement would be printed by every fourth thread in the
                              block: the threads of rank 0 in each <samp class="ph codeph">tile4</samp> group,
                              which correspond to those threads with ranks 0,4,8,12,etc. in the
                              <samp class="ph codeph">block</samp> group.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="partitions-cg-labeled"><a name="partitions-cg-labeled" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#partitions-cg-labeled" name="partitions-cg-labeled" shape="rect">C.5.2.&nbsp;<samp class="ph codeph">labeled_partition</samp></a></h3>
                        <div class="body conbody"><pre xml:space="preserve">
coalesced_group labeled_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> coalesced_group&amp; g, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> label);
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Size&gt;
</pre><pre xml:space="preserve">
coalesced_group labeled_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> thread_block_tile&lt;Size&gt;&amp; g, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> label);
</pre><p class="p">
                              The <samp class="ph codeph">labeled_partition</samp> method is a collective operation that
                              partitions the parent group into one-dimensional subgroups within which the
                              threads are coalesced. The implementation will evaluate a condition label
                              and assign threads that have the same value for label into the same group.
                              
                           </p>
                           <p class="p">
                              The implementation may cause the calling thread to wait until all the members
                              of the parent group have invoked the operation before resuming execution.
                              
                           </p>
                           <p class="p"><strong class="ph b">Note:</strong> This functionality is still being evaluated and may slightly change in the future.
                           </p>
                           <p class="p"><strong class="ph b">Codegen Requirements:</strong> Compute Capability 7.0 minimum, C++11
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="partitions-cg-binary"><a name="partitions-cg-binary" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#partitions-cg-binary" name="partitions-cg-binary" shape="rect">C.5.3.&nbsp;<samp class="ph codeph">binary_partition</samp></a></h3>
                        <div class="body conbody"><pre xml:space="preserve">
coalesced_group binary_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> coalesced_group&amp; g, bool pred);
template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Size&gt;
</pre><pre xml:space="preserve">
coalesced_group binary_partition(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> thread_block_tile&lt;Size&gt;&amp; g, bool pred);
</pre><p class="p">
                              The <samp class="ph codeph">binary_partition()</samp> method is a collective operation that
                              partitions the parent group into one-dimensional subgroups within which the
                              threads are coalesced. The implementation will evaluate a predicate
                              and assign threads that have the same value into the same group.
                              This is a specialized form of <samp class="ph codeph">labeled_partition()</samp>,
                              where the label can only be 0 or 1.
                              
                           </p>
                           <p class="p">
                              The implementation may cause the calling thread to wait until all the members
                              of the parent group have invoked the operation before resuming execution.
                              
                           </p>
                           <p class="p"><strong class="ph b">Note:</strong> This functionality is still being evaluated and may slightly change in the future.
                           </p>
                           <p class="p"><strong class="ph b">Codegen Requirements:</strong> Compute Capability 7.0 minimum, C++11
                           </p>
                           <p class="p"><strong class="ph b">Example:</strong></p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// This example divides a 32-sized tile into a group with odd</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// numbers and a group with even numbers</span>
_global__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> oddEven(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *inputArr) {
    cg::thread_block cta = cg::this_thread_block();
    cg::thread_block_tile&lt;32&gt; tile32 = cg::tiled_partition&lt;32&gt;(cta);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// inputArr contains random integers</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> elem = inputArr[cta.thread_rank()];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// after this, tile32 is split into 2 groups,</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// a subtile where elem&amp;1 is true and one where its false</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> subtile = cg::binary_partition(tile32, (elem &amp; 1));
}</pre></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="collectives-cg"><a name="collectives-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#collectives-cg" name="collectives-cg" shape="rect">C.6.&nbsp;Group Collectives</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="sync-collectives-cg"><a name="sync-collectives-cg" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#sync-collectives-cg" name="sync-collectives-cg" shape="rect">C.6.1.&nbsp;Synchronization</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="collectives-cg-sync"><a name="collectives-cg-sync" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#collectives-cg-sync" name="collectives-cg-sync" shape="rect">C.6.1.1.&nbsp;<samp class="ph codeph">sync</samp></a></h3>
                           <div class="body conbody">
                              <div class="p"><pre xml:space="preserve">cooperative_groups::sync(T&amp; group);</pre></div>
                              <p class="p"><samp class="ph codeph">sync</samp> synchronizes the threads named in the group.
                                 <samp class="ph codeph">T</samp> can be any of the existing group types, as all of them support synchronization.
                                 If the group is a <samp class="ph codeph">grid_group</samp> or a <samp class="ph codeph">multi_grid_group</samp> the kernel
                                 must have been launched using the appropriate cooperative launch APIs.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="data-transfer-collectives-cg"><a name="data-transfer-collectives-cg" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#data-transfer-collectives-cg" name="data-transfer-collectives-cg" shape="rect">C.6.2.&nbsp;Data Transfer</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="collectives-cg-memcpy-async"><a name="collectives-cg-memcpy-async" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#collectives-cg-memcpy-async" name="collectives-cg-memcpy-async" shape="rect">C.6.2.1.&nbsp;<samp class="ph codeph">memcpy_async</samp></a></h3>
                           <div class="body conbody">
                              <p class="p"><samp class="ph codeph">memcpy_async</samp> is a group-wide collective memcpy that utilizes hardware
                                 accelerated support for non-blocking memory transactions from global to shared memory.
                                 Given a set of threads named in the group, memcpy_async will move min(srcCount, dstCount)
                                 of elements of the input types through a single pipeline stage.  To achieve best
                                 performance, the source and destination inputs should be 16 byte aligned types.
                                 It is important to note that while this is a memcpy in the general case,
                                 it is only asynchronous if the source is global memory and the destination is shared
                                 memory and both can be addressed with 16, 8, or 4 byte alignments.
                                 Asynchronously copied data should only be read following a call to wait which signals
                                 that the corresponding stage has completed moving data to shared memory.
                                 
                              </p>
                              <p class="p">
                                 Having to wait on all outstanding requests can lose some flexibility (but gain simplicity).
                                 In order to efficiently overlap data transfer and execution, its important to be able to
                                 kick off an <strong class="ph b">N+1</strong><samp class="ph codeph">memcpy_async</samp> request while waiting on and operating
                                 on request <strong class="ph b">N</strong>. This is accomplished by relinquishing control over the async request
                                 to the pipeline object. To do so, pass the <samp class="ph codeph">
                                    cuda::pipeline</samp> object to <samp class="ph codeph">memcpy_async</samp> and wait on it
                                 using the collective stage-based <samp class="ph codeph">wait</samp> API. See
                                 <a class="xref" href="index.html#collectives-cg-wait" shape="rect">wait</a> for more details.
                                 
                              </p>
                              <p class="p"><strong class="ph b">Usage 1:</strong></p>
                              <div class="p"><pre xml:space="preserve">
template &lt;class TyGroup, typename TyElem, typename TySizeT&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> memcpy_async(
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> TyGroup &amp;group,
  TyElem *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> _dst,
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> TyElem *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> _src,
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> TySizeT &amp;count
);
</pre></div>
                              <p class="p"><strong class="ph b"><samp class="ph codeph">memcpy_async(group, destination, source, copy_shape)</samp></strong> performs a copy of
                                 <strong class="ph b">N bytes.</strong> If <samp class="ph codeph">copy_shape</samp> is of type <samp class="ph codeph">cuda::aligned_size_t&lt;N&gt;</samp>, alignment will
                                 be guaranteed to be at least <samp class="ph codeph">min(16, N)</samp></p>
                              <p class="p"><strong class="ph b">Usage 2:</strong></p>
                              <div class="p"><pre xml:space="preserve">
template &lt;class TyGroup, class TyElem, typename DstLayout, typename SrcLayout&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> memcpy_async(
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> TyGroup &amp;group,
  TyElem *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> dst,
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> DstLayout &amp;dstLayout,
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> TyElem *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__restrict__</span> src,
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> SrcLayout &amp;srcLayout
);
</pre></div>
                              <p class="p"><strong class="ph b"><samp class="ph codeph">memcpy_async(group, destination, copy_shape, source, copy_shape)</samp></strong> performs a copy of
                                 <strong class="ph b">N elements.</strong> If <samp class="ph codeph">copy_shape</samp> is of type <samp class="ph codeph">cuda::aligned_size_t&lt;N&gt;</samp>, alignment will
                                 be guaranteed to be at least <samp class="ph codeph">min(16, N)</samp>. <strong class="ph b">Input shapes must share the same alignment.</strong></p>
                              <p class="p"><strong class="ph b">Codegen Requirements:</strong> Compute Capability 3.5 minimum, Compute Capability 8.0 for asynchronicity, C++11
                              </p>
                              <p class="p"><strong class="ph b">Example:</strong></p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// This example streams elementsPerThreadBlock worth of data from global memory</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// into a limited sized shared memory (elementsInShared) block to operate on.</span>
cg::thread_block tb = cg::this_thread_block();
size_t index = 0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (index &lt; elementsPerThreadBlock) {
    size_t copyCount = cg::memcpy_async(tb, local_smem, elementsInShared, global_data + index, elementsPerThreadBlock - index);
    cg::wait(tb);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Work with local_smem</span>
    index += copyCount;
}
</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="collectives-cg-wait"><a name="collectives-cg-wait" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#collectives-cg-wait" name="collectives-cg-wait" shape="rect">C.6.2.2.&nbsp;<samp class="ph codeph">wait</samp></a></h3>
                           <div class="body conbody">
                              <div class="p"><pre xml:space="preserve">
template &lt;class TyGroup&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> wait(TyGroup &amp; group);
</pre></div>
                              <p class="p">
                                 The <samp class="ph codeph">wait</samp>collective synchronizes the named group of
                                 threads and blocks until all outstanding <samp class="ph codeph">memcpy_async</samp>
                                 requests have completed.
                                 
                              </p>
                              <p class="p"><strong class="ph b">Codegen Requirements:</strong> Compute Capability 3.5 minimum, Compute Capability 8.0 for asynchronicity, C++11
                              </p>
                              <p class="p"><strong class="ph b">Example:</strong></p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// This example streams elementsPerThreadBlock worth of data from global memory</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// into a limited sized shared memory (elementsInShared) block to operate on in</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// multiple (two) stages. As stage N is kicked off, we can wait on and operate on stage N-1.</span>
cg::thread_block tb = cg::this_thread_block();
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> stage = 0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// First kick off an extra request</span>
size_t index = cg::memcpy_async(tb, smem_ptr[stage], elementsInShared, global_data, elementsPerThreadBlock - index);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (index &lt; elementsPerThreadBlock) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Now we kick off the next request...</span>
    size_t copyCount = cg::memcpy_async(tb, smem_ptr[stage ^ 1], elementsInShared, global_data + index, elementsPerThreadBlock - index);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ... but we wait on the one before it</span>
    cg::wait_prior&lt;1&gt;(tb);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Its now available and we can work with smem_ptr[stage] here</span>
    index += copyCount;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// A cg::sync(tb) might be needed here depending on whether</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the work done with smem_ptr[stage] can release threads to race ahead or not</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Wrap to the next stage</span>
    stage ^= 1;
}
cg::wait(tb);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The last smem_ptr[stage] can be handled here</span>
</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="reduce-collectives-cg"><a name="reduce-collectives-cg" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#reduce-collectives-cg" name="reduce-collectives-cg" shape="rect">C.6.3.&nbsp;Data manipulation</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="collectives-cg-reduce"><a name="collectives-cg-reduce" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#collectives-cg-reduce" name="collectives-cg-reduce" shape="rect">C.6.3.1.&nbsp;<samp class="ph codeph">reduce</samp></a></h3>
                           <div class="body conbody">
                              <div class="p"><pre xml:space="preserve">
template &lt;typename TyArg, typename TyOp, typename TyGroup&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> reduce(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> TyGroup&amp; group, TyArg&amp;&amp; val, TyOp&amp;&amp; op) -&gt; decltype(op(val, val));</pre></div>
                              <p class="p"><samp class="ph codeph">reduce</samp> performs a reduction operation on the data provided by each
                                 thread named in the group passed in. This takes advantage of hardware acceleration (on compute 80 and higher
                                 devices) for the arithmetic add, min, or max operations and the logical AND, OR, or XOR,
                                 as well as providing a software fallback on older generation hardware. Only 4B types are accelerated by hardware.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">group</samp>: Valid group types are <samp class="ph codeph">coalesced_group</samp>
                                 and <samp class="ph codeph">thread_block_tile</samp>.
                                 
                              </p>
                              <div class="p"><samp class="ph codeph">val</samp>: Any type that satisfies the below requirements:
                                 
                                 <ul class="ul">
                                    <li class="li">Qualifies as trivially copyable i.e. <samp class="ph codeph">is_trivially_copyable&lt;TyArg&gt;::value == true</samp></li>
                                    <li class="li"><samp class="ph codeph">sizeof(TyArg) &lt;= 32</samp></li>
                                    <li class="li">Has suitable arithmetic or comparative operators for the given function object.</li>
                                 </ul>
                              </div>
                              <p class="p"><samp class="ph codeph">op</samp>: Valid function objects that will provide hardware acceleration with integral types are <samp class="ph codeph">plus(), less(), greater(), bit_and(), bit_xor(), bit_or()</samp>.
                                 These must be constructed, hence the TyVal template argument is required, i.e. <samp class="ph codeph">plus&lt;int&gt;()</samp>.
                                 Reduce also supports lambdas and other function objects that can be invoked using <samp class="ph codeph">operator()</samp></p>
                              <p class="p"><strong class="ph b">Codegen Requirements:</strong> Compute Capability 3.5 minimum, Compute Capability 8.0 for HW acceleration, C++11.
                              </p>
                              <p class="p"><strong class="ph b">Example:</strong></p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// The following example accepts input in *A and outputs a result into *sum</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// It spreads the data within the block, one element per thread</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define blocksz 256</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> block_reduce(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *sum) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> reduction_s[blocksz];

    cg::thread_block cta = cg::this_thread_block();
    cg::thread_block_tile&lt;32&gt; tile = cg::tiled_partition&lt;32&gt;(cta);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> tid = cta.thread_rank();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> beta = A[tid];
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// reduce across the tile</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cg::plus&lt;int&gt; allows cg::reduce() to know it can use hardware acceleration for addition</span>
    reduction_s[tid] = cg::reduce(tile, beta, cg::plus&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;());
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// synchronize the block so all data is ready</span>
    cg::sync(cta);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// single leader accumulates the result</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cta.thread_rank() == 0) {
        beta = 0;
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; blocksz; i += tile.size()) {
            beta += reduction_s[i];
        }
    }
    sum[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x] = beta;
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="collectives-cg-reduce-operators"><a name="collectives-cg-reduce-operators" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#collectives-cg-reduce-operators" name="collectives-cg-reduce-operators" shape="rect">C.6.3.2.&nbsp;<samp class="ph codeph">Reduce Operators</samp></a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 Below are the prototypes of function objects for some of the basic operations that can be done with <samp class="ph codeph">reduce</samp></p>
                              <div class="p"><pre xml:space="preserve">
namespace cooperative_groups {
  template &lt;typename Ty&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cg::plus;

  template &lt;typename Ty&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cg::less;

  template &lt;typename Ty&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cg::greater;

  template &lt;typename Ty&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cg::bit_and;

  template &lt;typename Ty&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cg::bit_xor;

  template &lt;typename Ty&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> cg::bit_or;
}</pre></div>
                              <p class="p">
                                 Reduce is limited to the information available to the implementation at compile time. Thus in order to make use of intrinsics
                                 introduced in CC 8.0,
                                 the <samp class="ph codeph">cg::</samp> namespace exposes several functional objects that mirror the hardware. These objects appear similar to those presented in
                                 the C++ STL,
                                 with the exception of <samp class="ph codeph">less/greater</samp>. The reason for any difference from the STL is that these function objects are designed
                                 to actually mirror the operation of the hardware intrinsics.
                                 
                              </p>
                              <div class="p"><strong class="ph b">Functional description:</strong><ul class="ul">
                                    <li class="li"><samp class="ph codeph">cg::plus:</samp> Accepts two values and returns the sum of both using operator+.
                                    </li>
                                    <li class="li"><samp class="ph codeph">cg::less:</samp> Accepts two values and returns the lesser using operator&lt;.
                                       This differs in that the <strong class="ph b">lower value is returned</strong> rather than a boolean.
                                       
                                    </li>
                                    <li class="li"><samp class="ph codeph">cg::greater:</samp> Accepts two values and returns the greater using operator&lt;.
                                       This differs in that the <strong class="ph b">greater value is returned</strong> rather than a boolean.
                                       
                                    </li>
                                    <li class="li"><samp class="ph codeph">cg::bit_and:</samp> Accepts two values and returns the result of operator&amp;.
                                    </li>
                                    <li class="li"><samp class="ph codeph">cg::bit_xor:</samp> Accepts two values and returns the result of operator^.
                                    </li>
                                    <li class="li"><samp class="ph codeph">cg::bit_or:</samp> Accepts two values and returns the result of operator|.
                                    </li>
                                 </ul>
                              </div>
                              <div class="p"><strong class="ph b">Example:</strong><pre xml:space="preserve">
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cg::plus&lt;int&gt; is specialized within cg::reduce and calls __reduce_add_sync(...) on CC 8.0+</span>
    cg::reduce(tile, (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>)val, cg::plus&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;());

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cg::plus&lt;float&gt; fails to match with an accelerator and instead performs a standard shuffle based reduction</span>
    cg::reduce(tile, (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>)val, cg::plus&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt;());

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// While individual components of a vector are supported, reduce will not use hardware intrinsics for the following</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// It will also be necessary to define a corresponding operator for vector and any custom types that may be used</span>
    int4 vec = {...};
    cg::reduce(tile, vec, cg::plus&lt;int4&gt;())

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Finally lambdas and other function objects cannot be inspected for dispatch</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// and will instead perform shuffle based reductions using the provided function object.</span>
    cg::reduce(tile, (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>)val, [](<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> l, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> r) -&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> {<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> l + r;});
}</pre></div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="grid-synchronization-cg"><a name="grid-synchronization-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#grid-synchronization-cg" name="grid-synchronization-cg" shape="rect">C.7.&nbsp;Grid Synchronization</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           Prior to the introduction of Cooperative Groups, the CUDA programming
                           model only allowed synchronization between thread blocks at a kernel
                           completion boundary. The kernel boundary carries with it an implicit
                           invalidation of state, and with it, potential performance
                           implications.
                           
                        </p>
                        <p class="p">
                           For example, in certain use cases, applications have a large number of
                           small kernels, with each kernel representing a stage in a processing
                           pipeline. The presence of these kernels is required by the current
                           CUDA programming model to ensure that the thread blocks operating on
                           one pipeline stage have produced data before the thread block
                           operating on the next pipeline stage is ready to consume it. In such
                           cases, the ability to provide global inter thread block
                           synchronization would allow the application to be restructured to have
                           persistent thread blocks, which are able to synchronize on the device
                           when a given stage is complete.
                           
                        </p>
                        <p class="p">
                           To synchronize across the grid, from within a kernel, you would simply use
                           the grid.sync() functionality:
                           
                        </p><pre xml:space="preserve">
grid_group grid = this_grid();
grid.sync();
</pre><p class="p">
                           And when launching the kernel it is necessary to use, instead
                           of the <samp class="ph codeph">&lt;&lt;&lt;...&gt;&gt;&gt;</samp> execution configuration syntax, the
                           <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g504b94170f83285c71031be6d5d15f73" target="_blank" shape="rect">cudaLaunchCooperativeKernel</a></samp> CUDA runtime
                           launch API or the <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html#group__CUDA__EXEC_1g06d753134145c4584c0c62525c1894cb" target="_blank" shape="rect">CUDA driver equivalent</a></samp>.
                           
                        </p>
                        <p class="p"><strong class="ph b">Example:</strong></p>
                        <p class="p">
                           To guarantee co-residency of the thread blocks on the GPU, the
                           number of blocks launched needs to be carefully considered. For
                           example, as many blocks as there are SMs can be launched as follows:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> device = 0;
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&amp;deviceProp, dev);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// initialize, then launch</span>
cudaLaunchCooperativeKernel((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)my_kernel, deviceProp.multiProcessorCount, numThreads, args);
</pre><p class="p">
                           Alternatively, you can maximize the exposed parallelism by calculating how many blocks can fit
                           simultaneously per-SM using the occupancy calculator as
                           follows:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-doccomment">/// This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> numBlocksPerSm = 0;
 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Number of threads my_kernel will be launched with</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> numThreads = 128;
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&amp;deviceProp, dev);
cudaOccupancyMaxActiveBlocksPerMultiprocessor(&amp;numBlocksPerSm, my_kernel, numThreads, 0);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// launch</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *kernelArgs[] = { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* add kernel args */</span> };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(numThreads, 1, 1);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(deviceProp.multiProcessorCount*numBlocksPerSm, 1, 1);
cudaLaunchCooperativeKernel((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)my_kernel, dimGrid, dimBlock, kernelArgs);
</pre><p class="p">
                           It is good practice to first ensure the device supports cooperative launches
                           by querying the device attribute <samp class="ph codeph">cudaDevAttrCooperativeLaunch</samp>:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> dev = 0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> supportsCoopLaunch = 0;
cudaDeviceGetAttribute(&amp;supportsCoopLaunch, cudaDevAttrCooperativeLaunch, dev);
</pre><p class="p">
                           which will set <samp class="ph codeph">supportsCoopLaunch</samp> to 1 if the property is supported
                           on device 0. Only devices with compute capability of 6.0 and higher are supported.
                           In addition, you need to be running on either of these:
                           
                        </p>
                        <div class="p"><a name="grid-synchronization-cg__ul_cwp_qgz_cgb" shape="rect">
                              <!-- --></a><ul class="ul" id="grid-synchronization-cg__ul_cwp_qgz_cgb">
                              <li class="li">The Linux platform without MPS</li>
                              <li class="li">The Linux platform with MPS and on a device with compute capability 7.0 or higher</li>
                              <li class="li">The latest Windows platform</li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="multi-device-synchronization-cg"><a name="multi-device-synchronization-cg" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#multi-device-synchronization-cg" name="multi-device-synchronization-cg" shape="rect">C.8.&nbsp;Multi-Device Synchronization</a></h3>
                     <div class="body conbody">
                        <div class="p">
                           In order to enable synchronization across multiple devices with Cooperative Groups, use of
                           the <samp class="ph codeph">cudaLaunchCooperativeKernelMultiDevice</samp> CUDA API is required. This, a
                           significant departure from existing CUDA APIs, will allow a single host thread to launch a
                           kernel across multiple devices. In addition to the constraints and guarantees made by
                           <samp class="ph codeph">cudaLaunchCooperativeKernel</samp>, this API has additional semantics:
                           
                           <ul class="ul">
                              <li class="li">
                                 This API will ensure that a launch is atomic, i.e. if the API call succeeds, then the
                                 provided number of thread blocks will launch on all specified devices.
                                 
                              </li>
                              <li class="li">
                                 The functions launched via this API must be identical. No explicit checks are done by
                                 the driver in this regard because it is largely not feasible. It is up to the
                                 application to ensure this.
                                 
                              </li>
                              <li class="li">
                                 No two entries in the provided <samp class="ph codeph">cudaLaunchParams</samp> may map to the same
                                 device.
                                 
                              </li>
                              <li class="li">
                                 All devices being targeted by this launch must be of the same compute capability -
                                 major and minor versions.
                                 
                              </li>
                              <li class="li">
                                 The block size, grid size and amount of shared memory per grid must be the same
                                 across all devices. Note that this means the maximum number of blocks that can be
                                 launched per device will be limited by the device with the least number of SMs.
                                 
                              </li>
                              <li class="li">
                                 Any user defined <samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__constant__</samp> or
                                 <samp class="ph codeph">__managed__</samp> device global variables present in the module that owns
                                 the CUfunction being launched are independently instantiated on every device. The user
                                 is responsible for initializing such device global variables appropriately.
                              </li>
                           </ul>
                        </div>
                        <p class="p">
                           Deprecation Notice: <samp class="ph codeph">cudaLaunchCooperativeKernelMultiDevice</samp> has been deprecated in CUDA 11.3 for all devices.
                           Example of an alternative approach can be found in the multi device conjugate gradient sample.
                           
                        </p>
                        <p class="p">
                           Optimal performance in multi-device synchronization is achieved by
                           enabling peer access via <samp class="ph codeph">cuCtxEnablePeerAccess</samp> or
                           <samp class="ph codeph">cudaDeviceEnablePeerAccess</samp> for all participating devices.
                           
                        </p>
                        <p class="p">
                           The launch parameters should be defined using an array of structs (one per device),
                           and launched with <samp class="ph codeph">cudaLaunchCooperativeKernelMultiDevice</samp></p>
                        <p class="p"><strong class="ph b">Example:</strong></p><pre xml:space="preserve">
cudaDeviceProp deviceProp;
cudaGetDeviceCount(&amp;numGpus);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Per device launch parameters</span>
cudaLaunchParams *launchParams = (cudaLaunchParams*)malloc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(cudaLaunchParams) * numGpus);
cudaStream_t *streams = (cudaStream_t*)malloc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(cudaStream_t) * numGpus);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The kernel arguments are copied over during launch</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Its also possible to have individual copies of kernel arguments per device, but</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the signature and name of the function/kernel must be the same.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *kernelArgs[] = { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* Add kernel arguments */</span> };

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; numGpus; i++) {
    cudaSetDevice(i);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Per device stream, but its also possible to use the default NULL stream of each device</span>
    cudaStreamCreate(&amp;streams[i]);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Loop over other devices and cudaDeviceEnablePeerAccess to get a faster barrier implementation</span>
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Since all devices must be of the same compute capability and have the same launch configuration</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// it is sufficient to query device 0 here</span>
cudaGetDeviceProperties(&amp;deviceProp[i], 0);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimBlock(numThreads, 1, 1);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> dimGrid(deviceProp.multiProcessorCount, 1, 1);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; numGpus; i++) {
    launchParamsList[i].func = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)my_kernel;
    launchParamsList[i].gridDim = dimGrid;
    launchParamsList[i].<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span> = dimBlock;
    launchParamsList[i].sharedMem = 0;
    launchParamsList[i].stream = streams[i];
    launchParamsList[i].args = kernelArgs;
}
cudaLaunchCooperativeKernelMultiDevice(launchParams, numGpus);
</pre><p class="p">Also, as with grid-wide synchronization, the resulting device code looks very similar:</p><pre xml:space="preserve">
multi_grid_group multi_grid = this_multi_grid();
multi_grid.sync();
</pre><p class="p">However, the code needs to be compiled in separate compilation by passing -rdc=true to nvcc.</p>
                        <p class="p">
                           It is good practice to first ensure the device supports multi-device
                           cooperative launches by querying the device attribute
                           <samp class="ph codeph">cudaDevAttrCooperativeMultiDeviceLaunch</samp>:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> dev = 0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> supportsMdCoopLaunch = 0;
cudaDeviceGetAttribute(&amp;supportsMdCoopLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, dev);
</pre><p class="p">
                           which will set <samp class="ph codeph">supportsMdCoopLaunch</samp> to 1 if the property is supported
                           on device 0. Only devices with compute capability of 6.0 and higher are supported.
                           In addition, you need to be running on the Linux platform (without MPS)
                           or on current versions of Windows with the device in TCC mode.
                           
                        </p>
                        <p class="p">
                           See the <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g20f8d75d8786c54cc168c47fde66ee52" target="_blank" shape="rect">cudaLaunchCooperativeKernelMultiDevice</a></samp> API documentation for more information.
                           
                        </p>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="cuda-dynamic-parallelism"><a name="cuda-dynamic-parallelism" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#cuda-dynamic-parallelism" name="cuda-dynamic-parallelism" shape="rect">D.&nbsp;CUDA Dynamic Parallelism</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="introduction-cuda-dynamic-parallelism"><a name="introduction-cuda-dynamic-parallelism" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#introduction-cuda-dynamic-parallelism" name="introduction-cuda-dynamic-parallelism" shape="rect">D.1.&nbsp;Introduction</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="overview"><a name="overview" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#overview" name="overview" shape="rect">D.1.1.&nbsp;Overview</a></h3>
                        <div class="body conbody">
                           <p class="p"><dfn class="term">Dynamic Parallelism</dfn> is an extension to the CUDA programming model enabling
                              a CUDA kernel to create and synchronize with new work directly on the GPU. The creation
                              of parallelism dynamically at whichever point in a program that it is needed offers
                              exciting new capabilities.
                           </p>
                           <p class="p">The ability to create work directly from the GPU can reduce the need to transfer
                              execution control and data between host and device, as launch configuration decisions
                              can now be made at runtime by threads executing on the device. Additionally,
                              data-dependent parallel work can be generated inline within a kernel at run-time, taking
                              advantage of the GPU's hardware schedulers and load balancers dynamically and adapting
                              in response to data-driven decisions or workloads. Algorithms and programming patterns
                              that had previously required modifications to eliminate recursion, irregular loop
                              structure, or other constructs that do not fit a flat, single-level of parallelism may
                              more transparently be expressed.
                           </p>
                           <p class="p">This document describes the extended capabilities of CUDA which enable Dynamic
                              Parallelism, including the modifications and additions to the CUDA  programming model
                              necessary to take advantage of these, as well as guidelines and best practices for
                              exploiting this added capacity.
                           </p>
                           <p class="p">Dynamic Parallelism is only supported by devices of compute capability 3.5 and
                              higher.
                           </p>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="glossary"><a name="glossary" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#glossary" name="glossary" shape="rect">D.1.2.&nbsp;Glossary</a></h3>
                        <div class="body refbody">
                           <div class="section">
                              <p class="p">Definitions for terms used in this guide.</p>
                              <dl class="dl">
                                 <dt class="dt dlterm">Grid</dt>
                                 <dd class="dd">A Grid is a collection of <dfn class="term">Threads</dfn>. Threads
                                    in a Grid execute a <dfn class="term">Kernel Function</dfn> and
                                    are divided into <dfn class="term">Thread Blocks</dfn>.
                                 </dd>
                                 <dt class="dt dlterm">Thread Block</dt>
                                 <dd class="dd">A Thread Block is a group of threads which execute on
                                    the same multiprocessor (<dfn class="term">SM</dfn>). Threads
                                    within a Thread Block have access to shared memory
                                    and can be explicitly synchronized.
                                 </dd>
                                 <dt class="dt dlterm">Kernel Function</dt>
                                 <dd class="dd">A Kernel Function is an implicitly parallel subroutine
                                    that executes under the CUDA execution and memory
                                    model for every Thread in a Grid.
                                 </dd>
                                 <dt class="dt dlterm">Host</dt>
                                 <dd class="dd">The Host refers to the execution environment that
                                    initially invoked CUDA. Typically the thread running
                                    on a system's CPU processor.
                                 </dd>
                                 <dt class="dt dlterm">Parent</dt>
                                 <dd class="dd">A <dfn class="term">Parent Thread</dfn>, Thread Block, or Grid is
                                    one that has launched new grid(s), the
                                    <dfn class="term">Child</dfn> Grid(s). The Parent is not
                                    considered completed until all of its launched Child
                                    Grids have also completed.
                                 </dd>
                                 <dt class="dt dlterm">Child</dt>
                                 <dd class="dd">A Child thread, block, or grid is one that has been
                                    launched by a Parent grid. A Child grid must
                                    complete before the Parent Thread, Thread Block, or
                                    Grid are considered complete.
                                 </dd>
                                 <dt class="dt dlterm">Thread Block Scope</dt>
                                 <dd class="dd">Objects with Thread Block Scope have the lifetime of a
                                    single Thread Block. They only have defined behavior
                                    when operated on by Threads in the Thread Block that
                                    created the object and are destroyed when the Thread
                                    Block that created them is complete.
                                 </dd>
                                 <dt class="dt dlterm">Device Runtime</dt>
                                 <dd class="dd">The Device Runtime refers to the runtime system and APIs
                                    available to enable Kernel Functions to use Dynamic
                                    Parallelism.
                                 </dd>
                              </dl>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="execution-environment-and-memory-model"><a name="execution-environment-and-memory-model" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#execution-environment-and-memory-model" name="execution-environment-and-memory-model" shape="rect">D.2.&nbsp;Execution Environment and Memory Model</a></h3>
                     <div class="body conbody"></div>
                     <div class="topic concept nested2" xml:lang="en-US" id="execution-environment"><a name="execution-environment" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#execution-environment" name="execution-environment" shape="rect">D.2.1.&nbsp;Execution Environment</a></h3>
                        <div class="body conbody">
                           <p class="p">The CUDA execution model is based on primitives of threads, thread blocks, and grids,
                              with kernel functions defining the program executed by individual threads within a
                              thread block and grid. When a kernel function is invoked the grid's properties are
                              described by an execution configuration, which has a special syntax in CUDA. Support for
                              dynamic parallelism in CUDA extends the ability to configure, launch, and synchronize
                              upon new grids to threads that are running on the device.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="parent-and-child-grids"><a name="parent-and-child-grids" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#parent-and-child-grids" name="parent-and-child-grids" shape="rect">D.2.1.1.&nbsp;Parent and Child Grids</a></h3>
                           <div class="body conbody">
                              <p class="p">A device thread that configures and launches a new grid belongs to the
                                 parent grid, and the grid created by the invocation is a child grid.
                              </p>
                              <p class="p">The invocation and completion of child grids is properly nested, meaning
                                 that the parent grid is not considered complete until all child grids
                                 created by its threads have completed. Even if the invoking threads do
                                 not explicitly synchronize on the child grids launched, the runtime
                                 guarantees an implicit synchronization between the parent and child.
                              </p>
                              <div class="fig fignone"><span class="figcap">Figure 12. Parent-Child Launch Nesting</span><img class="image" src="graphics/parent-child-launch-nesting.png" alt="A figure illustrating Parent-Child launch nesting."></img></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="scope-of-cuda-primitives"><a name="scope-of-cuda-primitives" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#scope-of-cuda-primitives" name="scope-of-cuda-primitives" shape="rect">D.2.1.2.&nbsp;Scope of CUDA Primitives</a></h3>
                           <div class="body conbody">
                              <p class="p">On both host and device, the CUDA runtime offers an API for launching kernels, for
                                 waiting for launched work to complete, and for tracking dependencies between launches
                                 via streams and events. On the host system, the state of launches and the CUDA
                                 primitives referencing streams and events are shared by all threads within a process;
                                 however processes execute independently and may not share CUDA objects.
                              </p>
                              <p class="p">A similar hierarchy exists on the device: launched kernels and CUDA objects are visible
                                 to all threads in a thread block, but are independent between thread blocks. This means
                                 for example that a stream may be created by one thread and used by any other thread in
                                 the same thread block, but may not be shared with threads in any other thread block.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="synchronization"><a name="synchronization" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#synchronization" name="synchronization" shape="rect">D.2.1.3.&nbsp;Synchronization</a></h3>
                           <div class="body conbody">
                              <p class="p">CUDA runtime operations from any thread, including kernel launches, are visible across a
                                 thread block. This means that an invoking thread in the parent grid may perform
                                 synchronization on the grids launched by that thread, by other threads in the thread
                                 block, or on streams created within the same thread block. Execution of a thread block
                                 is not considered complete until all launches by all threads in the block have
                                 completed. If all threads in a block exit before all child launches have completed, a
                                 synchronization operation will automatically be triggered.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="streams-and-events"><a name="streams-and-events" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#streams-and-events" name="streams-and-events" shape="rect">D.2.1.4.&nbsp;Streams and Events</a></h3>
                           <div class="body conbody">
                              <p class="p">CUDA <dfn class="term">Streams</dfn> and <dfn class="term">Events</dfn> allow control over
                                 dependencies between grid launches: grids launched into the same stream
                                 execute in-order, and events may be used to create dependencies between
                                 streams. Streams and events created on the device serve this exact same
                                 purpose.
                              </p>
                              <p class="p">Streams and events created within a grid exist within thread block
                                 scope but have undefined behavior when used outside of the thread block
                                 where they were created. As described above, all work launched by a
                                 thread block is implicitly synchronized when the block exits; work
                                 launched into streams is included in this, with all dependencies
                                 resolved appropriately. The behavior of operations on a stream that has
                                 been modified outside of thread block scope is undefined. 
                              </p>
                              <p class="p">Streams and events created on the host have undefined behavior when
                                 used within any kernel, just as streams and events created by a parent
                                 grid have undefined behavior if used within a child grid.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="ordering-and-concurrency"><a name="ordering-and-concurrency" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#ordering-and-concurrency" name="ordering-and-concurrency" shape="rect">D.2.1.5.&nbsp;Ordering and Concurrency</a></h3>
                           <div class="body conbody">
                              <p class="p"> The ordering of kernel launches from the device runtime follows CUDA Stream ordering
                                 semantics. Within a thread block, all kernel launches into the same stream are executed
                                 in-order. With multiple threads in the same thread block launching into the same stream,
                                 the ordering within the stream is dependent on the thread scheduling within the block,
                                 which may be controlled with synchronization primitives such as
                                 <samp class="ph codeph">__syncthreads()</samp>.
                              </p>
                              <p class="p">Note that because streams are shared by all threads within a thread block, the implicit
                                 <dfn class="term">NULL</dfn> stream is also shared. If multiple threads in a thread block
                                 launch into the implicit stream, then these launches will be executed in-order. If
                                 concurrency is desired, explicit named streams should be used.
                              </p>
                              <p class="p"><dfn class="term">Dynamic Parallelism</dfn> enables concurrency to be expressed more easily within a
                                 program; however, the device runtime introduces no new concurrency guarantees within the
                                 CUDA execution model. There is no guarantee of concurrent execution between any number
                                 of different thread blocks on a device.
                              </p>
                              <p class="p">The lack of concurrency guarantee extends to parent thread blocks and their child grids.
                                 When a parent thread block launches a child grid, the child is not guaranteed to begin
                                 execution until the parent thread block reaches an explicit synchronization point (e.g.
                                 <samp class="ph codeph">cudaDeviceSynchronize()</samp>). 
                              </p>
                              <p class="p">While concurrency will often easily be achieved, it may vary as a function of
                                 deviceconfiguration, application workload, and runtime scheduling. It is therefore
                                 unsafe to depend upon any concurrency between different thread blocks.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="device-management"><a name="device-management" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#device-management" name="device-management" shape="rect">D.2.1.6.&nbsp;Device Management</a></h3>
                           <div class="body conbody">
                              <p class="p">There is no multi-GPU support from the device runtime; the device runtime is only capable
                                 of operating on the device upon which it is currently executing. It is permitted,
                                 however, to query properties for any CUDA capable device in the system.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="memory-model"><a name="memory-model" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#memory-model" name="memory-model" shape="rect">D.2.2.&nbsp;Memory Model</a></h3>
                        <div class="body conbody">
                           <p class="p">Parent and child grids share the same global and constant memory
                              storage, but have distinct local and shared memory.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="coherence-and-consistency"><a name="coherence-and-consistency" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#coherence-and-consistency" name="coherence-and-consistency" shape="rect">D.2.2.1.&nbsp;Coherence and Consistency</a></h3>
                           <div class="topic concept nested4" xml:lang="en-US" id="global-memory"><a name="global-memory" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#global-memory" name="global-memory" shape="rect">D.2.2.1.1.&nbsp;Global Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Parent and child grids have coherent access to global memory, with weak
                                    consistency guarantees between child and parent. There are two points in
                                    the execution of a child grid when its view of memory is fully consistent
                                    with the parent thread: when the child grid is invoked by the parent, and
                                    when the child grid completes as signaled by a synchronization API
                                    invocation in the parent thread.
                                 </p>
                                 <p class="p">All global memory operations in the parent thread prior to the child
                                    grid's invocation are visible to the child grid. All memory operations of
                                    the child grid are visible to the parent after the parent has
                                    synchronized on the child grid's completion.
                                 </p>
                                 <p class="p">In the following example, the child grid executing
                                    <samp class="ph codeph">child_launch</samp> is only guaranteed to see the modifications
                                    to <samp class="ph codeph">data</samp> made before the child grid was launched. Since
                                    thread 0 of the parent is performing the launch, the child will be
                                    consistent with the memory seen by thread 0 of the parent. Due to the
                                    first <samp class="ph codeph">__syncthreads()</samp> call, the child will see
                                    <samp class="ph codeph">data[0]=0</samp>, <samp class="ph codeph">data[1]=1</samp>, ...,
                                    <samp class="ph codeph">data[255]=255</samp> (without the
                                    <samp class="ph codeph">__syncthreads()</samp> call, only <samp class="ph codeph">data[0]</samp>
                                    would be guaranteed to be seen by the child). When the child grid
                                    returns, thread 0 is guaranteed to see modifications made by the threads
                                    in its child grid. Those modifications become available to the other
                                    threads of the parent grid only after the second
                                    <samp class="ph codeph">__syncthreads()</samp> call:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> child_launch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data) {
   data[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = data[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x]+1;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> parent_launch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data) {
   data[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;

   __syncthreads();

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0) {
       child_launch<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 256 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data);
       cudaDeviceSynchronize();
   }

   __syncthreads();
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_launch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data) {
    parent_launch<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 256 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data);
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="zero-copy-memory"><a name="zero-copy-memory" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#zero-copy-memory" name="zero-copy-memory" shape="rect">D.2.2.1.2.&nbsp;Zero Copy Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Zero-copy system memory has identical coherence and consistency guarantees to global
                                    memory, and follows the semantics detailed above. A kernel may not allocate or free
                                    zero-copy memory, but may use pointers to zero-copy passed in from the host program.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="constant-memory"><a name="constant-memory" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#constant-memory" name="constant-memory" shape="rect">D.2.2.1.3.&nbsp;Constant Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Constants are immutable and may not be modified from the device, even between parent and
                                    child launches. That is to say, the value of all <samp class="ph codeph">__constant__</samp> variables
                                    must be set from the host prior to launch. Constant memory is inherited automatically by
                                    all child kernels from their respective parents.
                                 </p>
                                 <p class="p">Taking the address of a constant memory object from within a kernel thread has the same
                                    semantics as for all CUDA programs, and passing that pointer from parent to child or
                                    from a child to parent is naturally supported.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="shared-and-local-memory"><a name="shared-and-local-memory" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#shared-and-local-memory" name="shared-and-local-memory" shape="rect">D.2.2.1.4.&nbsp;Shared and Local Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Shared and Local memory is private to a thread block or thread, respectively, and is not
                                    visible or coherent between parent and child. Behavior is undefined when an object in
                                    one of these locations is referenced outside of the scope within which it belongs, and
                                    may cause an error.
                                 </p>
                                 <p class="p">The NVIDIA compiler will attempt to warn if it can detect that a pointer to local or
                                    shared memory is being passed as an argument to a kernel launch. At runtime, the
                                    programmer may use the <samp class="ph codeph">__isGlobal()</samp> intrinsic to determine whether a
                                    pointer references global memory and so may safely be passed to a child launch.
                                 </p>
                                 <p class="p">Note that calls to <samp class="ph codeph">cudaMemcpy*Async()</samp> or
                                    <samp class="ph codeph">cudaMemset*Async()</samp> may invoke new child kernels on the device in
                                    order to preserve stream semantics. As such, passing shared or local memory pointers to
                                    these APIs is illegal and will return an error.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="local-memory"><a name="local-memory" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#local-memory" name="local-memory" shape="rect">D.2.2.1.5.&nbsp;Local Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Local memory is private storage for an executing thread, and is not
                                    visible outside of that thread. It is illegal to pass a pointer to local
                                    memory as a launch argument when launching a child kernel. The result of
                                    dereferencing such a local memory pointer from a child will be
                                    undefined.
                                 </p>
                                 <p class="p">For example the following is illegal, with undefined behavior if
                                    <samp class="ph codeph">x_array</samp> is accessed by
                                    <samp class="ph codeph">child_launch</samp>:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x_array[10];       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Creates x_array in parent's local memory </span>
child_launch<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(x_array);</pre><p class="p">It is sometimes difficult for a programmer to be aware of when a
                                    variable is placed into local memory by the compiler. As a general rule,
                                    all storage passed to a child kernel should be allocated explicitly from
                                    the global-memory heap, either with <samp class="ph codeph">cudaMalloc()</samp>,
                                    <samp class="ph codeph">new()</samp> or by declaring <samp class="ph codeph">__device__</samp>
                                    storage at global scope. For example:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Correct - "value" is global storage</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value; 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> x() { 
    value = 5; 
    child<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(&amp;value); 
}</pre><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invalid - "value" is local storage</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> y() { 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> value = 5; 
    child<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(&amp;value); 
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="texture-memory-cdp"><a name="texture-memory-cdp" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#texture-memory-cdp" name="texture-memory-cdp" shape="rect">D.2.2.1.6.&nbsp;Texture Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Writes to the global memory region over which a texture is mapped are
                                    incoherent with respect to texture accesses. Coherence for texture memory
                                    is enforced at the invocation of a child grid and when a child grid
                                    completes. This means that writes to memory prior to a child kernel
                                    launch are reflected in texture memory accesses of the child.  Similarly,
                                    writes to memory by a child will be reflected in the texture memory
                                    accesses by a parent, but only after the parent synchronizes on the
                                    child's completion.  Concurrent accesses by parent and child may result
                                    in inconsistent data.
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="programming-interface-cdp"><a name="programming-interface-cdp" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#programming-interface-cdp" name="programming-interface-cdp" shape="rect">D.3.&nbsp;Programming Interface</a></h3>
                     <div class="body conbody">
                        <p class="p"></p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="cuda-c-cplusplus"><a name="cuda-c-cplusplus" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#cuda-c-cplusplus" name="cuda-c-cplusplus" shape="rect">D.3.1.&nbsp;CUDA C++ Reference</a></h3>
                        <div class="body conbody">
                           <p class="p">This section describes changes and additions to the CUDA C++ language extensions for
                              supporting <dfn class="term">Dynamic Parallelism</dfn>.
                           </p>
                           <p class="p">The language interface and API available to CUDA kernels using CUDA C++ for Dynamic
                              Parallelism, referred to as the <dfn class="term">Device Runtime</dfn>, is substantially like that
                              of the CUDA Runtime API available on the host. Where possible the syntax and semantics
                              of the CUDA Runtime API have been retained in order to facilitate ease of code reuse for
                              routines that may run in either the host or device environments.
                           </p>
                           <p class="p">As with all code in CUDA C++, the APIs and code outlined here is per-thread code. This
                              enables each thread to make unique, dynamic decisions regarding what kernel or operation
                              to execute next. There are no synchronization requirements between threads within a
                              block to execute any of the provided device runtime APIs, which enables the device
                              runtime API functions to be called in arbitrarily divergent kernel code without
                              deadlock.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="device-side-kernel-launch"><a name="device-side-kernel-launch" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#device-side-kernel-launch" name="device-side-kernel-launch" shape="rect">D.3.1.1.&nbsp;Device-Side Kernel Launch</a></h3>
                           <div class="body conbody">
                              <p class="p">Kernels may be launched from the device using the standard CUDA
                                 &lt;&lt;&lt; &gt;&gt;&gt; syntax:
                              </p><pre xml:space="preserve">kernel_name<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> Dg, Db, Ns, S <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>([kernel arguments]);</pre><ul class="ul">
                                 <li class="li"><samp class="ph codeph">Dg</samp> is of type <samp class="ph codeph">dim3</samp> and specifies
                                    the dimensions and size of the grid
                                 </li>
                                 <li class="li"><samp class="ph codeph">Db</samp> is of type <samp class="ph codeph">dim3</samp> and specifies
                                    the dimensions and size of each thread block
                                 </li>
                                 <li class="li"><samp class="ph codeph">Ns</samp> is of type <samp class="ph codeph">size_t</samp> and specifies
                                    the number of bytes of shared memory that is dynamically allocated per
                                    thread block for this call and addition to statically allocated memory.
                                    <samp class="ph codeph">Ns</samp> is an optional argument that defaults to 0.
                                 </li>
                                 <li class="li"><samp class="ph codeph">S</samp> is of type <samp class="ph codeph">cudaStream_t</samp> and
                                    specifies the stream associated with this call. The stream must have
                                    been allocated in the same thread block where the call is being made.
                                    <samp class="ph codeph">S</samp> is an optional argument that defaults to 0.
                                 </li>
                              </ul>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="launches-are-asynchronous"><a name="launches-are-asynchronous" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#launches-are-asynchronous" name="launches-are-asynchronous" shape="rect">D.3.1.1.1.&nbsp;Launches are Asynchronous</a></h3>
                              <div class="body conbody">
                                 <p class="p">Identical to host-side launches, all device-side kernel launches are
                                    asynchronous with respect to the launching thread. That is to say, the
                                    <samp class="ph codeph">&lt;&lt;&lt;&gt;&gt;&gt;</samp> launch command will return
                                    immediately and the launching thread will continue to execute until it
                                    hits an explicit launch-synchronization point such as
                                    <samp class="ph codeph">cudaDeviceSynchronize()</samp>.  The grid launch is posted to
                                    the device and will execute independently of the parent thread. The child
                                    grid may begin execution at any time after launch, but is not guaranteed
                                    to begin execution until the launching thread reaches an explicit
                                    launch-synchronization point.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="launch-environment-configuration"><a name="launch-environment-configuration" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#launch-environment-configuration" name="launch-environment-configuration" shape="rect">D.3.1.1.2.&nbsp;Launch Environment Configuration</a></h3>
                              <div class="body conbody">
                                 <p class="p">All global device configuration settings (e.g., shared memory and L1
                                    cache size as returned from <samp class="ph codeph">cudaDeviceGetCacheConfig()</samp>,
                                    and device limits returned from <samp class="ph codeph">cudaDeviceGetLimit()</samp>)
                                    will be inherited from the parent. Likewise, device limits such as stack size will remain
                                    as-configured.
                                 </p>
                                 <p class="p">For host-launched kernels, per-kernel configurations set from the host
                                    will take precedence over the global setting. These configurations will
                                    be used when the kernel is launched from the device as well. It is not
                                    possible to reconfigure a kernel's environment from the device.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="streams-cdp"><a name="streams-cdp" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#streams-cdp" name="streams-cdp" shape="rect">D.3.1.2.&nbsp;Streams</a></h3>
                           <div class="body conbody">
                              <p class="p">Both named and unnamed (NULL) streams are available from the device runtime. Named
                                 streams may be used by any thread within a thread-block, but stream handles may not be
                                 passed to other blocks or child/parent kernels. In other words, a stream should be
                                 treated as private to the block in which it is created. Stream handles are not
                                 guaranteed to be unique between blocks, so using a stream handle within a block that did
                                 not allocate it will result in undefined behavior.
                              </p>
                              <p class="p">Similar to host-side launch, work launched into separate streams may run concurrently,
                                 but actual concurrency is not guaranteed. Programs that depend upon concurrency between
                                 child kernels are not supported by the CUDA programming model and will have undefined
                                 behavior.
                              </p>
                              <p class="p">The host-side NULL stream's cross-stream barrier semantic is not supported on the device
                                 (see below for details). In order to retain semantic compatibility with the host
                                 runtime, all device streams must be created using the
                                 <samp class="ph codeph">cudaStreamCreateWithFlags()</samp> API, passing the
                                 <samp class="ph codeph">cudaStreamNonBlocking</samp> flag. The <samp class="ph codeph">cudaStreamCreate()</samp>
                                 call is a host-runtime- only API and will fail to compile for the device.
                              </p>
                              <p class="p">As <samp class="ph codeph">cudaStreamSynchronize()</samp> and <samp class="ph codeph">cudaStreamQuery()</samp> are
                                 unsupported by the device runtime, <samp class="ph codeph">cudaDeviceSynchronize()</samp> should be
                                 used instead when the application needs to know that stream-launched child kernels have
                                 completed.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="implicit-null-stream"><a name="implicit-null-stream" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#implicit-null-stream" name="implicit-null-stream" shape="rect">D.3.1.2.1.&nbsp;The Implicit (NULL) Stream</a></h3>
                              <div class="body conbody">
                                 <p class="p">Within a host program, the unnamed (NULL) stream has additional barrier
                                    synchronization semantics with other streams (see <a class="xref" href="index.html#default-stream" shape="rect">Default Stream</a> for details). The device runtime offers a
                                    single implicit, unnamed stream shared between all threads in a block,
                                    but as all named streams must be created with the
                                    <samp class="ph codeph">cudaStreamNonBlocking</samp> flag, work launched into the NULL
                                    stream will not insert an implicit dependency on pending work in any
                                    other streams (including NULL streams of other thread blocks).
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="events-cdp"><a name="events-cdp" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#events-cdp" name="events-cdp" shape="rect">D.3.1.3.&nbsp;Events</a></h3>
                           <div class="body conbody">
                              <p class="p">Only the inter-stream synchronization capabilities of CUDA events are supported. This
                                 means that <samp class="ph codeph">cudaStreamWaitEvent()</samp> is supported, but
                                 <samp class="ph codeph">cudaEventSynchronize()</samp>, <samp class="ph codeph">cudaEventElapsedTime()</samp>,
                                 and <samp class="ph codeph">cudaEventQuery()</samp> are not. As
                                 <samp class="ph codeph">cudaEventElapsedTime()</samp> is not supported, cudaEvents must be created
                                 via <samp class="ph codeph">cudaEventCreateWithFlags()</samp>, passing the
                                 <samp class="ph codeph">cudaEventDisableTiming</samp> flag.
                              </p>
                              <p class="p">As for all device runtime objects, event objects may be shared between all threads
                                 withinthe thread-block which created them but are local to that block and may not be
                                 passed to other kernels, or between blocks within the same kernel. Event handles are not
                                 guaranteed to be unique between blocks, so using an event handle within a block that did
                                 not create it will result in undefined behavior.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="synchronization-programming-interface"><a name="synchronization-programming-interface" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#synchronization-programming-interface" name="synchronization-programming-interface" shape="rect">D.3.1.4.&nbsp;Synchronization</a></h3>
                           <div class="body conbody">
                              <p class="p">The <samp class="ph codeph">cudaDeviceSynchronize()</samp> function will synchronize on all work
                                 launched by any thread in the thread-block up to the point where cudaDeviceSynchronize()
                                 was called. Note that <samp class="ph codeph">cudaDeviceSynchronize()</samp> may be called from within
                                 divergent code (see <a class="xref" href="index.html#block-wide-synchronization" shape="rect">Block Wide Synchronization</a>). 
                              </p>
                              <p class="p">It is up to the program to perform sufficient additional inter-thread synchronization,
                                 for example via a call to <samp class="ph codeph">__syncthreads()</samp>, if the calling thread is
                                 intended to synchronize with child grids invoked from other threads.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="block-wide-synchronization"><a name="block-wide-synchronization" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#block-wide-synchronization" name="block-wide-synchronization" shape="rect">D.3.1.4.1.&nbsp;Block Wide Synchronization</a></h3>
                              <div class="body conbody">
                                 <p class="p">The <samp class="ph codeph">cudaDeviceSynchronize()</samp> function does not imply intra-block
                                    synchronization. In particular, without explicit synchronization via a
                                    <samp class="ph codeph">__syncthreads()</samp> directive the calling thread can make no
                                    assumptions about what work has been launched by any thread other than itself. For
                                    example if multiple threads within a block are each launching work and synchronization
                                    is desired for all this work at once (perhaps because of event-based dependencies), it
                                    is up to the program to guarantee that this work is submitted by all threads before
                                    calling <samp class="ph codeph">cudaDeviceSynchronize()</samp>.
                                 </p>
                                 <p class="p">Because the implementation is permitted to synchronize on launches from any thread in the
                                    block, it is quite possible that simultaneous calls to
                                    <samp class="ph codeph">cudaDeviceSynchronize()</samp> by multiple threads will drain all work in
                                    the first call and then have no effect for the later calls.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="device-management-programming"><a name="device-management-programming" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#device-management-programming" name="device-management-programming" shape="rect">D.3.1.5.&nbsp;Device Management</a></h3>
                           <div class="body conbody">
                              <p class="p">Only the device on which a kernel is running will be controllable from
                                 that kernel. This means that device APIs such as
                                 <samp class="ph codeph">cudaSetDevice()</samp> are not supported by the device runtime.
                                 The active device as seen from the GPU (returned from
                                 <samp class="ph codeph">cudaGetDevice()</samp>) will have the same device number as
                                 seen from the host system. The <samp class="ph codeph">cudaDeviceGetAttribute()</samp>
                                 call may request information about another device as this API allows
                                 specification of a device ID as a parameter of the call. Note that the
                                 catch-all <samp class="ph codeph">cudaGetDeviceProperties()</samp> API is not offered
                                 by the device runtime - properties must be queried individually.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="memory-declarations"><a name="memory-declarations" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#memory-declarations" name="memory-declarations" shape="rect">D.3.1.6.&nbsp;Memory Declarations</a></h3>
                           <div class="body conbody">
                              <p class="p"></p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="device-and-constant-memory"><a name="device-and-constant-memory" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#device-and-constant-memory" name="device-and-constant-memory" shape="rect">D.3.1.6.1.&nbsp;Device and Constant Memory</a></h3>
                              <div class="body conbody">
                                 <p class="p">Memory declared at file scope with <samp class="ph codeph">__device__</samp> or
                                    <samp class="ph codeph">__constant__</samp> memory space specifiers behaves identically when using
                                    the device runtime. All kernels may read or write device variables,
                                    whether the kernel was initially launched by the host or device runtime.
                                    Equivalently, all kernels will have the same view of
                                    <samp class="ph codeph">__constant__</samp>s as declared at the module scope.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="textures-and-surfaces"><a name="textures-and-surfaces" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#textures-and-surfaces" name="textures-and-surfaces" shape="rect">D.3.1.6.2.&nbsp;Textures &amp; Surfaces</a></h3>
                              <div class="body conbody">
                                 <p class="p">CUDA supports dynamically created texture and surface objects<a name="fnsrc_14" href="#fntarg_14" shape="rect"><sup>1</sup></a>, where a texture
                                    reference may be created on the host, passed to a kernel, used by that
                                    kernel, and then destroyed from the host. The device runtime does not
                                    allow creation or destruction of texture or surface objects from within
                                    device code, but texture and surface objects created from the host may be
                                    used and passed around freely on the device. Regardless of where they are
                                    created, dynamically created texture objects are always valid and may be
                                    passed to child kernels from a parent.
                                 </p>
                                 <div class="note note"><span class="notetitle">Note:</span> The device runtime does not support legacy module-scope (i.e.,
                                    Fermi-style) textures and surfaces within a kernel launched from the
                                    device. Module-scope (legacy) textures may be created from the host and
                                    used in device code as for any kernel, but may only be used by a
                                    top-level kernel (i.e., the one which is launched from the host).
                                 </div>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="shared-memory-variable-declarations"><a name="shared-memory-variable-declarations" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-variable-declarations" name="shared-memory-variable-declarations" shape="rect">D.3.1.6.3.&nbsp;Shared Memory Variable Declarations</a></h3>
                              <div class="body conbody">
                                 <p class="p">In CUDA C++ shared memory can be declared either as a statically sized
                                    file-scope or function-scoped variable, or as an <samp class="ph codeph">extern</samp>
                                    variable with the size determined at runtime by the kernel's caller via a
                                    launch configuration argument. Both types of declarations are valid under
                                    the device runtime.
                                 </p>
                                 <p class="p"></p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> permute(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> n, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data) {
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> smem[];
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (n &lt;= 1)
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>;

   smem[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = data[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x];
   __syncthreads();

   permute_data(smem, n);
   __syncthreads();

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Write back to GMEM since we can't pass SMEM to children.</span>
   data[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = smem[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x];
   __syncthreads();

   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x == 0) {
       permute<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 256, n/2*<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(n/2, data);
       permute<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 256, n/2*<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(n/2, data+n/2);
   }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_launch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data) {
    permute<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 256, 256*<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(256, data);
}</pre></div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="symbol-addresses"><a name="symbol-addresses" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#symbol-addresses" name="symbol-addresses" shape="rect">D.3.1.6.4.&nbsp;Symbol Addresses</a></h3>
                              <div class="body conbody">
                                 <p class="p">Device-side symbols (i.e., those marked <samp class="ph codeph">__device__</samp>) may
                                    be referenced from within a kernel simply via the <samp class="ph codeph">&amp;</samp>
                                    operator, as all global-scope device variables are in the kernel's
                                    visible address space. This also applies to <samp class="ph codeph">__constant__</samp>
                                    symbols, although in this case the pointer will reference read-only
                                    data.
                                 </p>
                                 <p class="p">Given that device-side symbols can be referenced directly, those CUDA
                                    runtime APIs which reference symbols (e.g.,
                                    <samp class="ph codeph">cudaMemcpyToSymbol()</samp> or
                                    <samp class="ph codeph">cudaGetSymbolAddress()</samp>) are redundant and hence not
                                    supported by the device runtime. Note this implies that constant data
                                    cannot be altered from within a running kernel, even ahead of a child
                                    kernel launch, as references to <samp class="ph codeph">__constant__</samp> space are
                                    read-only.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="api-errors-and-launch-failures"><a name="api-errors-and-launch-failures" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#api-errors-and-launch-failures" name="api-errors-and-launch-failures" shape="rect">D.3.1.7.&nbsp;API Errors and Launch Failures</a></h3>
                           <div class="body conbody">
                              <p class="p">As usual for the CUDA runtime, any function may return an error code.
                                 The last error code returned is recorded and may be retrieved via the
                                 <samp class="ph codeph">cudaGetLastError()</samp> call. Errors are recorded per-thread,
                                 so that each thread can identify the most recent error that it has
                                 generated. The error code is of type <samp class="ph codeph">cudaError_t</samp>.
                              </p>
                              <p class="p">Similar to a host-side launch, device-side launches may fail for many
                                 reasons (invalid arguments, etc). The user must call
                                 <samp class="ph codeph">cudaGetLastError()</samp> to determine if a launch generated an
                                 error, however lack of an error after launch does not imply the child
                                 kernel completed successfully.
                              </p>
                              <p class="p">For device-side exceptions, e.g., access to an invalid address, an error
                                 in a child grid will be returned to the host instead of being returned by
                                 the parent's call to <samp class="ph codeph">cudaDeviceSynchronize()</samp>.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="launch-setup-apis"><a name="launch-setup-apis" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#launch-setup-apis" name="launch-setup-apis" shape="rect">D.3.1.7.1.&nbsp;Launch Setup APIs</a></h3>
                              <div class="body conbody">
                                 <p class="p">Kernel launch is a system-level mechanism exposed through the device
                                    runtime library, and as such is available directly from PTX via the
                                    underlying <samp class="ph codeph">cudaGetParameterBuffer()</samp> and
                                    <samp class="ph codeph">cudaLaunchDevice()</samp> APIs. It is permitted for a CUDA
                                    application to call these APIs itself, with the same requirements as for
                                    PTX. In both cases, the user is then responsible for correctly populating
                                    all necessary data structures in the correct format according to
                                    specification. Backwards compatibility is guaranteed in these data
                                    structures.
                                 </p>
                                 <p class="p">As with host-side launch, the device-side operator
                                    <samp class="ph codeph">&lt;&lt;&lt;&gt;&gt;&gt;</samp> maps to underlying kernel launch
                                    APIs. This is so that users targeting PTX will be able to enact a launch,
                                    and so that the compiler front-end can translate
                                    <samp class="ph codeph">&lt;&lt;&lt;&gt;&gt;&gt;</samp> into these calls.
                                 </p>
                                 <div class="tablenoborder">
                                    <table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                       <caption><span class="tablecap">Table 5. New Device-only Launch Implementation Functions</span></caption>
                                       <thead class="thead" align="left">
                                          <tr class="row">
                                             <th class="entry" valign="top" width="50%" id="d225e21976" rowspan="1" colspan="1">Runtime API Launch Functions</th>
                                             <th class="entry" valign="top" width="50%" id="d225e21979" rowspan="1" colspan="1">Description of Difference From Host Runtime Behaviour
                                                (behaviour is identical if no description) 
                                             </th>
                                          </tr>
                                       </thead>
                                       <tbody class="tbody">
                                          <tr class="row">
                                             <td class="entry" valign="top" width="50%" headers="d225e21976" rowspan="1" colspan="1"><samp class="ph codeph">cudaGetParameterBuffer</samp></td>
                                             <td class="entry" valign="top" width="50%" headers="d225e21979" rowspan="1" colspan="1">Generated automatically from
                                                <samp class="ph codeph">&lt;&lt;&lt;&gt;&gt;&gt;</samp>. Note different API to host
                                                equivalent.
                                             </td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" valign="top" width="50%" headers="d225e21976" rowspan="1" colspan="1"><samp class="ph codeph">cudaLaunchDevice</samp></td>
                                             <td class="entry" valign="top" width="50%" headers="d225e21979" rowspan="1" colspan="1">Generated automatically from
                                                <samp class="ph codeph">&lt;&lt;&lt;&gt;&gt;&gt;</samp>. Note different API to host
                                                equivalent.
                                             </td>
                                          </tr>
                                       </tbody>
                                    </table>
                                 </div>
                                 <p class="p">The APIs for these launch functions are different to those of the CUDA
                                    Runtime API, and are defined as follows:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span>   device   cudaError_t cudaGetParameterBuffer(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> **params);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> cudaError_t cudaLaunchDevice(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *kernel,
                                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *params, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> gridDim,
                                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>,
                                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> sharedMemSize = 0,
                                        cudaStream_t stream = 0);</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="api-reference"><a name="api-reference" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#api-reference" name="api-reference" shape="rect">D.3.1.8.&nbsp;API Reference</a></h3>
                           <div class="body conbody">
                              <p class="p">The portions of the CUDA Runtime API supported in the device runtime are detailed here.
                                 Host and device runtime APIs have identical syntax; semantics are the same except where
                                 indicated. The table below provides an overview of the API relative to the version
                                 available from the host.
                              </p>
                              <div class="tablenoborder"><a name="api-reference__table_k4z_ccs_v3" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="api-reference__table_k4z_ccs_v3" class="table" frame="border" border="1" rules="all">
                                    <caption><span class="tablecap">Table 6. Supported API Functions</span></caption>
                                    <thead class="thead" align="left">
                                       <tr class="row">
                                          <th class="entry" valign="top" width="50%" id="d225e22048" rowspan="1" colspan="1">Runtime API Functions</th>
                                          <th class="entry" valign="top" width="50%" id="d225e22051" rowspan="1" colspan="1">Details</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaDeviceSynchronize</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">Synchronizes on work launched from thread's own block only </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaDeviceGetCacheConfig</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaDeviceGetLimit</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaGetLastError</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">Last error is per-thread state, not per-block state</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaPeekAtLastError</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaGetErrorString</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaGetDeviceCount</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaDeviceGetAttribute</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">Will return attributes for any device</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaGetDevice</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">Always returns current device ID as would be seen from host </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaStreamCreateWithFlags</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">Must pass <samp class="ph codeph">cudaStreamNonBlocking</samp> flag 
                                          </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaStreamDestroy</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaStreamWaitEvent</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaEventCreateWithFlags</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">Must pass <samp class="ph codeph">cudaEventDisableTiming</samp> flag 
                                          </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaEventRecord</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaEventDestroy</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaFuncGetAttributes</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMemcpyAsync</samp></td>
                                          <td class="entry" rowspan="4" valign="top" width="50%" headers="d225e22051" colspan="1">Notes about all <samp class="ph codeph">memcpy/memset</samp>
                                             functions:<a name="api-reference__ul_n44_vds_v3" shape="rect">
                                                <!-- --></a><ul class="ul" id="api-reference__ul_n44_vds_v3">
                                                <li class="li">Only async <samp class="ph codeph">memcpy/set</samp> functions are
                                                   supported
                                                </li>
                                                <li class="li">Only device-to-device <samp class="ph codeph">memcpy</samp> is permitted
                                                </li>
                                                <li class="li">May not pass in local or shared memory pointers</li>
                                             </ul>
                                          </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMemcpy2DAsync</samp></td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMemcpy3DAsync</samp></td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMemsetAsync</samp></td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMemset2DAsync</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMemset3DAsync</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaRuntimeGetVersion</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaMalloc</samp></td>
                                          <td class="entry" rowspan="2" valign="top" width="50%" headers="d225e22051" colspan="1">May not call <samp class="ph codeph">cudaFree</samp> on the device on
                                             a pointer created on the host, and vice-versa
                                          </td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaFree</samp></td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaOccupancyMaxActiveBlocksPerMultiprocessor</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaOccupancyMaxPotentialBlockSize</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="50%" headers="d225e22048" rowspan="1" colspan="1"><samp class="ph codeph">cudaOccupancyMaxPotentialBlockSizeVariableSMem</samp></td>
                                          <td class="entry" valign="top" width="50%" headers="d225e22051" rowspan="1" colspan="1">&nbsp;</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="device-side-launch-from-ptx"><a name="device-side-launch-from-ptx" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#device-side-launch-from-ptx" name="device-side-launch-from-ptx" shape="rect">D.3.2.&nbsp;Device-side Launch from PTX</a></h3>
                        <div class="body conbody">
                           <p class="p">This section is for the programming language and compiler implementers who target
                              <dfn class="term">Parallel Thread Execution</dfn> (PTX) and plan to support <dfn class="term">Dynamic
                                 Parallelism</dfn> in their language. It provides the low-level details related to
                              supporting kernel launches at the PTX level.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="kernel-launch-apis"><a name="kernel-launch-apis" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#kernel-launch-apis" name="kernel-launch-apis" shape="rect">D.3.2.1.&nbsp;Kernel Launch APIs</a></h3>
                           <div class="body conbody">
                              <p class="p">Device-side kernel launches can be implemented using the following two APIs accessible
                                 from PTX: <samp class="ph codeph">cudaLaunchDevice()</samp> and
                                 <samp class="ph codeph">cudaGetParameterBuffer()</samp>. <samp class="ph codeph">cudaLaunchDevice()</samp>
                                 launches the specified kernel with the parameter buffer that is obtained by calling
                                 <samp class="ph codeph">cudaGetParameterBuffer()</samp> and filled with the parameters to the
                                 launched kernel. The parameter buffer can be NULL, i.e., no need to invoke
                                 <samp class="ph codeph">cudaGetParameterBuffer()</samp>, if the launched kernel does not take any
                                 parameters.
                              </p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="cudalaunchdevice"><a name="cudalaunchdevice" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#cudalaunchdevice" name="cudalaunchdevice" shape="rect">D.3.2.1.1.&nbsp;cudaLaunchDevice</a></h3>
                              <div class="body conbody">
                                 <p class="p">At the PTX level, <samp class="ph codeph">cudaLaunchDevice()</samp>needs to be
                                    declared in one of the two forms shown below before it is used.
                                 </p><pre xml:space="preserve">// PTX-level Declaration of cudaLaunchDevice() when .address_size is 64
.extern .func(.param .b32 func_retval0) cudaLaunchDevice 
( 
  .param .b64 func, 
  .param .b64 parameterBuffer, 
  .param .align 4 .b8 gridDimension[12], 
  .param .align 4 .b8 blockDimension[12], 
  .param .b32 sharedMemSize, 
  .param .b64 stream 
) 
;</pre><pre xml:space="preserve">// PTX-level Declaration of cudaLaunchDevice() when .address_size is 32
.extern .func(.param .b32 func_retval0) cudaLaunchDevice
(
  .param .b32 func,
  .param .b32 parameterBuffer,
  .param .align 4 .b8 gridDimension[12],
  .param .align 4 .b8 blockDimension[12],
  .param .b32 sharedMemSize,
  .param .b32 stream
)
;</pre><p class="p">The CUDA-level declaration below is mapped to one of the aforementioned
                                    PTX-level declarations and is found in the system header file
                                    <samp class="ph codeph">cuda_device_runtime_api.h</samp>. The function is defined in
                                    the <samp class="ph codeph">cudadevrt</samp> system library, which must be linked with
                                    a program in order to use device-side kernel launch functionality.
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// CUDA-level declaration of cudaLaunchDevice()</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"C"</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> 
cudaError_t cudaLaunchDevice(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *func, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *parameterBuffer, 
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> gridDimension, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">dim3</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>ension, 
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> sharedMemSize, 
                             cudaStream_t stream);</pre><p class="p">The first parameter is a pointer to the kernel to be is launched, and
                                    the second parameter is the parameter buffer that holds the actual
                                    parameters to the launched kernel. The layout of the parameter buffer is
                                    explained in <a class="xref" href="index.html#parameter-buffer-layout" shape="rect">Parameter Buffer Layout</a>, below. Other
                                    parameters specify the launch configuration, i.e., as grid dimension,
                                    block dimension, shared memory size, and the stream associated with the
                                    launch (please refer to <a class="xref" href="index.html#execution-configuration" shape="rect">Execution Configuration</a> for the
                                    detailed description of launch configuration.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="cudagetparameterbuffer"><a name="cudagetparameterbuffer" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#cudagetparameterbuffer" name="cudagetparameterbuffer" shape="rect">D.3.2.1.2.&nbsp;cudaGetParameterBuffer</a></h3>
                              <div class="body conbody">
                                 <p class="p"><samp class="ph codeph">cudaGetParameterBuffer()</samp> needs to be declared at the
                                    PTX level before it's used. The PTX-level declaration must be in one of
                                    the two forms given below, depending on address size:
                                 </p><pre xml:space="preserve">// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64
// When .address_size is 64
.extern .func(.param .b64 func_retval0) cudaGetParameterBuffer
(
  .param .b64 alignment,
  .param .b64 size
)
;</pre><pre xml:space="preserve">// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 32
.extern .func(.param .b32 func_retval0) cudaGetParameterBuffer
(
  .param .b32 alignment,
  .param .b32 size
)
;</pre><p class="p">The following CUDA-level declaration of
                                    <samp class="ph codeph">cudaGetParameterBuffer()</samp> is mapped to the aforementioned
                                    PTX-level declaration:
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// CUDA-level Declaration of cudaGetParameterBuffer()</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"C"</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *cudaGetParameterBuffer(size_t alignment, size_t size);</pre><p class="p">The first parameter specifies the alignment requirement of the parameter
                                    buffer and the second parameter the size requirement in bytes. In the
                                    current implementation, the parameter buffer returned by
                                    <samp class="ph codeph">cudaGetParameterBuffer()</samp> is always guaranteed to be 64-
                                    byte aligned, and the alignment requirement parameter is ignored.
                                    However, it is recommended to pass the correct alignment requirement
                                    value - which is the largest alignment of any parameter to be placed in
                                    the parameter buffer - to <samp class="ph codeph">cudaGetParameterBuffer()</samp> to
                                    ensure portability in the future.
                                 </p>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="parameter-buffer-layout"><a name="parameter-buffer-layout" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#parameter-buffer-layout" name="parameter-buffer-layout" shape="rect">D.3.2.2.&nbsp;Parameter Buffer Layout</a></h3>
                           <div class="body conbody">
                              <p class="p"> Parameter reordering in the parameter buffer is prohibited, and each individual
                                 parameter placed in the parameter buffer is required to be aligned. That is, each
                                 parameter must be placed at the <em class="ph i">n</em><sup class="ph sup">th</sup> byte in the parameter buffer,
                                 where <em class="ph i">n</em> is the smallest multiple of the parameter size that is greater than the
                                 offset of the last byte taken by the preceding parameter. The maximum size of the
                                 parameter buffer is 4KB.
                              </p>
                              <p class="p">For a more detailed description of PTX code generated by the CUDA compiler, please refer
                                 to the PTX-3.5 specification. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="toolkit-support-for-dynamic-parallelism"><a name="toolkit-support-for-dynamic-parallelism" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#toolkit-support-for-dynamic-parallelism" name="toolkit-support-for-dynamic-parallelism" shape="rect">D.3.3.&nbsp;Toolkit Support for Dynamic Parallelism</a></h3>
                        <div class="body conbody">
                           <p class="p"></p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="including-device-runtime-api-in-cuda-code"><a name="including-device-runtime-api-in-cuda-code" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#including-device-runtime-api-in-cuda-code" name="including-device-runtime-api-in-cuda-code" shape="rect">D.3.3.1.&nbsp;Including Device Runtime API in CUDA Code</a></h3>
                           <div class="body conbody">
                              <p class="p">Similar to the host-side runtime API, prototypes for the CUDA device runtime API are
                                 included automatically during program compilation. There is no need to include<samp class="ph codeph">
                                    cuda_device_runtime_api.h</samp> explicitly.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="compiling-and-linking"><a name="compiling-and-linking" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#compiling-and-linking" name="compiling-and-linking" shape="rect">D.3.3.2.&nbsp;Compiling and Linking</a></h3>
                           <div class="body conbody">
                              <p class="p">When compiling and linking CUDA programs using dynamic parallelism with
                                 <samp class="ph codeph">nvcc</samp>, the program will automatically link against the static device
                                 runtime library <samp class="ph codeph">libcudadevrt</samp>.
                              </p>
                              <p class="p">The device runtime is offered as a static library
                                 (<samp class="ph codeph">cudadevrt.lib</samp> on Windows,
                                 <samp class="ph codeph">libcudadevrt.a</samp> under Linux), against which a
                                 GPU application that uses the device runtime must be linked. Linking of
                                 device libraries can be accomplished through <samp class="ph codeph">nvcc</samp> and/or
                                 <samp class="ph codeph">nvlink</samp>. Two simple examples are shown below.
                              </p>
                              <p class="p">A device runtime program may be compiled and linked in a single step, if all required source files can be specified from the
                                 command line:
                              </p><pre class="pre screen" xml:space="preserve">$ nvcc -arch=sm_35 -rdc=true hello_world.cu -o hello -lcudadevrt</pre><p class="p">It is also possible to compile CUDA .cu source files first to object
                                 files, and then link these together in a two-stage process:
                              </p><pre class="pre screen" xml:space="preserve">$ nvcc -arch=sm_35 -dc hello_world.cu -o hello_world.o
$ nvcc -arch=sm_35 -rdc=true hello_world.o -o hello -lcudadevrt</pre><p class="p">Please see the <cite class="cite">Using Separate Compilation</cite> section of
                                 <cite class="cite">The CUDA Driver Compiler NVCC</cite> guide for more details.
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="programming-guidelines"><a name="programming-guidelines" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#programming-guidelines" name="programming-guidelines" shape="rect">D.4.&nbsp;Programming Guidelines</a></h3>
                     <div class="body conbody">
                        <p class="p"></p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="basics"><a name="basics" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#basics" name="basics" shape="rect">D.4.1.&nbsp;Basics</a></h3>
                        <div class="body conbody">
                           <p class="p">The device runtime is a functional subset of the host runtime. API level
                              device management, kernel launching, device memcpy, stream management,
                              and event management are exposed from the device runtime.
                           </p>
                           <p class="p">Programming for the device runtime should be familiar to someone who
                              already has experience with CUDA. Device runtime syntax and semantics are
                              largely the same as that of the host API, with any exceptions detailed
                              earlier in this document.
                           </p>
                           <p class="p">The following example shows a simple <dfn class="term">Hello World</dfn> program
                              incorporating dynamic parallelism:
                           </p><pre xml:space="preserve">#include &lt;stdio.h&gt; 

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> childKernel() 
{ 
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Hello "</span>); 
} 

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> parentKernel() 
{ 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// launch child </span>
    childKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaSuccess != cudaGetLastError()) { 
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>; 
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// wait for child to complete </span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaSuccess != cudaDeviceSynchronize()) { 
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>; 
    } 

    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"World!\n"</span>); 
} 

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> argc, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *argv[]) 
{ 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// launch parent </span>
    parentKernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaSuccess != cudaGetLastError()) { 
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; 
    } 

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// wait for parent to complete </span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (cudaSuccess != cudaDeviceSynchronize()) { 
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 2; 
    } 

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; 
}</pre><p class="p">This program may be built in a single step from the command line as
                              follows:
                           </p><pre class="pre screen" xml:space="preserve">$ nvcc -arch=sm_35 -rdc=true hello_world.cu -o hello -lcudadevrt</pre></div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="performance"><a name="performance" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#performance" name="performance" shape="rect">D.4.2.&nbsp;Performance</a></h3>
                        <div class="body conbody">
                           <p class="p"></p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="synchronization-performance"><a name="synchronization-performance" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#synchronization-performance" name="synchronization-performance" shape="rect">D.4.2.1.&nbsp;Synchronization</a></h3>
                           <div class="body conbody">
                              <p class="p">Synchronization by one thread may impact the performance of other threads in the same
                                 <dfn class="term">Thread Block</dfn>, even when those other threads do not call
                                 <samp class="ph codeph">cudaDeviceSynchronize()</samp> themselves. This impact will depend upon
                                 the underlying implementation. In general the implicit synchronization of child kernels done
                                 when a thread block ends is more efficient compared to calling cudaDeviceSynchronize() 
                                 explicitly. It is therefore recommended to only call cudaDeviceSynchronize() if it is needed
                                 to synchronize with a child kernel before a thread block ends.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="dynamic-parallelism-enabled-kernel-overhead"><a name="dynamic-parallelism-enabled-kernel-overhead" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#dynamic-parallelism-enabled-kernel-overhead" name="dynamic-parallelism-enabled-kernel-overhead" shape="rect">D.4.2.2.&nbsp;Dynamic-parallelism-enabled Kernel Overhead</a></h3>
                           <div class="body conbody">
                              <p class="p">System software which is active when controlling dynamic launches may
                                 impose an overhead on any kernel which is running at the time, whether or
                                 not it invokes kernel launches of its own. This overhead arises from the
                                 device runtime's execution tracking and management software and may
                                 result in decreased performance for e.g., library calls when made from the
                                 device compared to from the host side. This overhead is, in general,
                                 incurred for applications that link against the device runtime
                                 library.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="implementation-restrictions-and-limitations"><a name="implementation-restrictions-and-limitations" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#implementation-restrictions-and-limitations" name="implementation-restrictions-and-limitations" shape="rect">D.4.3.&nbsp;Implementation Restrictions and Limitations</a></h3>
                        <div class="body conbody">
                           <p class="p"><dfn class="term">Dynamic Parallelism</dfn> guarantees all semantics described in this document,
                              however, certain hardware and software resources are implementation-dependent and limit
                              the scale, performance and other properties of a program which uses the device
                              runtime.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="runtime"><a name="runtime" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#runtime" name="runtime" shape="rect">D.4.3.1.&nbsp;Runtime</a></h3>
                           <div class="body conbody">
                              <p class="p"></p>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="memory-footprint"><a name="memory-footprint" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#memory-footprint" name="memory-footprint" shape="rect">D.4.3.1.1.&nbsp;Memory Footprint</a></h3>
                              <div class="body conbody">
                                 <p class="p">The device runtime system software reserves memory for various management purposes, in
                                    particular one reservation which is used for saving parent-grid state during
                                    synchronization, and a second reservation for tracking pending grid launches.
                                    Configuration controls are available to reduce the size of these reservations in
                                    exchange for certain launch limitations. See <a class="xref" href="index.html#configuration-options" shape="rect">Configuration Options</a>, below, for details.
                                 </p>
                                 <p class="p">The majority of reserved memory is allocated as backing-store for parent kernel state,
                                    for use when synchronizing on a child launch. Conservatively, this memory must support
                                    storing of state for the maximum number of live threads possible on the device. This
                                    means that each parent generation at which <samp class="ph codeph">cudaDeviceSynchronize()</samp> is
                                    callable may require up to 860MB of device memory, depending on the device
                                    configuration, which will be unavailable for program use even if it is not all consumed.
                                    
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="nesting-and-synchronization-depth"><a name="nesting-and-synchronization-depth" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#nesting-and-synchronization-depth" name="nesting-and-synchronization-depth" shape="rect">D.4.3.1.2.&nbsp;Nesting and Synchronization Depth</a></h3>
                              <div class="body conbody">
                                 <p class="p">Using the device runtime, one kernel may launch another kernel, and that
                                    kernel may launch another, and so on. Each subordinate launch is
                                    considered a new <dfn class="term">nesting level</dfn>, and the total number of
                                    levels is the <dfn class="term">nesting depth</dfn> of the program. The
                                    <dfn class="term">synchronization depth</dfn> is defined as the deepest level at
                                    which the program will explicitly synchronize on a child launch.
                                    Typically this is one less than the nesting depth of the program, but if
                                    the program does not need to call
                                    <samp class="ph codeph">cudaDeviceSynchronize()</samp> at all levels then the
                                    synchronization depth might be substantially different to the nesting
                                    depth.
                                 </p>
                                 <p class="p">The overall maximum nesting depth is limited to 24, but practically
                                    speaking the real limit will be the amount of memory required by the
                                    system for each new level (see <a class="xref" href="index.html#memory-footprint" shape="rect">Memory Footprint</a> above). Any launch
                                    which would result in a kernel at a deeper level than the maximum will
                                    fail. Note that this may also apply to
                                    <samp class="ph codeph">cudaMemcpyAsync()</samp>, which might itself generate a kernel
                                    launch. See <a class="xref" href="index.html#configuration-options" shape="rect">Configuration Options</a> for details.
                                 </p>
                                 <p class="p">By default, sufficient storage is reserved for two levels of
                                    synchronization. This maximum synchronization depth (and hence reserved
                                    storage) may be controlled by calling
                                    <samp class="ph codeph">cudaDeviceSetLimit()</samp> and specifying
                                    <samp class="ph codeph">cudaLimitDevRuntimeSyncDepth</samp>. The number of levels to be
                                    supported must be configured before the top-level kernel is launched from
                                    the host, in order to guarantee successful execution of a nested program.
                                    Calling <samp class="ph codeph">cudaDeviceSynchronize()</samp> at a depth greater than
                                    the specified maximum synchronization depth will return an error.
                                 </p>
                                 <p class="p">An optimization is permitted where the system detects that it need not
                                    reserve space for the parent's state in cases where the parent kernel
                                    never calls <samp class="ph codeph">cudaDeviceSynchronize()</samp>. In this case,
                                    because explicit parent/child synchronization never occurs, the memory
                                    footprint required for a program will be much less than the conservative
                                    maximum. Such a program could specify a shallower maximum synchronization
                                    depth to avoid over-allocation of backing store.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="pending-kernel-launches"><a name="pending-kernel-launches" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#pending-kernel-launches" name="pending-kernel-launches" shape="rect">D.4.3.1.3.&nbsp;Pending Kernel Launches</a></h3>
                              <div class="body conbody">
                                 <p class="p">When a kernel is launched, all associated configuration and parameter data is tracked
                                    until the kernel completes. This data is stored within a system-managed launch pool.
                                 </p>
                                 <p class="p">The launch pool is divided into a fixed-size pool and a virtualized pool with lower
                                    performance. The device runtime system software will try to track launch data in the
                                    fixed-size pool first. The virtualized pool will be used to track new launches when the
                                    fixed-size pool is full.
                                 </p>
                                 <p class="p">The size of the fixed-size launch pool is configurable by calling
                                    <samp class="ph codeph">cudaDeviceSetLimit()</samp> from the host and specifying
                                    <samp class="ph codeph">cudaLimitDevRuntimePendingLaunchCount</samp>.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="configuration-options"><a name="configuration-options" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#configuration-options" name="configuration-options" shape="rect">D.4.3.1.4.&nbsp;Configuration Options</a></h3>
                              <div class="body conbody">
                                 <p class="p">Resource allocation for the device runtime system software is controlled
                                    via the <samp class="ph codeph">cudaDeviceSetLimit()</samp> API from the host program.
                                    Limits must be set before any kernel is launched, and may not be changed
                                    while the GPU is actively running programs.
                                 </p>
                                 <p class="p">The following named limits may be set:</p>
                                 <div class="tablenoborder">
                                    <table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                       <thead class="thead" align="left">
                                          <tr class="row">
                                             <th class="entry" valign="top" width="55.55555555555556%" id="d225e22815" rowspan="1" colspan="1">Limit</th>
                                             <th class="entry" valign="top" width="44.44444444444444%" id="d225e22818" rowspan="1" colspan="1">Behavior</th>
                                          </tr>
                                       </thead>
                                       <tbody class="tbody">
                                          <tr class="row">
                                             <td class="entry" valign="top" width="55.55555555555556%" headers="d225e22815" rowspan="1" colspan="1"><samp class="ph codeph">cudaLimitDevRuntimeSyncDepth</samp></td>
                                             <td class="entry" valign="top" width="44.44444444444444%" headers="d225e22818" rowspan="1" colspan="1">Sets the maximum depth at which
                                                <samp class="ph codeph">cudaDeviceSynchronize()</samp> may be called.
                                                Launches may be performed deeper than this, but explicit
                                                synchronization deeper than this limit will return the
                                                <samp class="ph codeph">cudaErrorLaunchMaxDepthExceeded</samp>. The default
                                                maximum sync depth is 2.
                                             </td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" valign="top" width="55.55555555555556%" headers="d225e22815" rowspan="1" colspan="1"><samp class="ph codeph">cudaLimitDevRuntimePendingLaunchCount</samp></td>
                                             <td class="entry" valign="top" width="44.44444444444444%" headers="d225e22818" rowspan="1" colspan="1">Controls the amount of memory set aside for buffering
                                                kernel launches which have not yet begun to execute, due either
                                                to unresolved dependencies or lack of execution resources. When
                                                the buffer is full, the device runtime system software will
                                                attempt to track new pending launches in a lower performance
                                                virtualized buffer. If the virtualized buffer is also full,
                                                i.e. when all available heap space is consumed, launches will
                                                not occur, and the thread's last error will be set to
                                                <samp class="ph codeph">cudaErrorLaunchPendingCountExceeded</samp>. The
                                                default pending launch count is 2048 launches. 
                                             </td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" valign="top" width="55.55555555555556%" headers="d225e22815" rowspan="1" colspan="1"><samp class="ph codeph">cudaLimitStackSize</samp></td>
                                             <td class="entry" valign="top" width="44.44444444444444%" headers="d225e22818" rowspan="1" colspan="1">Controls the stack size in bytes of each GPU thread.
                                                The CUDA driver automatically increases the per-thread stack size
                                                for each kernel launch as needed. This size isn't reset back to the
                                                original value after each launch. To set the per-thread stack size 
                                                to a different value, <samp class="ph codeph">cudaDeviceSetLimit()</samp> can be called to 
                                                set this limit. The stack will be immediately resized, and if necessary, 
                                                the device will block until all preceding requested tasks are complete.
                                                <samp class="ph codeph">cudaDeviceGetLimit()</samp> can be called to get the current 
                                                per-thread stack size. 
                                             </td>
                                          </tr>
                                       </tbody>
                                    </table>
                                 </div>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="memory-allocation-and-lifetime"><a name="memory-allocation-and-lifetime" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#memory-allocation-and-lifetime" name="memory-allocation-and-lifetime" shape="rect">D.4.3.1.5.&nbsp;Memory Allocation and Lifetime</a></h3>
                              <div class="body conbody">
                                 <p class="p"><samp class="ph codeph">cudaMalloc()</samp> and <samp class="ph codeph">cudaFree()</samp> have
                                    distinct semantics between the host and device environments. When invoked
                                    from the host, <samp class="ph codeph">cudaMalloc()</samp> allocates a new region from
                                    unused device memory. When invoked from the device runtime these
                                    functions map to device-side <samp class="ph codeph">malloc()</samp> and
                                    <samp class="ph codeph">free()</samp>. This implies that within the device environment
                                    the total allocatable memory is limited to the device
                                    <samp class="ph codeph">malloc()</samp> heap size, which may be smaller than the
                                    available unused device memory. Also, it is an error to invoke
                                    <samp class="ph codeph">cudaFree()</samp> from the host program on a pointer which was
                                    allocated by <samp class="ph codeph">cudaMalloc()</samp> on the device or
                                    vice-versa.
                                 </p>
                                 <div class="tablenoborder">
                                    <table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                       <thead class="thead" align="left">
                                          <tr class="row">
                                             <th class="entry" valign="top" width="30.303030303030305%" id="d225e22923" rowspan="1" colspan="1">&nbsp;</th>
                                             <th class="entry" valign="top" width="30.303030303030305%" id="d225e22925" rowspan="1" colspan="1"><samp class="ph codeph">cudaMalloc()</samp> on Host 
                                             </th>
                                             <th class="entry" valign="top" width="39.3939393939394%" id="d225e22930" rowspan="1" colspan="1"><samp class="ph codeph">cudaMalloc()</samp> on Device 
                                             </th>
                                          </tr>
                                       </thead>
                                       <tbody class="tbody">
                                          <tr class="row">
                                             <td class="entry" valign="top" width="30.303030303030305%" headers="d225e22923" rowspan="1" colspan="1"><samp class="ph codeph">cudaFree()</samp> on Host
                                             </td>
                                             <td class="entry" valign="top" width="30.303030303030305%" headers="d225e22925" rowspan="1" colspan="1">Supported</td>
                                             <td class="entry" valign="top" width="39.3939393939394%" headers="d225e22930" rowspan="1" colspan="1">Not Supported</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" valign="top" width="30.303030303030305%" headers="d225e22923" rowspan="1" colspan="1"><samp class="ph codeph">cudaFree()</samp> on Device
                                             </td>
                                             <td class="entry" valign="top" width="30.303030303030305%" headers="d225e22925" rowspan="1" colspan="1">Not Supported</td>
                                             <td class="entry" valign="top" width="39.3939393939394%" headers="d225e22930" rowspan="1" colspan="1">Supported</td>
                                          </tr>
                                          <tr class="row">
                                             <td class="entry" valign="top" width="30.303030303030305%" headers="d225e22923" rowspan="1" colspan="1">Allocation limit</td>
                                             <td class="entry" valign="top" width="30.303030303030305%" headers="d225e22925" rowspan="1" colspan="1">Free device memory</td>
                                             <td class="entry" valign="top" width="39.3939393939394%" headers="d225e22930" rowspan="1" colspan="1"><samp class="ph codeph">cudaLimitMallocHeapSize</samp></td>
                                          </tr>
                                       </tbody>
                                    </table>
                                 </div>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="sm-id-and-warp-id"><a name="sm-id-and-warp-id" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#sm-id-and-warp-id" name="sm-id-and-warp-id" shape="rect">D.4.3.1.6.&nbsp;SM Id and Warp Id</a></h3>
                              <div class="body conbody">
                                 <p class="p">Note that in PTX <samp class="ph codeph">%smid</samp> and <samp class="ph codeph">%warpid</samp> are
                                    defined as volatile values. The device runtime may reschedule thread
                                    blocks onto different SMs in order to more efficiently manage resources.
                                    As such, it is unsafe to rely upon <samp class="ph codeph">%smid</samp> or
                                    <samp class="ph codeph">%warpid</samp> remaining unchanged across the lifetime of a
                                    thread or thread block.
                                 </p>
                              </div>
                           </div>
                           <div class="topic concept nested4" xml:lang="en-US" id="ecc-errors"><a name="ecc-errors" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#ecc-errors" name="ecc-errors" shape="rect">D.4.3.1.7.&nbsp;ECC Errors</a></h3>
                              <div class="body conbody">
                                 <p class="p">No notification of ECC errors is available to code within a CUDA kernel. ECC errors are
                                    reported at the host side once the entire launch tree has completed. Any ECC errors
                                    which arise during execution of a nested program will either generate an exception or
                                    continue execution (depending upon error and configuration).
                                 </p>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="virtual-memory-management"><a name="virtual-memory-management" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#virtual-memory-management" name="virtual-memory-management" shape="rect">E.&nbsp;Virtual Memory Management</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="introduction-virtual-memory-management"><a name="introduction-virtual-memory-management" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#introduction-virtual-memory-management" name="introduction-virtual-memory-management" shape="rect">E.1.&nbsp;Introduction</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__VA.html" target="_blank" shape="rect">Virtual Memory Management APIs</a></samp>
                           provide a way for the application to directly manage the unified virtual address
                           space that CUDA provides to map physical memory to virtual addresses accessible by
                           the GPU. Introduced in CUDA 10.2, these APIs additionally provide a new way to
                           interop with other processes and graphics APIs like OpenGL and Vulkan, as well as
                           provide newer memory attributes that a user can tune to fit their applications.
                           
                        </p>
                        <p class="p">
                           Historically, memory allocation calls (eg. cudaMalloc) in the CUDA programming model have
                           returned a memory address that points to the GPU memory. The address thus obtained
                           could be used with any CUDA API or inside a device kernel. However, the memory allocated
                           could not be resized depending on the user's memory needs. In order to increase an allocation's
                           size, the user had to explicitly allocate a larger buffer, copy data from the initial allocation, free
                           it and then continue to keep track of the newer allocation's address. This often lead to lower
                           performance and higher peak memory utilization for applications. Essentially, users had a
                           malloc-like interface for allocating GPU memory, but did not have a corresponding realloc to compliment it.
                           The Virtual Memory Management APIs decouple the idea of an address and memory and allow the application to
                           handle them separately. The APIs allow applications to map and unmap memory from a virtual address
                           range as they see fit.
                           
                        </p>
                        <p class="p">
                           In the case of enabling peer device access to memory allocations via <samp class="ph codeph">cudaEnablePeerAccess</samp>,
                           all past and future user allocations are mapped to the target peer device. This lead to users unwittingly
                           paying runtime cost of mapping all cudaMalloc allocations to peer devices. However, in most situations
                           applications communicate by sharing only a few allocations with another device and not all allocations
                           are required to be mapped to all the devices. With Virtual Memory Management applications can specifically
                           choose certain allocations to be accessible from target devices.
                           
                        </p>
                        <div class="p">
                           The CUDA Virtual Memory Management APIs expose fine grained control to the user for managing the
                           GPU memory in applications. It provides APIs that lets users:
                           
                           <ul class="ul">
                              <li class="li"> Place memory allocated on different devices into a contiguous VA range. </li>
                              <li class="li"> Perform interprocess communication for memory sharing using platform specific mechanisms. </li>
                              <li class="li"> Opt into newer memory types on the devices that support them. </li>
                           </ul>
                        </div>
                        <div class="p">
                           In order to allocate memory, the Virtual Memory Management programming model exposes
                           the following functionality:
                           
                           <ul class="ul">
                              <li class="li"> Allocating physical memory. </li>
                              <li class="li"> Reserving a VA range. </li>
                              <li class="li"> Mapping allocated memory to the VA range. </li>
                              <li class="li"> Controlling access rights on the mapped range. </li>
                           </ul>
                        </div>
                        <p class="p">
                           Note that the suite of APIs described in this section require a system
                           that supports UVA.
                           
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="querying-vmm-support"><a name="querying-vmm-support" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#querying-vmm-support" name="querying-vmm-support" shape="rect">E.2.&nbsp;Query for support</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           Before attempting to use Virtual Memory Management APIs, applications must ensure that the device(s) they wish to use
                           support CUDA Virtual Memory Management. The following code sample shows querying for Virtual Memory Management support:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> deviceSupportsVmm;
CUresult result = cuDeviceGetAttribute(&amp;deviceSupportsVmm, CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED, device);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (deviceSupportsVmm != 0) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// `device` supports Virtual Memory Management </span>
}
      </pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="allocating-physical-memory"><a name="allocating-physical-memory" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#allocating-physical-memory" name="allocating-physical-memory" shape="rect">E.3.&nbsp;Allocating Physical Memory</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The first step in memory allocation via Virtual Memory Management APIs is to create a physical memory chunk that
                           will provide a backing for the allocation. In order to allocate physical memory, applications must use the <samp class="ph codeph">cuMemCreate</samp> API.
                           The allocation created by this function does not have any device or host mappings. The function argument
                           <samp class="ph codeph">CUmemGenericAllocationHandle</samp> describes the properties of the memory to allocate such as
                           the location of the allocation, if the allocation is going to be shared to another process (or other Graphics APIs),
                           or the physical attributes of the memory to be allocated. Users must ensure the requested allocation's
                           size must be aligned to appropriate granularity. Information regarding an allocation's granulariy requirements
                           can be queried using <samp class="ph codeph">cuMemGetAllocationGranularity</samp>. The following code snippet shows allocating physical
                           memory with <samp class="ph codeph">cuMemCreate</samp>:
                           
                        </p><pre xml:space="preserve">
CUmemGenericAllocationHandle allocatePhysicalMemory(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> device, size_t size) {
    CUmemAllocationProp prop = {};
    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;
    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;
    prop.location.id = device;
    cuMemGetAllocationGranularity(&amp;granularity, &amp;prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Ensure size matches granularity requirements for the allocation</span>
    size_t padded_size = ROUND_UP(size, granularity);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate physical memory</span>
    CUmemGenericAllocationHandle allocHandle;
    cuMemCreate(&amp;allocHandle, padded_size, &amp;prop, 0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> allocHandle;
}
      </pre><p class="p">
                           The memory allocated by <samp class="ph codeph">cuMemCreate</samp> is referenced by the <samp class="ph codeph">CUmemGenericAllocationHandle</samp> it returns.
                           This is a departure from the cudaMalloc-style of allocation which returns a pointer to the GPU memory which was directly
                           accessible by CUDA kernel executing on the device. The memory allocated cannot be used for any operations other than
                           querying properties using <samp class="ph codeph">cuMemGetAllocationPropertiesFromHandle</samp>. In order to make this memory accessible,
                           applications must map this memory into a VA range reserved by <samp class="ph codeph">cuMemAddressReserve</samp> and provide suitable
                           access rights to it. Applications must free the allocated memory using the <samp class="ph codeph">cuMemRelease</samp> API.
                           
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shareable-physical-memory"><a name="shareable-physical-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shareable-physical-memory" name="shareable-physical-memory" shape="rect">E.3.1.&nbsp;Shareable Memory Allocations</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              With <samp class="ph codeph">cuMemCreate</samp> users now have the facility to indicate to CUDA, at allocation time,
                              that they have earmarked a particular allocation for Inter process communication and/or graphics interop
                              purposes. Applications can do this by setting <samp class="ph codeph">CUmemAllocationProp::requestedHandleTypes</samp>
                              to a platform specific field. On Windows, when <samp class="ph codeph">CUmemAllocationProp::requestedHandleTypes</samp> is
                              set to <samp class="ph codeph">CU_MEM_HANDLE_TYPE_WIN32</samp> applications must also specify a LPSECURITYATTRIBUTES attribute
                              in <samp class="ph codeph">CUmemAllocationProp::win32HandleMetaData</samp>. This security attribute defines the scope of which
                              exported allocations may be tranferred to other processes.
                              
                           </p>
                           <p class="p">
                              The CUDA Virtual Memory Management API functions do not support the legacy interprocess communication functions with
                              their memory. Instead, they expose a new mechanism for interprocess communication that utilizes operating system specific
                              handles. Applications can obtain these OS specific handles corresponding to the allocations by using
                              <samp class="ph codeph">cuMemExportToShareableHandle</samp>. The handles thus obtained can be transferred by using the usual OS native
                              mecahnisms for inter process communication. The recepient process should import the allocation by using
                              <samp class="ph codeph">cuMemImportFromShareableHandle</samp>.
                              
                           </p>
                           <p class="p">
                              Users must ensure they query for support of the requested handle type before attempting to export memory allocated with
                              <samp class="ph codeph">cuMemCreate</samp>. The following code snippet illustrates query for handle type support in a
                              platform specific way.
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> deviceSupportsIpcHandle;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#if defined(__linux__)</span>
    cuDeviceGetAttribute(&amp;deviceSupportsIpcHandle, CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED, device));
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else</span>
    cuDeviceGetAttribute(&amp;deviceSupportsIpcHandle, CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED, device));
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
      </pre><p class="p">
                              Users should set the <samp class="ph codeph">CUmemAllocationProp::requestedHandleTypes</samp> appropriately as shown below:
                              
                           </p><pre xml:space="preserve">
#<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> defined(__linux__)
    prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else</span>
    prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_WIN32;
    prop.win32HandleMetaData = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Windows specific LPSECURITYATTRIBUTES attribute.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
      </pre><p class="p">
                              The <samp class="ph codeph"><a class="xref" href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/memMapIPCDrv" target="_blank" shape="rect">memMapIpcDrv</a></samp> sample can be used as an example for using IPC with Virtual Memory Management allocations.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="physical-memory-type"><a name="physical-memory-type" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#physical-memory-type" name="physical-memory-type" shape="rect">E.3.2.&nbsp;Memory Type</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              Before CUDA 10.2, applications had no user controlled way of allocating any special type of memory that certain
                              devices may support. With <samp class="ph codeph">cuMemCreate</samp> applications can additionally specify memory type requirements using the
                              <samp class="ph codeph">CUmemAllocationProp::allocFlags</samp> to opt into any specific memory features. Applications must also
                              ensure that the requested memory type is supported on the device of allocation. 
                              
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="physical-memory-type-compression"><a name="physical-memory-type-compression" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#physical-memory-type-compression" name="physical-memory-type-compression" shape="rect">E.3.2.1.&nbsp;Compressible Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 Compressible memory can be used to accelerate accesses to data with unstructured sparsity and other compressible data patterns.
                                 Compression can save DRAM bandwidth, L2 read bandwidth and L2 capacity depending on the data being operated on.
                                 Applications that want to allocate compressible memory on devices that support Compute Data Compression can do so
                                 by setting <samp class="ph codeph">CUmemAllocationProp::allocFlags::compressionType</samp> to <samp class="ph codeph">CU_MEM_ALLOCATION_COMP_GENERIC</samp>.
                                 Users  must query if device supports Compute Data Compression by using <samp class="ph codeph">CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED</samp>.
                                 The following code snippet illustrates querying compressible memory support <samp class="ph codeph">cuDeviceGetAttribute</samp>.
                                 
                              </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> compressionSupported = 0;
cuDeviceGetAttribute(&amp;compressionSupported, CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED, device);
      </pre><p class="p">
                                 On devices that support Compute Data Compression, users need to opt in at allocation time as shown below:
                                 
                              </p><pre xml:space="preserve">
prop.allocFlags.compressionType = CU_MEM_ALLOCATION_COMP_GENERIC;
      </pre><p class="p">
                                 Due to various reasons such as limited HW resources, the allocation may not have compression attributes, the user is expected
                                 to
                                 query back the properties of the allocated memory using <samp class="ph codeph">cuMemGetAllocationPropertiesFromHandle</samp> and check for
                                 compression attribute.
                                 
                              </p><pre xml:space="preserve">
CUmemAllocationPropPrivate allocationProp = {};
cuMemGetAllocationPropertiesFromHandle(&amp;allocationProp, allocationHandle);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (allocationProp.allocFlags.compressionType == CU_MEM_ALLOCATION_COMP_GENERIC)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Obtained compressible memory allocation</span>
}
      </pre></div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="address-reservation-and-free"><a name="address-reservation-and-free" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#address-reservation-and-free" name="address-reservation-and-free" shape="rect">E.4.&nbsp;Reserving a Virtual Address Range</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           Since with Virtual Memory Management the notions of address and memory are distinct,
                           applications must carve out an address range that can hold the memory allocations made
                           by <samp class="ph codeph">cuMemCreate</samp>. The address range reserved must be atleast as large as
                           the sum of the sizes of all the physical memory allocations the user plans to place in them.
                           
                        </p>
                        <p class="p">
                           Applications can reserve a virtual address range by passing appropriate
                           parameters to <samp class="ph codeph">cuMemAddressReserve</samp>. The address range obtained
                           will not have any device or host physical memory associated with it. The reserved
                           virtual address range can be mapped to memory chunks belonging to any device in the system,
                           thus provding the application a continuous VA range backed and mapped by memory belonging to
                           different devices. Applications are expected to return the virtual address range back to
                           CUDA using <samp class="ph codeph">cuMemAddressFree</samp>. Users must ensure that the entire VA range is
                           unmapped before calling <samp class="ph codeph">cuMemAddressFree</samp>. These functions are conceptually
                           similar to mmap/munmap(on Linux) or VirtualAlloc/VirtualFree(on Windows) functions. The
                           following code snippet illustrates the usage for the function:
                           
                        </p><pre xml:space="preserve">
CUdeviceptr ptr;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// `ptr` holds the returned start of virtual address range reserved.</span>
CUresult result = cuMemAddressReserve(&amp;ptr, size, 0, 0, 0); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// alignment = 0 for default alignment</span>
      </pre></div>
                  </div>
                  <div class="topic concept nested1" id="virtual-aliasing-support"><a name="virtual-aliasing-support" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#virtual-aliasing-support" name="virtual-aliasing-support" shape="rect">E.5.&nbsp;Virtual Aliasing Support</a></h3>
                     <div class="body conbody">
                        <p class="p">The Virtual Memory Management APIs provide a way to create multiple virtual memory mappings or
                           views to the same allocation using multiple calls to <samp class="ph codeph">cuMemMap</samp> with
                           different virtual addresses, so-called virtual aliasing. Unless otherwise noted in the PTX
                           ISA, writes to one view of the allocation are considered inconsistent and incoherent with
                           any other view of the same memory until the writing device operation (grid launch, memcpy,
                           memset, etc.) completes. Grids present on the GPU prior to a writing device operation but
                           reading after the writing device operation completes are also considered to have
                           inconsistent and incoherent views.
                        </p>
                        <div class="p">For example, the following snippet is considered undefined, assuming device pointers A
                           and B are virtual aliases of the same memory
                           allocation:<pre xml:space="preserve">__global__ void foo(char *A, char *B) {
  *A = 0x1;
  printf(%d\n, *B);    // Undefined behavior!  *B can take on either
// the previous value or some value in-between.
}</pre></div>
                        <div class="p">The following is defined behavior, assuming these two kernels are ordered monotonically
                           (via streams or
                           events).<pre xml:space="preserve">__global__ void foo1(char *A) {
  *A = 0x1;
}

__global__ void foo2(char *B) {
  printf(%d\n, *B);    // *B == *A == 0x1 assuming foo2 waits for foo1
// to complete before launching
}

cudaMemcpyAsync(B, input, size, stream1);    // Aliases are allowed at
// operation boundaries
foo1&lt;&lt;&lt;1,1,0,stream1&gt;&gt;&gt;(A);                  // allowing foo1 to access A.
cudaEventRecord(event, stream1);
cudaStreamWaitEvent(stream2, event);
foo2&lt;&lt;&lt;1,1,0,stream2&gt;&gt;&gt;(B);
cudaStreamWaitEvent(stream3, event);
cudaMemcpyAsync(output, B, size, stream3);  // Both launches of foo2 and
                                            // cudaMemcpy (which both
                                            // read) wait for foo1 (which writes)
                                            // to complete before proceeding</pre></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="mapping-memory"><a name="mapping-memory" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#mapping-memory" name="mapping-memory" shape="rect">E.6.&nbsp;Mapping Memory</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The allocated physical memory and the carved out virtual address space from the previous two sections
                           represent the memory and address distinction introduced by the Virtual Memory Management APIs. For the
                           allocated memory to be useable, the user must first place the memory in the address space. The address range
                           obtained from <samp class="ph codeph">cuMemAddressReserve</samp> and the physical allocation obtained from
                           <samp class="ph codeph">cuMemCreate</samp> or <samp class="ph codeph">cuMemImportFromShareableHandle</samp> must be associated with each
                           other by using <samp class="ph codeph">cuMemMap</samp>.  
                           
                        </p>
                        <p class="p">
                           Users can associate allocations from multiple devices to reside in contiguous virtual address ranges as
                           long as they have carved out enough address space. In order to decouple the physical allocation and the
                           address range users must unmap the address of the mapping by uisng <samp class="ph codeph">cuMemUnmap</samp>. Users can
                           map and unmap memory to the same address range as many times as they want, as long as they ensure they don't
                           attempt to create mappings on a VA range reservations that are already mapped. The following code snippet
                           illustrates the usage for the function:
                           
                        </p><pre xml:space="preserve">
CUdeviceptr ptr;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// `ptr`: address in the address range previously reserved by cuMemAddressReserve.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// `allocHandle`: CUmemGenericAllocationHandle obtained by a previous call to cuMemCreate. </span>
CUresult result = cuMemMap(ptr, size, 0, allocHandle, 0);
      </pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="accessing-memory"><a name="accessing-memory" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#accessing-memory" name="accessing-memory" shape="rect">E.7.&nbsp;Control Access Rights</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The Virtual Memory Management APIs enable applications to explicitly protect their VA ranges
                           with access control mechanisms. Mapping the allocation to a region of the address range
                           using <samp class="ph codeph">cuMemMap</samp> does not make the address accessible, and would result in a
                           program crash if accessed by a CUDA kernel. Users must specifically select access control using the
                           <samp class="ph codeph">cuMemSetAccess</samp> function, which allows or restricts access for specific devices
                           to a mapped address range. The following code snippet illustrates the usage for the function:
                           
                        </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> setAccessOnDevice(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> device, CUdeviceptr ptr, size_t size) {
    CUmemAccessDesc accessDesc = {};
    accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;
    accessDesc.location.id = device;
    accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Make the address accessible</span>
    cuMemSetAccess(ptr, size, &amp;accessDesc, 1);
}
      </pre><p class="p">
                           The access control mechanism exposed with Virtual Memory Management allows users to be explicit
                           about which allocations they wish to share with other peer devices on the system. As specified
                           earlier, <samp class="ph codeph">cudaEnablePeerAccess</samp> forces all prior and future cudaMalloc'd allocations
                           to be mapped to the target peer device. This can be convenient in many cases as user doesn't have to
                           worry about tracking the mapping state of every allocation to every device in the system. But for
                           users concerned with performance of their applications this approach <a class="xref" href="https://devblogs.nvidia.com/introducing-low-level-gpu-virtual-memory-management/" target="_blank" shape="rect">has performance implications</a>. With access control at allocation
                           granularity Virtual Memory Mangement exposes a mechanism to have peer mappings with minimal overhead.
                           
                        </p>
                        <p class="p">
                           The <samp class="ph codeph"><a class="xref" href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAddMMAP" target="_blank" shape="rect">vectorAddMMAP</a></samp> sample can be used as an example for using the Virtual Memory Management APIs.
                           
                        </p>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="stream-ordered-memory-allocator"><a name="stream-ordered-memory-allocator" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#stream-ordered-memory-allocator" name="stream-ordered-memory-allocator" shape="rect">F.&nbsp;Stream Ordered Memory Allocator</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-memory-allocator-intro"><a name="stream-ordered-memory-allocator-intro" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-memory-allocator-intro" name="stream-ordered-memory-allocator-intro" shape="rect">F.1.&nbsp;Introduction</a></h3>
                     <div class="body conbody">
                        <p class="p">The Stream Order
                           Memory Allocator enables applications to order memory allocation and deallocation with other
                           work launched into a CUDA stream such as kernel launches and asynchronous copies. This
                           improves application memory use by taking advantage of stream-ordering semantics to reuse
                           memory allocations. The allocator also allows applications to control the allocators memory
                           caching behavior. When set up with an appropriate release threshold, the caching behavior
                           allows the allocator to avoid expensive calls into the OS when the application indicates it
                           is willing to accept a bigger memory footprint. The allocator also supports the easy and
                           secure sharing of allocations between processes.
                        </p>
                        <p class="p">For many applications, the Stream Ordered Memory Allocator reduces the need for custom
                           memory management abstractions, and makes it easier to create high-performance custom memory
                           management for applications that need it. For applications and libraries that already have
                           custom memory allocators, adopting the Stream Ordered Memory Allocator enables multiple
                           libraries to share a common pool of memory managed by the driver, thus reducing excess
                           memory consumption. Additionally, the driver can perform optimizations based on its
                           awareness of the allocator and other stream management apis. Finally, Nsight Compute and the
                           Next-Gen CUDA debugger is aware of the allocator as part of their CUDA 11.3 toolkit
                           support.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-querying-memory-support"><a name="stream-ordered-querying-memory-support" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-querying-memory-support" name="stream-ordered-querying-memory-support" shape="rect">F.2.&nbsp;Query for Support</a></h3>
                     <div class="body conbody">
                        <p class="p">The user can determine whether or not a device supports the stream ordered memory allocator
                           by calling <samp class="ph codeph">cudaDeviceGetAttribute()</samp> with the device attribute
                           <samp class="ph codeph">cudaDevAttrMemoryPoolsSupported</samp>.
                        </p>
                        <p class="p">Starting with CUDA 11.3, IPC memory pool support can be queried with the
                           <samp class="ph codeph">cudaDevAttrMemoryPoolSupportedHandleTypes</samp> device attribute. Previous
                           drivers will return <samp class="ph codeph">cudaErrorInvalidValue</samp> as those drivers are unaware of
                           the attribute enum.
                        </p><pre xml:space="preserve">
int deviceSupportsMemoryPools;
cudaError_t result = cudaDeviceGetAttribute(&amp;deviceSupportsMemoryPools,
          cudaDevAttrMemoryPoolsSupported, device);
if (result == CUDA_SUCCESS &amp;&amp; deviceSupportsMemoryPools != 0) {
    // `device` supports the Stream Ordered Memory Allocator
}
int poolSupportedHandleTypes;
result = cudaDeviceGetAttribute(&amp;poolSupportedHandleTypes,
          cudaDevAttrMemoryPoolSupportedHandleTypes, device);
if (result == CUDA_SUCCESS &amp;&amp;
         (poolSupportedHandleTypes &amp; cudaMemHandleTypePosixFileDescriptor)) {
   // Pools on the specified device can be created with posix file descriptor-based IPC
}</pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-api-fundamentals"><a name="stream-ordered-api-fundamentals" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-api-fundamentals" name="stream-ordered-api-fundamentals" shape="rect">F.3.&nbsp;API Fundamentals (cudaMalloc and cudaFreeAsync)</a></h3>
                     <div class="body conbody">
                        <p class="p">The APIs <samp class="ph codeph">cudaMallocAsync</samp> and <samp class="ph codeph">cudaFreeAsync</samp> form the core
                           of the allocator. <samp class="ph codeph">cudaMallocAsync</samp> returns an allocation and
                           <samp class="ph codeph">cudaFreeAsync</samp> frees an allocation. Both APIs accept stream arguments to
                           define when the allocation will become and stop being available for use. The pointer value
                           returned by <samp class="ph codeph">cudaMallocAsync</samp> is determined synchronously and is available
                           for constructing future work. It is important to note that <samp class="ph codeph">cudaMallocAsync</samp>
                           ignores the current device/context when determining where the allocation will reside.
                           Instead, <samp class="ph codeph">cudaMallocAsync</samp> determines the resident device based on the
                           specified memory pool or the supplied stream. The simplest use pattern is when the memory is
                           allocated, used, and freed back into the same stream.
                        </p><pre xml:space="preserve">void *ptr;
size_t size = 512;
cudaMallocAsync(&amp;ptr, size, cudaStreamPerThread);
// do work using the allocation
kernel&lt;&lt;&lt;..., cudaStreamPerThread&gt;&gt;&gt;(ptr, ...);
// An asynchronous free can be specified without synchronizing the cpu and GPU
cudaFreeAsync(ptr, cudaStreamPerThread);</pre><p class="p">When using an
                           allocation in a stream other than the allocating stream, the user must guarantee that the
                           access will happen after the allocation operation, otherwise the behavior is undefined. The
                           user may make this guarantee either by synchronizing the allocating stream, or by using cuda
                           events to synchronize the producing and consuming streams.
                        </p>
                        <p class="p"><samp class="ph codeph">cudaFreeAsync()</samp> inserts a free operation into the stream. The
                           user must guarantee that the free operation happens after the allocation operation and any
                           use of the allocation. Also, any use of the allocation after the free operation starts
                           results in undefined behavior. Events and/or stream synchronizing operations should be used
                           to guarantee any access to the allocation on other streams is complete before the freeing
                           stream begins the free operation.
                        </p>
                        <div class="p"><pre xml:space="preserve">cudaMallocAsync(&amp;ptr, size, stream1);
cudaEventRecord(event1, stream1);
//stream2 must wait for the allocation to be ready before accessing
cudaStreamWaitEvent(stream2, event1);
kernel&lt;&lt;&lt;..., stream2&gt;&gt;&gt;(ptr, ...);
cudaEventRecord(event2, stream2);
// stream3 must wait for stream2 to finish accessing the allocation before
// freeing the allocation
cudaStreamWaitEvent(stream3, event2);
cudaFreeAsync(ptr, stream3);</pre></div>
                        <div class="p">The user can free allocations allocated with <samp class="ph codeph">cudaMalloc()</samp> with
                           <samp class="ph codeph">cudaFreeAsync()</samp>. The user must make the same guarantees about accesses
                           being complete before the free operation
                           begins.<pre xml:space="preserve">cudaMalloc(&amp;ptr, size);
kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...);
cudaFreeAsync(ptr, stream);</pre></div>
                        <div class="p">The user can free memory allocated with <samp class="ph codeph">cudaMallocAsync</samp> with
                           <samp class="ph codeph">cudaFree()</samp>. When freeing such allocations through the
                           <samp class="ph codeph">cudaFree()</samp> API, the driver assumes that all accesses to the allocation
                           are complete and performs no further synchronization. The user can use
                           <samp class="ph codeph">cudaStreamQuery</samp> / <samp class="ph codeph">cudaStreamSynchronize</samp> /
                           <samp class="ph codeph">cudaEventQuery</samp> / <samp class="ph codeph">cudaEventSynchronize</samp> /
                           <samp class="ph codeph">cudaDeviceSynchronize</samp> to guarantee that the appropriate asynchronous work
                           is complete and that the GPU will not try to access the
                           allocation.<pre xml:space="preserve">cudaMallocAsync(&amp;ptr, size,stream);
kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...);
// synchronize is needed to avoid prematurely freeing the memory
cudaStreamSynchronize(stream);
cudaFree(ptr);</pre></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-memory-pools"><a name="stream-ordered-memory-pools" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-memory-pools" name="stream-ordered-memory-pools" shape="rect">F.4.&nbsp;Memory Pools and the cudaMemPool_t</a></h3>
                     <div class="body conbody">
                        <p class="p">Memory pools
                           encapsulate virtual address and physical memory resources that are allocated and managed
                           according to the pools attributes and properties. The primary aspect of a memory pool is the
                           kind and location of memory it manages.
                        </p>
                        <p dir="ltr" class="p">All calls to <samp class="ph codeph">cudaMallocAsync</samp> use the resources of a memory pool.
                           In the absence of a specified memory pool, cudaMallocAsync api uses the current memory pool
                           of the supplied stream's device. The current memory pool for a device may be set with
                           <samp class="ph codeph">cudaDeviceSetMempool</samp> and queried with
                           <samp class="ph codeph">cudaDeviceGetMempool</samp>. By default (in the absence of a
                           <samp class="ph codeph">cudaDeviceSetMempool</samp> call), the current memory pool is the default memory
                           pool of a device. The API <samp class="ph codeph">cudaMallocFromPoolAsync</samp> and &lt;hyperlink&gt;c++
                           overloads of cudaMallocAsync&lt;\hyperlink&gt; allow a user to specify the pool to be used for
                           an allocation without setting it as the current pool. The APIs
                           <samp class="ph codeph">cudaDeviceGetDefaultMempool</samp> and <samp class="ph codeph">cudaMemPoolCreate</samp> give
                           users handles to memory pools.
                        </p>
                        <div class="p">
                           <div class="note note"><span class="notetitle">Note:</span> The mempool current to a device will be local to that device. So allocating without
                              specifying a memory pool will always yield an allocation local to the streams
                              device.
                           </div>
                        </div>
                        <div class="note note"><span class="notetitle">Note:</span><samp class="ph codeph">cudaMemPoolSetAttribute</samp> and <samp class="ph codeph">cudaMemPoolGetAttribute</samp>
                           control the attributes of the memory pools.
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-default-implicit"><a name="stream-ordered-default-implicit" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-default-implicit" name="stream-ordered-default-implicit" shape="rect">F.5.&nbsp;Default/Impicit Pools</a></h3>
                     <div class="body conbody">
                        <p class="p">The default memory pool of a device may be retrieved with the
                           <samp class="ph codeph">cudaDeviceGetDefaultMempool</samp> API. Allocations from the default memory pool
                           of a device are non-migratable device allocation located on that device. These allocations
                           will always be accessible from that device. The accessibility of the default memory pool may
                           be modified with <samp class="ph codeph">cudaMemPoolSetAccess</samp> and queried by
                           <samp class="ph codeph">cudaMemPoolGetAccess</samp>. Since the default pools do not need to be
                           explicitly created, they are sometimes referred to as implicit pools. The default memory
                           pool of a device does not support IPC.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-explicit-pools"><a name="stream-ordered-explicit-pools" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-explicit-pools" name="stream-ordered-explicit-pools" shape="rect">F.6.&nbsp;Explicit Pools</a></h3>
                     <div class="body conbody">
                        <p class="p">The API <samp class="ph codeph">cudaMemPoolCreate</samp> creates an explicit pool. Currently memory pools
                           can only allocate device allocations. The device the allocations will be resident on must be
                           designated in the properties structure. The primary use case for explicit pools is IPC
                           capability.
                        </p><pre xml:space="preserve">
// create a pool similar to the implicit pool on device 0
int device = 0;
cudaMemPoolProps poolProps = { };
poolProps.allocType = cudaMemAllocationTypePinned;
poolProps.location.id = device;
poolProps.location.type = cudaMemLocationTypeDevice;

cudaMemPoolCreate(&amp;memPool, &amp;poolProps));</pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-physical-page-caching-behavior"><a name="stream-ordered-physical-page-caching-behavior" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-physical-page-caching-behavior" name="stream-ordered-physical-page-caching-behavior" shape="rect">F.7.&nbsp;Physical Page Caching Behavior</a></h3>
                     <div class="body conbody">
                        <p class="p">By default, the
                           allocator tries to minimize the physical memory owned by a pool. To minimize the OS calls to
                           allocate and free physical memory, applications must configure a memory footprint for each
                           pool. Applications can do this with the release threshold attribute
                           (<samp class="ph codeph">cudaMemPoolAttrReleaseThreshold</samp>).
                        </p>
                        <p class="p">The release threshold is the amount of memory in bytes a pool should hold onto
                           before trying to release memory back to the OS. When more than the release threshold bytes
                           of memory are held by the memory pool, the allocator will try to release memory back to the
                           OS on the next call to stream, event or device synchronize. Setting the release threshold to
                           UINT64_MAX will prevent the driver from attempting to shrink the pool after every
                           synchronization.
                        </p><pre xml:space="preserve">cuuint64_t setVal = UINT64_MAX;
cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReleaseThreshold, &amp;setVal);</pre><div class="p">Applications that set <samp class="ph codeph">cudaMemPoolAttrReleaseThreshold</samp> high enough to
                           effectively disable memory pool shrinking may wish to explicitly shrink a memory pool's
                           memory footprint. cudaMemPoolTrimTo allows such applications to do so. When trimming a
                           memory pools footprint, the <samp class="ph codeph">minBytesToKeep</samp> parameter allows an application
                           to hold onto an amount of memory it expects to need in a subsequent phase of
                           execution.<pre xml:space="preserve">cuuint64_t setVal = UINT64_MAX;
cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReleaseThreshold, &amp;setVal);

// application phase needing a lot of memory from the stream ordered allocator
for (i=0; i&lt;10; i++) {
    for (j=0; j&lt;10; j++) {
        cudaMallocAsync(&amp;ptrs[j],size[j], stream);
    }
    kernel&lt;&lt;&lt;...,stream&gt;&gt;&gt;(ptrs,...);
    for (j=0; j&lt;10; j++) {
        cudaFreeAsync(ptrs[j], stream);
    }
}

// Process does not need as much memory for the next phase.
// Synchronize so that the trim operation will know that the allocations are no 
// longer in use.
cudaStreamSynchronize(stream);
cudaMemPoolTrimTo(mempool, 0);

// Some other process/allocation mechanism can now use the physical memory 
// released by the trimming operation.</pre></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-resource-usage-statisitics"><a name="stream-ordered-resource-usage-statisitics" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-resource-usage-statisitics" name="stream-ordered-resource-usage-statisitics" shape="rect">F.8.&nbsp;Resource Usage Statistics</a></h3>
                     <div class="body conbody">
                        <p class="p">In CUDA 11.3, the
                           pool attributes <samp class="ph codeph">cudaMemPoolAttrReservedMemCurrent</samp>,
                           <samp class="ph codeph">cudaMemPoolAttrReservedMemHigh</samp>,
                           <samp class="ph codeph">cudaMemPoolAttrUsedMemCurrent</samp>, and
                           <samp class="ph codeph">cudaMemPoolAttrUsedMemHigh</samp> were added to query the memory usage of a
                           pool.
                        </p>
                        <p class="p">Querying the <samp class="ph codeph">cudaMemPoolAttrReservedMemCurrent</samp> attribute of a
                           pool reports the current total physical GPU memory consumed by the pool. Querying the
                           <samp class="ph codeph">cudaMemPoolAttrUsedMemCurrent</samp> of a pool returns the total size of all of
                           the memory allocated from the pool and not available for reuse. 
                        </p>
                        <p class="p">The<samp class="ph codeph"> cudaMemPoolAttr*MemHigh</samp> attributes are watermarks recording
                           the max value achieved by the respective <samp class="ph codeph">cudaMemPoolAttr*MemCurrent</samp>
                           attribute since last reset. They can be reset to the current value by using the
                           <samp class="ph codeph">cudaMemPoolSetAttribute</samp> API.
                        </p><pre xml:space="preserve">// sample helper functions for getting the usage statistics in bulk
struct usageStatistics {
    cuuint64_t reserved;
    cuuint64_t reservedHigh;
    cuuint64_t used;
    cuuint64_t usedHigh;
};

void getUsageStatistics(cudaMemoryPool_t memPool, struct usageStatistics *statistics)
{
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrReservedMemCurrent, statistics-&gt;reserved);
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrReservedMemHigh, statistics-&gt;reservedHigh);
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrUsedMemCurrent, statistics-&gt;used);
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrUsedMemHigh, statistics-&gt;usedHigh);
}


// resetting the watermarks will make them take on the current value.
void resetStatistics(cudaMemoryPool_t memPool)
{
    cuuint64_t value = 0;
    cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReservedMemHigh, &amp;value);
    cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrUsedMemHigh, &amp;value);
}</pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-memory-reuse-policies"><a name="stream-ordered-memory-reuse-policies" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-memory-reuse-policies" name="stream-ordered-memory-reuse-policies" shape="rect">F.9.&nbsp;Memory Reuse Policies</a></h3>
                     <div class="body conbody">
                        <p class="p">In order to service an allocation request, the driver attempts to reuse memory that was
                           previously freed via <samp class="ph codeph">cudaFreeAsync()</samp> before attempting to allocate more
                           memory from the OS. For example, memory freed in a stream can immediately be reused for a
                           subsequent allocation request in the same stream. Similarly, when a stream is synchronized
                           with the CPU, the memory that was previously freed in that stream becomes available for
                           reuse for an allocation in any stream.
                        </p>
                        <p class="p">The stream ordered allocator has a few controllable allocation policies. The pool
                           attributes <samp class="ph codeph">cudaMemPoolReuseFollowEventDependencies</samp>,
                           <samp class="ph codeph">cudaMemPoolReuseAllowOpportunistic</samp>, and
                           <samp class="ph codeph">cudaMemPoolReuseAllowInternalDependencies</samp> control these policies.
                           Upgrading to a newer CUDA driver may change, enhance, augment and/or reorder the reuse
                           policies.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-mempoolreusefollow"><a name="stream-ordered-mempoolreusefollow" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-mempoolreusefollow" name="stream-ordered-mempoolreusefollow" shape="rect">F.9.1.&nbsp;cudaMemPoolReuseFollowEventDependencies</a></h3>
                        <div class="body conbody">
                           <div class="p">Before allocating
                              more physical GPU memory, the allocator examines dependency information established by CUDA
                              events and tries to allocate from memory freed in another stream.
                              <pre xml:space="preserve">cudaMallocAsync(&amp;ptr, size, originalStream);
kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...);
cudaFreeAsync(ptr, originalStream);
cudaEventRecord(event,originalStream);

// waiting on the event that captures the free in another stream 
// allows the allocator to reuse the memory to satisfy 
// a new allocation request in the other stream when
// cudaMemPoolReuseFollowEventDependencies is enabled.
cudaStreamWaitEvent(otherStream, event);
cudaMallocAsync(&amp;ptr2, size, otherStream);
</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-mempoolreuseallowopportunistic"><a name="stream-ordered-mempoolreuseallowopportunistic" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-mempoolreuseallowopportunistic" name="stream-ordered-mempoolreuseallowopportunistic" shape="rect">F.9.2.&nbsp;cudaMemPoolReuseAllowOpportunistic</a></h3>
                        <div class="body conbody">
                           <div class="p">According to the cudaMemPoolReuseAllowOpportunistic policy, the allocator examines freed
                              allocations to see if the frees stream order semantic has been met (ie. the stream has
                              passed the point of execution indicated by the free). When this is disabled, the allocator
                              will still reuse memory made available when a stream is synchronized with the cpu. Disabling
                              this policy does not stop the cudaMemPoolReuseFollowEventDependencies from
                              applying.<pre xml:space="preserve">cudaMallocAsync(&amp;ptr, size, originalStream);
kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...);
cudaFreeAsync(ptr, originalStream);


// after some time, the kernel finishes running
wait(10);

// When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request
// can be fulfilled with the prior allocation based on the progress of originalStream.
cudaMallocAsync(&amp;ptr2, size, otherStream);</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-mempoolreuseallowinternal"><a name="stream-ordered-mempoolreuseallowinternal" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-mempoolreuseallowinternal" name="stream-ordered-mempoolreuseallowinternal" shape="rect">F.9.3.&nbsp;cudaMemPoolReuseAllowInternalDependencies</a></h3>
                        <div class="body conbody">
                           <div class="p">Failing to allocate and map more physical memory from the OS, the driver will look for
                              memory whose availability depends on another stream's pending progress. If such memory is
                              found, the driver will insert the required dependency into the allocating stream and reuse
                              the
                              memory.<pre xml:space="preserve">cudaMallocAsync(&amp;ptr, size, originalStream);
kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...);
cudaFreeAsync(ptr, originalStream);

// When cudaMemPoolReuseAllowInternalDependencies is enabled
// and the driver fails to allocate more physical memory, the driver may
// effectively perform a cudaStreamWaitEvent in the allocating stream
// to make sure that future work in otherStream happens after the work
// in the original stream that would be allowed to access the original allocation. 
cudaMallocAsync(&amp;ptr2, size, otherStream);</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-disabling-reuse-policies"><a name="stream-ordered-disabling-reuse-policies" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-disabling-reuse-policies" name="stream-ordered-disabling-reuse-policies" shape="rect">F.9.4.&nbsp;Disabling Reuse Policies</a></h3>
                        <div class="body conbody">
                           <p class="p">While the controllable reuse policies improve memory reuse, users may want to disable them.
                              Allowing opportunistic reuse (i.e. <samp class="ph codeph">cudaMemPoolReuseAllowOpportunistic</samp>)
                              introduces run to run variance in allocation patterns based on the interleaving of cpu and
                              GPU execution. Internal dependency insertion (i.e.
                              <samp class="ph codeph">cudaMemPoolReuseAllowInternalDependencies</samp>) can serialize work in
                              unexpected and potentially non-deterministic ways when the user would rather explicitly
                              synchronize an event or stream on allocation failure.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-deviceaccessibility"><a name="stream-ordered-deviceaccessibility" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-deviceaccessibility" name="stream-ordered-deviceaccessibility" shape="rect">F.10.&nbsp;Device Accessibility for Multi-GPU Support</a></h3>
                     <div class="body conbody">
                        <p class="p">Just like allocation accessibility controlled through the virtual memory management APIs,
                           memory pool allocation accessibility does not follow
                           <samp class="ph codeph">cudaDeviceEnablePeerAccess</samp> or <samp class="ph codeph">cuCtxEnablePeerAccess</samp>.
                           Instead, the api <samp class="ph codeph">cudaMemPoolSetAccess</samp> modifies what devices can access
                           allocations from a pool. By default, allocations are accessible from the device where the
                           allocations are located. This access cannot be revoked. To enable access from other devices,
                           the accessing device must be peer capable with the memory pool's device; check with
                           <samp class="ph codeph">cudaDeviceCanAccessPeer</samp>. If the peer capability is not checked, the set
                           access may fail with <samp class="ph codeph">cudaErrorInvalidDevice</samp>. If no allocations had been
                           made from the pool, the <samp class="ph codeph">cudaMemPoolSetAccess</samp> call may succeed even when the
                           devices are not peer capable; in this case, the next allocation from the pool will fail.
                        </p>
                        <div class="p">It is worth noting that <samp class="ph codeph">cudaMemPoolSetAccess</samp> affects all allocations from
                           the memory pool, not just future ones. Also the accessibility reported by
                           <samp class="ph codeph">cudaMemPoolGetAccess</samp> applies to all allocations from the pool, not just
                           future ones. It is recommended that the accessibility settings of a pool for a given GPU not
                           be changed frequently; once a pool is made accessible from a given GPU, it should remain
                           accessible from that GPU for the lifetime of the
                           pool.<pre xml:space="preserve">// snippet showing usage of cudaMemPoolSetAccess:
cudaError_t setAccessOnDevice(cudaMemPool_t memPool, int residentDevice,
              int accessingDevice) {
    cudaMemAccessDesc accessDesc = {};
    accessDesc.location.type = cudaMemLocationTypeDevice;
    accessDesc.location.id = accessingDevice;
    accessDesc.flags = cudaMemAccessFlagsProtReadWrite;

    int canAccess = 0;
    cudaError_t error = cudaDeviceCanAccessPeer(&amp;canAccess, accessingDevice,
              residentDevice);
    if (error != cudaSuccess) {
        return error;
    } else if (canAccess == 0) {
        return cudaErrorPeerAccessUnsupported;
    }

    // Make the address accessible
    return cudaMemPoolSetAccess(memPool, &amp;accessDesc, 1);
}</pre></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-ipc-memory-pools"><a name="stream-ordered-ipc-memory-pools" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-ipc-memory-pools" name="stream-ordered-ipc-memory-pools" shape="rect">F.11.&nbsp;IPC Memory Pools</a></h3>
                     <div class="body conbody">
                        <p class="p">IPC capable memory pools allow easy, efficient and secure sharing of GPU memory between
                           processes. CUDA's IPC memory pools provide the same security benefits as CUDA's virtual
                           memory management APIs.
                        </p>
                        <p class="p">There are two phases to sharing memory between processes with memory pools. The processes
                           first need to share access to the pool, then share specific allocations from that pool. The
                           first phase establishes and enforces security. The second phase coordinates what virtual
                           addresses are used in each process and when mappings need to be valid in the importing
                           process.
                        </p>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-creating-and-sharing-ipc-memory-pools"><a name="stream-ordered-creating-and-sharing-ipc-memory-pools" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-creating-and-sharing-ipc-memory-pools" name="stream-ordered-creating-and-sharing-ipc-memory-pools" shape="rect">F.11.1.&nbsp;Creating and Sharing IPC Memory Pools</a></h3>
                        <div class="body conbody">
                           <div class="p">Sharing access to a pool involves retrieving an OS native handle to the pool (with the
                              <samp class="ph codeph">cudaMemPoolExportToShareableHandle()</samp> API), transferring the handle to the
                              importing process using the usual OS native IPC mechanisms, and creating an imported memory
                              pool (with the <samp class="ph codeph">cudaMemPoolImportFromShareableHandle()</samp> API). For
                              <samp class="ph codeph">cudaMemPoolExportToShareableHandle</samp> to succeed, the memory pool had to be
                              created with the requested handle type specified in the pool properties structure. Please
                              reference samples for the appropriate IPC mechanisms to transfer the OS native handle
                              between processes. The rest of the procedure can be found in the following code
                              snippets.<pre xml:space="preserve"> // in exporting process
// create an exportable IPC capable pool on device 0
cudaMemPoolProps poolProps = { };
poolProps.allocType = cudaMemAllocationTypePinned;
poolProps.location.id = 0;
poolProps.location.type = cudaMemLocationTypeDevice;

// Setting handleTypes to a non zero value will make the pool exportable (IPC capable)
poolProps.handleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR;

cudaMemPoolCreate(&amp;memPool, &amp;poolProps));

// FD based handles are integer types
int fdHandle = 0;


// Retrieve an OS native handle to the pool.
// Note that a pointer to the handle memory is passed in here.
 cudaMemPoolExportToShareableHandle(&amp;fdHandle,
             memPool,
             CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR,
             0);

// The handle must be sent to the importing process with the appropriate
// OS specific APIs.</pre><pre xml:space="preserve">// in importing process
int fdHandle;
// The handle needs to be retrieved from the exporting process with the
// appropriate OS specific APIs.
// Create an imported pool from the shareable handle.
// Note that the handle is passed by value here. 
cudaMemPoolImportFromShareableHandle(&amp;importedMemPool,
          (void*)fdHandle,
          CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR,
          0);</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-set-access-importing-process"><a name="stream-ordered-set-access-importing-process" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-set-access-importing-process" name="stream-ordered-set-access-importing-process" shape="rect">F.11.2.&nbsp;Set Access in the Importing Process</a></h3>
                        <div class="body conbody">
                           <p class="p">Imported memory pools are initially only accessible from their resident device. The
                              imported memory pool does not inherit any accessibility set by the exporting process. The
                              importing process needs to enable access (with <samp class="ph codeph">cudaMemPoolSetAccess</samp>) from
                              any GPU it plans to access the memory from.
                           </p>
                           <p class="p">If the imported memory pool belongs to a non-visible device in the importing process, the
                              user must use the <samp class="ph codeph">cudaMemPoolSetAccess</samp> API to enable access from the GPUs
                              the allocations will be used on.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-allocations-from-exported-pool"><a name="stream-ordered-allocations-from-exported-pool" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-allocations-from-exported-pool" name="stream-ordered-allocations-from-exported-pool" shape="rect">F.11.3.&nbsp;Creating and Sharing Allocations from an Exported Pool</a></h3>
                        <div class="body conbody">
                           <p class="p">Once the pool has
                              been shared, allocations made with <samp class="ph codeph">cudaMallocAsync()</samp> from the pool in the
                              exporting process can be shared with other processes that have imported the pool. Since the
                              pool's security policy is established and verified at the pool level, the OS does not need
                              extra bookkeeping to provide security for specific pool allocations; In other words, the
                              opaque <samp class="ph codeph">cudaMemPoolPtrExportData</samp> required to import a pool allocation may be
                              sent to the importing process using any mechanism.
                           </p>
                           <div class="p">While allocations may be exported and even imported without synchronizing with the
                              allocating stream in any way, the importing process must follow the same rules as the
                              exporting process when accessing the allocation. Namely, access to the allocation must
                              happen after the stream ordering of the allocation operation in the allocating stream. The
                              two following code snippets show <samp class="ph codeph">cudaMemPoolExportPointer()</samp> and
                              <samp class="ph codeph">cudaMemPoolImportPointer()</samp> sharing the allocation with an ipc event used
                              to guarantee that the allocation isnt accessed in the importing process before the
                              allocation is
                              ready.<pre xml:space="preserve">// preparing an allocation in the exporting process
cudaMemPoolPtrExportData exportData;
cudaEvent_t readyIpcEvent;
cudaIpcEventHandle_t readyIpcEventHandle;

// ipc event for coordinating between processes
// cudaEventInterprocess flag makes the event an ipc event
// cudaEventDisableTiming  is set for performance reasons

cudaEventCreate(
        &amp;readyIpcEvent, cudaEventDisableTiming | cudaEventInterprocess)

// allocate from the exporting mem pool
cudaMallocAsync(&amp;ptr, size,exportMemPool, stream);

// event for sharing when the allocation is ready.
cudaEventRecord(readyIpcEvent, stream);
cudaMemPoolExportPointer(&amp;exportData, ptr);
cudaIpcGetEventHandle(&amp;readyIpcEventHandle, readyIpcEvent);

// Share IPC event and pointer export data with the importing process using
//  any mechanism. Here we copy the data into shared memory
shmem-&gt;ptrData = exportData;
shmem-&gt;readyIpcEventHandle = readyIpcEventHandle;
// signal consumers data is ready</pre><pre xml:space="preserve">// Importing an allocation
cudaMemPoolPtrExportData *importData = &amp;shmem-&gt;prtData;
cudaEvent_t readyIpcEvent;
cudaIpcEventHandle_t *readyIpcEventHandle = &amp;shmem-&gt;readyIpcEventHandle;

// Need to retrieve the ipc event handle and the export data from the
// exporting process using any mechanism.  Here we are using shmem and just
// need synchronization to make sure the shared memory is filled in.

cudaIpcOpenEventHandle(&amp;readyIpcEvent, readyIpcEventHandle);

// import the allocation. The operation does not block on the allocation being ready.
cudaMemPoolImportPointer(&amp;ptr, importedMemPool, importData);

// Wait for the prior stream operations in the allocating stream to complete before
// using the allocation in the importing process.
cudaStreamWaitEvent(stream, readyIpcEvent);
kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...);</pre></div>
                           <div class="p">When freeing the allocation, the allocation needs to be freed in the importing process
                              before it is freed in the exporting process. The following code snippet demonstrates the use
                              of CUDA IPC events to provide the required synchronization between the
                              <samp class="ph codeph">cudaFreeAsync</samp> operations in both processes. Access to the allocation from
                              the importing process is obviously restricted by the free operation in the importing process
                              side. It is worth noting that <samp class="ph codeph">cudaFree</samp> can be used to free the allocation
                              in both processes and that other stream synchronization APIs may be used instead of CUDA IPC
                              events.<pre xml:space="preserve">// The free must happen in importing process before the exporting process
kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...); 

// Last access in importing process
cudaFreeAsync(ptr, stream); 

// Access not allowed in the importing process after the free
cudaIpcEventRecord(finishedIpcEvent, stream);</pre><pre xml:space="preserve">// Exporting process
// The exporting process needs to coordinate its free with the stream order 
// of the importing processs free.
cudaStreamWaitEvent(stream, finishedIpcEvent);
kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptrInExportingProcess, ...); 

// The free in the importing process doesnt stop the exporting process 
// from using the allocation.
cudFreeAsync(ptrInExportingProcess,stream);</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-export-pool-limitations"><a name="stream-ordered-export-pool-limitations" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-export-pool-limitations" name="stream-ordered-export-pool-limitations" shape="rect">F.11.4.&nbsp;IPC Export Pool Limitations</a></h3>
                        <div class="body conbody">
                           <p class="p">IPC pools currently do not support releasing physical blocks back to the OS. As a result
                              the <samp class="ph codeph">cudaMemPoolTrimTo</samp> API acts as a no-op and the
                              <samp class="ph codeph">cudaMemPoolAttrReleaseThreshold</samp> effectively gets ignored. This behavior
                              is controlled by the driver, not the runtime and may change in a future driver update.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-import-pool-limitations"><a name="stream-ordered-import-pool-limitations" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-import-pool-limitations" name="stream-ordered-import-pool-limitations" shape="rect">F.11.5.&nbsp;IPC Import Pool Limitations</a></h3>
                        <div class="body conbody">
                           <p class="p">Allocating from an
                              import pool is not allowed; specifically, import pools cannot be set current and cannot be
                              used in the <samp class="ph codeph">cudaMallocFromPoolAsync</samp> API. As such, the allocation reuse
                              policy attributes are meaningless for these pools.
                           </p>
                           <p class="p">IPC pools currently do not support releasing physical blocks back to the OS. As a
                              result the <samp class="ph codeph">cudaMemPoolTrimTo</samp> API acts as a no-op and the
                              <samp class="ph codeph">cudaMemPoolAttrReleaseThreshold</samp> effectively gets ignored.
                           </p>
                           <p class="p">The resource usage stat attribute queries only reflect the allocations imported
                              into the process and the associated physical memory.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-synchronization-api-actions"><a name="stream-ordered-synchronization-api-actions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-synchronization-api-actions" name="stream-ordered-synchronization-api-actions" shape="rect">F.12.&nbsp;Synchronization API Actions</a></h3>
                     <div class="body conbody">
                        <p class="p">One of the optimizations that comes with the allocator being part of the cuda driver is
                           integration with the synchronize APIs. When the user requests that the CUDA driver
                           synchronize, the driver waits for asynchronous work to complete. Before returning, the
                           driver will determine what frees the synchronization guaranteed to be completed. These
                           allocations are made available for allocation regardless of specified stream or disabled
                           allocation policies. The driver also checks <samp class="ph codeph">cudaMemPoolAttrReleaseThreshold</samp>
                           here and releases any excess physical memory that it can.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="stream-ordered-addendums"><a name="stream-ordered-addendums" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-addendums" name="stream-ordered-addendums" shape="rect">F.13.&nbsp;Addendums</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-cudamemcpyasync"><a name="stream-ordered-cudamemcpyasync" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-cudamemcpyasync" name="stream-ordered-cudamemcpyasync" shape="rect">F.13.1.&nbsp;cudaMemcpyAsync Current Context/Device Sensitivity</a></h3>
                        <div class="body conbody">
                           <p class="p">In the current CUDA driver, any async <samp class="ph codeph">memcpy</samp> involving memory from
                              <samp class="ph codeph">cudaMallocAsync</samp> should be done using the specified streams context as
                              the calling threads current context. This is not necessary for
                              <samp class="ph codeph">cudaMemcpyPeerAsync</samp>, as the device primary contexts specified in the API
                              are referenced instead of the current context.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-cupointergetattribute"><a name="stream-ordered-cupointergetattribute" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-cupointergetattribute" name="stream-ordered-cupointergetattribute" shape="rect">F.13.2.&nbsp;cuPointerGetAttribute Query</a></h3>
                        <div class="body conbody">
                           <p class="p">Invoking <samp class="ph codeph">cuPointerGetAttribute</samp> on an allocation after invoking
                              <samp class="ph codeph">cudaFreeAsync</samp> on it results in undefined behavior. Specifically, it does
                              not matter if an allocation is still accessible from a given stream: the behavior is still
                              undefined.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-cugraphaddmemsetnode"><a name="stream-ordered-cugraphaddmemsetnode" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-cugraphaddmemsetnode" name="stream-ordered-cugraphaddmemsetnode" shape="rect">F.13.3.&nbsp;cuGraphAddMemsetNode</a></h3>
                        <div class="body conbody">
                           <p class="p"><samp class="ph codeph">cuGraphAddMemsetNode</samp> does not work with memory allocated via the stream
                              ordered allocator. However, memsets of the allocations can be stream captured.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="stream-ordered-pointer-attributes"><a name="stream-ordered-pointer-attributes" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#stream-ordered-pointer-attributes" name="stream-ordered-pointer-attributes" shape="rect">F.13.4.&nbsp;Pointer Attributes</a></h3>
                        <div class="body conbody">
                           <p class="p">The <samp class="ph codeph">cuPointerGetAttributes</samp> query works on stream ordered allocations.
                              Since stream ordered allocations are not context associated, querying
                              <samp class="ph codeph">CU_POINTER_ATTRIBUTE_CONTEXT</samp> will succeed but return NULL in
                              <samp class="ph codeph">*data</samp>. The attribute <samp class="ph codeph">CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL</samp>
                              can be used to determine the location of the allocation: this can be useful when selecting a
                              context for making p2h2p copies using <samp class="ph codeph">cudaMemcpyPeerAsync</samp>. The attribute
                              <samp class="ph codeph">CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE</samp> was added in CUDA 11.3 and can be
                              useful for debugging and for confirming which pool an allocation comes from before doing
                              IPC.
                           </p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="mathematical-functions-appendix"><a name="mathematical-functions-appendix" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#mathematical-functions-appendix" name="mathematical-functions-appendix" shape="rect">G.&nbsp;Mathematical Functions</a></h2>
                  <div class="body conbody">
                     <p class="p">The reference manual lists, along with their description, all the functions of the C/C++ standard library mathematical functions
                        that are supported in device code, as well as all intrinsic functions (that are only supported in device code).
                        		
                     </p>
                     <p class="p">This appendix provides accuracy information for some of these functions when applicable. It
                        uses ULP for quantification. For further information on the definition of the Unit in the
                        Last Place (ULP), please see Jean-Michel Muller's paper <em class="ph i">On the definition of ulp(x)</em>,
                        RR-5504, LIP RR-2005-09, INRIA, LIP. 2005, pp.16 at <a class="xref" href="https://hal.inria.fr/inria-00070503/document" target="_blank" shape="rect">https://hal.inria.fr/inria-00070503/document</a>.
                     </p>
                     <p class="p">Mathematical functions supported in device code do not set the global <samp class="ph codeph">errno</samp> variable, nor
                        report any floating-point exceptions to indicate errors; thus, if error diagnostic
                        mechanisms are required, the user should implement additional screening for inputs and
                        outputs of the functions. The user is responsible for the validity of pointer arguments. The
                        user must not pass uninitialized parameters to the Mathematical functions as this may result
                        in undefined behavior: functions are inlined in the user program and thus are subject to
                        compiler optimizations.
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="standard-functions"><a name="standard-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#standard-functions" name="standard-functions" shape="rect">G.1.&nbsp;Standard Functions</a></h3>
                     <div class="body conbody">
                        <p class="p">The functions from this section can be used in both host and device
                           code.
                        </p>
                        <p class="p">This section specifies the error bounds of each function when executed
                           on the device and also when executed on the host in the case where the
                           host does not supply the function.
                        </p>
                        <p class="p">The error bounds are generated from extensive but not exhaustive tests,
                           so they are not guaranteed bounds.
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Single-Precision Floating-Point Functions</h3>
                           <p class="p">Addition and multiplication are IEEE-compliant, so have a maximum
                              error of 0.5 ulp.
                           </p>
                           <p class="p">The recommended way to round a single-precision floating-point operand
                              to an integer, with the result being a single-precision floating-point
                              number is <samp class="ph codeph">rintf()</samp>, not <samp class="ph codeph">roundf()</samp>. The
                              reason is that <samp class="ph codeph">roundf()</samp> maps to a 4-instruction
                              sequence on the device, whereas <samp class="ph codeph">rintf()</samp> maps to a
                              single instruction. <samp class="ph codeph">truncf()</samp>,
                              <samp class="ph codeph">ceilf()</samp>, and <samp class="ph codeph">floorf()</samp> each map to a
                              single instruction as well.
                           </p>
                           <div class="tablenoborder"><a name="standard-functions__single-precision-stdlib" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="standard-functions__single-precision-stdlib" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 7. Single-Precision Mathematical Standard Library Functions with
                                       Maximum ULP Error</span>. <span class="desc tabledesc">The maximum error is stated as the absolute value of the
                                       difference in ulps between a correctly rounded single-precision
                                       result and the result returned by the CUDA library function.</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="40%" id="d225e24193" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="60%" id="d225e24196" rowspan="1" colspan="1">Maximum ulp error</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">x+y</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">0 (IEEE-754 round-to-nearest-even)</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">x*y</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">0 (IEEE-754 round-to-nearest-even)</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">x/y</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">0 for compute capability 
                                             2 when compiled with <samp class="ph codeph">-prec-div=true</samp></p>
                                          <p class="p">2 (full range), otherwise</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">1/x</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">0 for compute capability 
                                             2 when compiled with <samp class="ph codeph">-prec-div=true</samp></p>
                                          <p class="p">1 (full range), otherwise</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1">
                                          <p class="p"><samp class="ph codeph">rsqrtf(x)</samp></p>
                                          <p class="p"><samp class="ph codeph">1/sqrtf(x)</samp></p>
                                       </td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">2 (full range)</p>
                                          <p class="p">Applies to <samp class="ph codeph">1/sqrtf(x)</samp> only when it is
                                             converted to <samp class="ph codeph">rsqrtf(x)</samp> by the compiler.
                                          </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">sqrtf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">0 when compiled with <samp class="ph codeph">-prec-sqrt=true</samp></p>
                                          <p class="p">Otherwise 1 for compute capability 
                                             5.2
                                          </p>
                                          <p class="p">and 3 for older architectures</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">cbrtf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">rcbrtf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">hypotf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">rhypotf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">norm3df(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">rnorm3df(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">norm4df(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">rnorm4df(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">normf(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">rnormf(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">expf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">exp2f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">exp10f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">expm1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">logf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">log2f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">log10f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">log1pf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">sinf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">cosf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">tanf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">sincosf(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">sinpif(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">cospif(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">sincospif(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">asinf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">acosf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">atanf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">atan2f(y,x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">sinhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">coshf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">tanhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">asinhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">acoshf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">atanhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">powf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">9 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">erff(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">erfcf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">erfinvf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">erfcinvf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">erfcxf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">normcdff(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">5 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">normcdfinvf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">5 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">lgammaf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">6 (outside interval -10.001 ... -2.264; larger
                                          inside)
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">tgammaf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">11 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">fmaf(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">frexpf(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">ldexpf(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">scalbnf(x,n)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">scalblnf(x,l)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">logbf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">ilogbf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">j0f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">j1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">jnf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          For n = 128, the maximum absolute error is 2.2 x 10<sup class="ph sup">-6</sup></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">y0f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">y1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">ynf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">
                                          <p class="p">ceil(2 + 2.5n) for |x| &lt; n</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i0f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">fmodf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">remainderf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">remquof(x,y,iptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">modff(x,iptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">fdimf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">truncf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">roundf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">rintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">nearbyintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">ceilf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">floorf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">lrintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">lroundf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">llrintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d225e24193" rowspan="1" colspan="1"><samp class="ph codeph">llroundf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d225e24196" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Double-Precision Floating-Point Functions</h3>
                           <p class="p">The recommended way to round a double-precision floating-point operand
                              to an integer, with the result being a double-precision floating-point
                              number is <samp class="ph codeph">rint()</samp>, not <samp class="ph codeph">round()</samp>. The
                              reason is that <samp class="ph codeph">round()</samp> maps to a 5-instruction
                              sequence on the device, whereas <samp class="ph codeph">rint()</samp> maps to a
                              single instruction. <samp class="ph codeph">trunc()</samp>, <samp class="ph codeph">ceil()</samp>,
                              and <samp class="ph codeph">floor()</samp> each map to a single instruction as
                              well.
                           </p>
                           <div class="tablenoborder">
                              <table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 8. Double-Precision Mathematical Standard Library Functions with
                                       Maximum ULP Error</span>. <span class="desc tabledesc">The maximum error is stated as the absolute value of the
                                       difference in ulps between a correctly rounded double-precision
                                       result and the result returned by the CUDA library function.</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="51.690821256038646%" id="d225e25333" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="48.30917874396135%" id="d225e25336" rowspan="1" colspan="1">Maximum ulp error</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">x+y</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 0 (IEEE-754 round-to-nearest-even) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">x*y</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 0 (IEEE-754 round-to-nearest-even) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">x/y</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 0 (IEEE-754 round-to-nearest-even) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">1/x</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p">0 (IEEE-754 round-to-nearest-even)</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">sqrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (IEEE-754 round-to-nearest-even) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rsqrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 1 (full range) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">cbrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rcbrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">hypot(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rhypot(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">norm3d(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rnorm3d(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">norm4d(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rnorm4d(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">norm(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rnorm(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">exp(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">exp2(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">exp10(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">expm1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">log(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">log2(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">log10(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">log1p(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">sin(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">cos(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">tan(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">sincos(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">sinpi(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">cospi(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">sincospi(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">asin(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">acos(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">atan(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">atan2(y,x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">sinh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">cosh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">tanh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">asinh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">acosh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">atanh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">pow(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">erf(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">erfc(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 5 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">erfinv(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 5 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">erfcinv(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">erfcx(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">normcdf(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 5 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">normcdfinv(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 8 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">lgamma(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 4 (outside interval -11.0001 ... -2.2637; larger
                                          inside)
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">tgamma(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 8 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">fma(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (IEEE-754 round-to-nearest-even) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">frexp(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">ldexp(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">scalbn(x,n)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">scalbln(x,l)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">logb(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">ilogb(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">j0(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">j1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">jn(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          For n = 128, the maximum absolute error is 5 x 10<sup class="ph sup">-12</sup></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">y0(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">y1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">yn(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1">
                                          <p class="p">For |x| &gt; 1.5n, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i0(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">fmod(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">remainder(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">remquo(x,y,iptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">modf(x,iptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">fdim(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">trunc(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">round(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">rint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">nearbyint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">ceil(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">floor(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">lrint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">lround(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">llrint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d225e25333" rowspan="1" colspan="1"><samp class="ph codeph">llround(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d225e25336" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="intrinsic-functions"><a name="intrinsic-functions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#intrinsic-functions" name="intrinsic-functions" shape="rect">G.2.&nbsp;Intrinsic Functions</a></h3>
                     <div class="body conbody">
                        <p class="p">The functions from this section can only be used in device code.</p>
                        <p class="p">Among these functions are the less accurate, but faster versions of some
                           of the functions of <a class="xref" href="index.html#standard-functions" shape="rect">Standard Functions</a> .They have the
                           same name prefixed with <samp class="ph codeph">__</samp> (such as
                           <samp class="ph codeph">__sinf(x)</samp>). They are faster as they map to fewer native
                           instructions.  The compiler has an option
                           (<samp class="ph codeph">-use_fast_math</samp>) that forces each function in <a class="xref" href="index.html#intrinsic-functions__functions-affected-use-fast-math" shape="rect">Table 9</a> to
                           compile to its intrinsic counterpart. In addition to reducing the
                           accuracy of the affected functions, it may also cause some differences in
                           special case handling. A more robust approach is to selectively replace
                           mathematical function calls by calls to intrinsic functions only where it
                           is merited by the performance gains and where changed properties such as
                           reduced accuracy and different special case handling can be
                           tolerated.
                        </p>
                        <div class="tablenoborder"><a name="intrinsic-functions__functions-affected-use-fast-math" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="intrinsic-functions__functions-affected-use-fast-math" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 9. Functions Affected by -use_fast_math</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row">
                                    <th class="entry" valign="top" width="50%" id="d225e26432" rowspan="1" colspan="1">Operator/Function</th>
                                    <th class="entry" valign="top" width="50%" id="d225e26435" rowspan="1" colspan="1">Device Function</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">x/y</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__fdividef(x,y)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">sinf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__sinf(x)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">cosf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__cosf(x)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">tanf(x) </samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__tanf(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">sincosf(x,sptr,cptr)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__sincosf(x,sptr,cptr)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">logf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__logf(x)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">log2f(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__log2f(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">log10f(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__log10f(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">expf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__expf(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">exp10f(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__exp10f(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d225e26432" rowspan="1" colspan="1"><samp class="ph codeph">powf(x,y)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d225e26435" rowspan="1" colspan="1"><samp class="ph codeph">__powf(x,y)</samp></td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Single-Precision Floating-Point Functions</h3>
                           <p class="p"><samp class="ph codeph">__fadd_[rn,rz,ru,rd]()</samp> and
                              <samp class="ph codeph">__fmul_[rn,rz,ru,rd]()</samp> map to addition and
                              multiplication operations that the compiler never merges into FMADs. By
                              contrast, additions and multiplications generated from the '*' and '+'
                              operators will frequently be combined into FMADs.
                           </p>
                           <p class="p">Functions suffixed with <samp class="ph codeph">_rn</samp> operate using the round to
                              nearest even rounding mode.
                           </p>
                           <p class="p">Functions suffixed with <samp class="ph codeph">_rz</samp> operate using the round
                              towards zero rounding mode.
                           </p>
                           <p class="p">Functions suffixed with <samp class="ph codeph">_ru</samp> operate using the round up
                              (to positive infinity) rounding mode.
                           </p>
                           <p class="p">Functions suffixed with <samp class="ph codeph">_rd</samp> operate using the round
                              down (to negative infinity) rounding mode.
                           </p>
                           <p class="p">The accuracy of floating-point division varies depending on whether the code is compiled with
                              <samp class="ph codeph">-prec-div=false</samp> or <samp class="ph codeph">-prec-div=true</samp>.
                              When the code is compiled with
                              <samp class="ph codeph">-prec-div=false</samp>, both the regular division
                              <samp class="ph codeph">/</samp> operator and <samp class="ph codeph">__fdividef(x,y)</samp> have
                              the same accuracy, but for 2<sup class="ph sup">126</sup> &lt; <samp class="ph codeph">y</samp> &lt;
                              2<sup class="ph sup">128</sup>, <samp class="ph codeph">__fdividef(x,y)</samp> delivers a result of
                              zero, whereas the <samp class="ph codeph">/</samp> operator delivers the correct
                              result to within the accuracy stated in <a class="xref" href="index.html#intrinsic-functions__single-precision-floating-point-intrinsic-functions-supported-by-cuda-runtime-library" title="(Supported by the CUDA Runtime Library with Respective Error Bounds)" shape="rect">Table 10</a>.
                              Also, for 2<sup class="ph sup">126</sup> &lt; <samp class="ph codeph">y</samp> &lt;
                              2<sup class="ph sup">128</sup>, if <samp class="ph codeph">x</samp> is infinity,
                              <samp class="ph codeph">__fdividef(x,y)</samp> delivers a <samp class="ph codeph">NaN</samp> (as a
                              result of multiplying infinity by zero), while the <samp class="ph codeph">/</samp>
                              operator returns infinity. On the other hand, the <samp class="ph codeph">/</samp>
                              operator is IEEE-compliant when the code is compiled with <samp class="ph codeph">-prec-div=true</samp>
                              or without any <samp class="ph codeph">-prec-div</samp> option at all since its
                              default value is true.
                           </p>
                           <div class="tablenoborder"><a name="intrinsic-functions__single-precision-floating-point-intrinsic-functions-supported-by-cuda-runtime-library" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="intrinsic-functions__single-precision-floating-point-intrinsic-functions-supported-by-cuda-runtime-library" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 10. Single-Precision Floating-Point Intrinsic Functions</span>. <span class="desc tabledesc">(Supported by the CUDA Runtime Library with Respective Error Bounds)</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="50%" id="d225e26751" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="50%" id="d225e26754" rowspan="1" colspan="1">Error bounds</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fadd_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fsub_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fmul_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fmaf_[rn,rz,ru,rd](x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__frcp_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">  IEEE-compliant. </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fsqrt_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">  IEEE-compliant. </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__frsqrt_rn(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1"> IEEE-compliant. </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fdiv_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">
                                          <p class="p"> IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__fdividef(x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">For <samp class="ph codeph">y</samp> in [2<sup class="ph sup">-126</sup>,
                                          2<sup class="ph sup">126</sup>], the maximum ulp error is 2.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__expf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">The maximum ulp error is <samp class="ph codeph">2 + floor(abs(1.16 *
                                             x))</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__exp10f(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">The maximum ulp error is <samp class="ph codeph">2+ floor(abs(2.95 *
                                             x))</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__logf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [0.5, 2], the maximum absolute
                                          error is 2<sup class="ph sup">-21.41</sup>, otherwise, the maximum ulp error
                                          is 3.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__log2f(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [0.5, 2], the maximum absolute
                                          error is 2<sup class="ph sup">-22</sup>, otherwise, the maximum ulp error is
                                          2.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__log10f(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [0.5, 2], the maximum absolute
                                          error is 2<sup class="ph sup">-24</sup>, otherwise, the maximum ulp error is
                                          3.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__sinf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [-,], the maximum absolute
                                          error is 2<sup class="ph sup">-21.41</sup>, and larger otherwise.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__cosf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [-,], the maximum absolute
                                          error is 2<sup class="ph sup">-21.19</sup>, and larger otherwise.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__sincosf(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">Same as <samp class="ph codeph">__sinf(x)</samp> and
                                          <samp class="ph codeph">__cosf(x)</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__tanf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">Derived from its implementation as <samp class="ph codeph">__sinf(x) *
                                             (1/__cosf(x))</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e26751" rowspan="1" colspan="1"><samp class="ph codeph">__powf(x, y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e26754" rowspan="1" colspan="1">Derived from its implementation as <samp class="ph codeph">exp2f(y *
                                             __log2f(x))</samp>.
                                       </td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Double-Precision Floating-Point Functions</h3>
                           <p class="p"><samp class="ph codeph">__dadd_rn()</samp> and <samp class="ph codeph">__dmul_rn()</samp> map to
                              addition and multiplication operations that the compiler never merges
                              into FMADs. By contrast, additions and multiplications generated from
                              the '*' and '+' operators will frequently be combined into FMADs.
                           </p>
                           <div class="tablenoborder">
                              <table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 11. Double-Precision Floating-Point Intrinsic Functions</span>. <span class="desc tabledesc">(Supported by the CUDA Runtime Library with Respective Error Bounds)</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="50%" id="d225e27099" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="50%" id="d225e27102" rowspan="1" colspan="1">Error bounds</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__dadd_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__dsub_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__dmul_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__fma_[rn,rz,ru,rd](x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__ddiv_[rn,rz,ru,rd](x,y)(x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                          <p class="p">
                                             Requires compute capability <u class="ph u">&gt;</u> 2.
                                             
                                          </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__drcp_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p"> IEEE-compliant.</p>
                                          <p class="p">
                                             Requires compute capability <u class="ph u">&gt;</u> 2.
                                             
                                          </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e27099" rowspan="1" colspan="1"><samp class="ph codeph">__dsqrt_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e27102" rowspan="1" colspan="1">
                                          <p class="p"> IEEE-compliant.</p>
                                          <p class="p">
                                             Requires compute capability <u class="ph u">&gt;</u> 2.
                                             
                                          </p>
                                       </td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="c-cplusplus-language-support"><a name="c-cplusplus-language-support" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#c-cplusplus-language-support" name="c-cplusplus-language-support" shape="rect">H.&nbsp;C++ Language Support</a></h2>
                  <div class="body conbody">
                     <p class="p">As described in <a class="xref" href="index.html#compilation-with-nvcc" shape="rect">Compilation with NVCC</a>, CUDA source files compiled 
                        with <samp class="ph codeph">nvcc</samp> can include a mix of host code and device code. 
                        The CUDA frontend compiler aims to emulate the host compiler behavior with respect to 
                        C++ input code.  The input source code is processed according to the C++ ISO/IEC 14882:2003,
                        C++ ISO/IEC 14882:2011, C++ ISO/IEC 14882:2014 or C++ ISO/IEC 14882:2017 specifications, and the CUDA frontend
                        compiler aims to emulate any host compiler divergences from the ISO specification. In addition,
                        the supported language is extended with CUDA-specific constructs described in this document 
                        <a name="fnsrc_15" href="#fntarg_15" shape="rect"><sup>15</sup></a>, and is subject 
                        to the restrictions described below.
                        
                     </p>
                     <p class="p"><a class="xref" href="index.html#cpp11-language-features" shape="rect">C++11 Language Features</a>, <a class="xref" href="index.html#cpp14-language-features" shape="rect">C++14 Language Features</a> and 
                        <a class="xref" href="index.html#cpp17-language-features" shape="rect">C++17 Language Features</a> provide support matrices
                        for the C++11, C++14 and C++17 features, respectively.
                        <a class="xref" href="index.html#restrictions" shape="rect">Restrictions</a> lists the language restrictions.
                        <a class="xref" href="index.html#polymorphic-function-wrappers" shape="rect">Polymorphic Function Wrappers</a> and <a class="xref" href="index.html#extended-lambda" shape="rect">Extended Lambdas</a> describe additional features.
                        <a class="xref" href="index.html#code-samples" shape="rect">Code Samples</a> gives code samples.
                        
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="cpp11-language-features"><a name="cpp11-language-features" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#cpp11-language-features" name="cpp11-language-features" shape="rect">H.1.&nbsp;C++11 Language Features</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The following table lists new language features that have been accepted into the C++11 standard.
                           The "Proposal" column provides a link to the ISO C++ committee proposal that describes the feature,
                           while the "Available in nvcc (device code)" column indicates the first version of nvcc
                           that contains an implementation of this feature (if it has been implemented) for device code.
                           
                        </p>
                        <div class="tablenoborder"><a name="cpp11-language-features__cpp11-language-features-support-matrix" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="cpp11-language-features__cpp11-language-features-support-matrix" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 12. C++11 Language Features</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="71.42857142857143%" id="d225e27315" rowspan="1" colspan="1">Language Feature</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d225e27318" rowspan="1" colspan="1">C++11 Proposal</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d225e27321" rowspan="1" colspan="1">Available in nvcc (device code)</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Rvalue references</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2118.html" target="_blank" shape="rect">N2118</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">
                                       &nbsp;&nbsp;&nbsp;&nbsp;Rvalue references for <samp class="ph codeph">*this</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2439.htm" target="_blank" shape="rect">N2439</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Initialization of class objects by rvalues</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1610.html" target="_blank" shape="rect">N1610</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Non-static data member initializers</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2008/n2756.htm" target="_blank" shape="rect">N2756</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Variadic templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2242.pdf" target="_blank" shape="rect">N2242</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;Extending variadic template template parameters</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2555.pdf" target="_blank" shape="rect">N2555</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Initializer lists</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2672.htm" target="_blank" shape="rect">N2672</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Static assertions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1720.html" target="_blank" shape="rect">N1720</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1"><samp class="ph codeph">auto</samp>-typed variables
                                       
                                    </td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1984.pdf" target="_blank" shape="rect">N1984</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">
                                       &nbsp;&nbsp;&nbsp;&nbsp;Multi-declarator <samp class="ph codeph">auto</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1737.pdf" target="_blank" shape="rect">N1737</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;Removal of auto as a storage-class specifier</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2546.htm" target="_blank" shape="rect">N2546</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;New function declarator syntax</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2541.htm" target="_blank" shape="rect">N2541</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Lambda expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2009/n2927.pdf" target="_blank" shape="rect">N2927</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Declared type of an expression</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2343.pdf" target="_blank" shape="rect">N2343</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;Incomplete return types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2011/n3276.pdf" target="_blank" shape="rect">N3276</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Right angle brackets</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1757.html" target="_blank" shape="rect">N1757</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Default template arguments for function templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#226" target="_blank" shape="rect">DR226</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Solving the SFINAE problem for expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2634.html" target="_blank" shape="rect">DR339</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Alias templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2258.pdf" target="_blank" shape="rect">N2258</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Extern templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1987.htm" target="_blank" shape="rect">N1987</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Null pointer constant</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2431.pdf" target="_blank" shape="rect">N2431</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Strongly-typed enums</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2347.pdf" target="_blank" shape="rect">N2347</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Forward declarations for enums</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2764.pdf" target="_blank" shape="rect">N2764</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#1206" target="_blank" shape="rect">DR1206</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Standardized attribute syntax</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2761.pdf" target="_blank" shape="rect">N2761</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Generalized constant expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2235.pdf" target="_blank" shape="rect">N2235</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Alignment support</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2341.pdf" target="_blank" shape="rect">N2341</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Conditionally-support behavior</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1627.pdf" target="_blank" shape="rect">N1627</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Changing undefined behavior into diagnosable errors</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1727.pdf" target="_blank" shape="rect">N1727</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Delegating constructors</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1986.pdf" target="_blank" shape="rect">N1986</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Inheriting constructors</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2540.htm" target="_blank" shape="rect">N2540</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Explicit conversion operators</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2437.pdf" target="_blank" shape="rect">N2437</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">New character types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2249.html" target="_blank" shape="rect">N2249</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Unicode string literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2442.htm" target="_blank" shape="rect">N2442</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Raw string literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2442.htm" target="_blank" shape="rect">N2442</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Universal character names in literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2170.html" target="_blank" shape="rect">N2170</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">User-defined literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2765.pdf" target="_blank" shape="rect">N2765</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Standard Layout Types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2342.htm" target="_blank" shape="rect">N2342</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Defaulted functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2346.htm" target="_blank" shape="rect">N2346</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Deleted functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2346.htm" target="_blank" shape="rect">N2346</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Extended friend declarations</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1791.pdf" target="_blank" shape="rect">N1791</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">
                                       Extending <samp class="ph codeph">sizeof</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2253.html" target="_blank" shape="rect">N2253</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#850" target="_blank" shape="rect">DR850</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Inline namespaces</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2535.htm" target="_blank" shape="rect">N2535</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Unrestricted unions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2544.pdf" target="_blank" shape="rect">N2544</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Local and unnamed types as template arguments</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2657.htm" target="_blank" shape="rect">N2657</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Range-based for</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2009/n2930.html" target="_blank" shape="rect">N2930</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Explicit virtual overrides</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2009/n2928.htm" target="_blank" shape="rect">N2928</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3206.htm" target="_blank" shape="rect">N3206</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2011/n3272.htm" target="_blank" shape="rect">N3272</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Minimal support for garbage collection and reachability-based leak detection</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2670.htm" target="_blank" shape="rect">N2670</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">
                                       N/A (see <a class="xref" href="index.html#restrictions" shape="rect">Restrictions</a>)
                                       
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Allowing move constructors to throw [noexcept]</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3050.html" target="_blank" shape="rect">N3050</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Defining move special member functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3053.html" target="_blank" shape="rect">N3053</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e27315 d225e27318 d225e27321" rowspan="1"><strong class="ph b">Concurrency</strong></td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Sequence points</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2239.html" target="_blank" shape="rect">N2239</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Atomic operations</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2427.html" target="_blank" shape="rect">N2427</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Strong Compare and Exchange</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2748.html" target="_blank" shape="rect">N2748</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Bidirectional Fences</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2752.htm" target="_blank" shape="rect">N2752</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Memory model</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2429.htm" target="_blank" shape="rect">N2429</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Data-dependency ordering: atomics and memory model</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2664.htm" target="_blank" shape="rect">N2664</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Propagating exceptions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2179.html" target="_blank" shape="rect">N2179</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Allow atomics use in signal handlers</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2547.htm" target="_blank" shape="rect">N2547</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Thread-local storage</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2659.htm" target="_blank" shape="rect">N2659</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Dynamic initialization and destruction with concurrency</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2660.htm" target="_blank" shape="rect">N2660</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e27315 d225e27318 d225e27321" rowspan="1"><strong class="ph b">C99 Features in C++11</strong></td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1"><samp class="ph codeph">__func__</samp> predefined identifier
                                       
                                    </td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2340.htm" target="_blank" shape="rect">N2340</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">C99 preprocessor</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1653.htm" target="_blank" shape="rect">N1653</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1"><samp class="ph codeph">long long</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1811.pdf" target="_blank" shape="rect">N1811</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e27315" rowspan="1" colspan="1">Extended integral types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27318" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1988.pdf" target="_blank" shape="rect">N1988</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e27321" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="cpp14-language-features"><a name="cpp14-language-features" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#cpp14-language-features" name="cpp14-language-features" shape="rect">H.2.&nbsp;C++14 Language Features</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           The following table lists new language features that have been accepted into the C++14 standard.
                           
                        </p>
                        <div class="tablenoborder"><a name="cpp14-language-features__cpp14-language-features-support-matrix" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="cpp14-language-features__cpp14-language-features-support-matrix" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 13. C++14 Language Features</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="71.42857142857143%" id="d225e28373" rowspan="1" colspan="1">Language Feature</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d225e28376" rowspan="1" colspan="1">C++14 Proposal</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d225e28379" rowspan="1" colspan="1">Available in nvcc (device code)</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Tweak to certain C++ contextual conversions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3323.pdf" target="_blank" shape="rect">N3323</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Binary literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3472.pdf" target="_blank" shape="rect">N3472</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Functions with deduced return type</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3638.html" target="_blank" shape="rect">N3638</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Generalized lambda capture (init-capture)</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3648.html" target="_blank" shape="rect">N3648</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Generic (polymorphic) lambda expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3649.html" target="_blank" shape="rect">N3649</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Variable templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3651.pdf" target="_blank" shape="rect">N3651</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Relaxing requirements on constexpr functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3652.html" target="_blank" shape="rect">N3652</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Member initializers and aggregates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3653.html" target="_blank" shape="rect">N3653</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Clarifying memory allocation</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3664.html" target="_blank" shape="rect">N3664</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Sized deallocation</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/n3778.html" target="_blank" shape="rect">N3778</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1"><samp class="ph codeph">[[deprecated]]</samp> attribute
                                    </td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3760.html" target="_blank" shape="rect">N3760</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d225e28373" rowspan="1" colspan="1">Single-quotation-mark as a digit separator</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28376" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3781.pdf" target="_blank" shape="rect">N3781</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d225e28379" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="cpp17-language-features"><a name="cpp17-language-features" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#cpp17-language-features" name="cpp17-language-features" shape="rect">H.3.&nbsp;C++17 Language Features</a></h3>
                     <div class="body conbody">
                        <p class="p">
                           All C++17 language features are supported in nvcc version 11.0 and later,
                           subject to restrictions described <a class="xref" href="index.html#cpp17" shape="rect"> here</a>.
                           
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="restrictions"><a name="restrictions" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#restrictions" name="restrictions" shape="rect">H.4.&nbsp;Restrictions</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="host-compiler-extensions"><a name="host-compiler-extensions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#host-compiler-extensions" name="host-compiler-extensions" shape="rect">H.4.1.&nbsp;Host Compiler Extensions</a></h3>
                        <div class="body conbody">
                           <p class="p">Host compiler specific language extensions are not supported in device code. </p>
                           <p class="p">__int128 and _Complex types are only supported in host code. </p>
                           <p class="p">__float128 type is only supported in host code on 64-bit x86 Linux platforms.
                              A constant expression of __float128 type may be processed by the compiler in a
                              floating point representation with lower precision.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="preprocessor-symbols"><a name="preprocessor-symbols" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#preprocessor-symbols" name="preprocessor-symbols" shape="rect">H.4.2.&nbsp;Preprocessor Symbols</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="cuda-arch-macro"><a name="cuda-arch-macro" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#cuda-arch-macro" name="cuda-arch-macro" shape="rect">H.4.2.1.&nbsp;__CUDA_ARCH__</a></h3>
                           <div class="body conbody">
                              <ol class="ol">
                                 <li class="li"> The type signature of the following entities shall not depend on whether 
                                    <samp class="ph codeph">__CUDA_ARCH__</samp> is defined or not, or on a particular 
                                    value of <samp class="ph codeph">__CUDA_ARCH__</samp>:
                                    
                                    <ul class="ul">
                                       <li class="li"><samp class="ph codeph">__global__</samp> functions and function 
                                          templates
                                          
                                       </li>
                                       <li class="li"><samp class="ph codeph">__device__</samp> and <samp class="ph codeph">__constant__</samp> 
                                          variables
                                          
                                       </li>
                                       <li class="li">textures and surfaces
                                          
                                       </li>
                                    </ul>
                                    <p class="p">Example:</p><pre xml:space="preserve">
#<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> !defined(__CUDA_ARCH__)
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> mytype;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span> mytype;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> mytype xxx;         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: xxx's type depends on __CUDA_ARCH__</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(mytype in, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: foo's type depends on __CUDA_ARCH__</span>
                    mytype *ptr)
{
  *ptr = in;
}</pre><p class="p"></p>
                                 </li>
                                 <li class="li"> If a <samp class="ph codeph">__global__</samp> function template is instantiated 
                                    and launched from the host, then the function template must be 
                                    instantiated with the same template arguments irrespective of whether 
                                    <samp class="ph codeph">__CUDA_ARCH__</samp> is defined and regardless of the 
                                    value of <samp class="ph codeph">__CUDA_ARCH__</samp>.
                                    
                                    <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> result;
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern(T in)
{
  result = in;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#if !defined(__CUDA_ARCH__)</span>
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(1);      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: "kern&lt;int&gt;" instantiation only</span>
                         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// when __CUDA_ARCH__ is undefined!</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
  foo();
  cudaDeviceSynchronize();
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}
</pre><p class="p"></p>
                                 </li>
                                 <li class="li"> In separate compilation mode, the presence or absence of a 
                                    definition of a function or variable with external linkage
                                    shall not depend on whether <samp class="ph codeph">__CUDA_ARCH__</samp> is 
                                    defined or on a particular value of <samp class="ph codeph">__CUDA_ARCH__</samp><a name="fnsrc_16" href="#fntarg_16" shape="rect"><sup>16</sup></a>.
                                    
                                    <p class="p">Example:</p><pre xml:space="preserve">
#<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> !defined(__CUDA_ARCH__)
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { }                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: The definition of foo()</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// is only present when __CUDA_ARCH__</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// is undefined</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
</pre><p class="p"></p>
                                 </li>
                                 <li class="li">In separate compilation, <samp class="ph codeph">__CUDA_ARCH__</samp> must not be used in headers such that different objects could contain different behavior.  Or, it must be guaranteed that
                                    all objects will compile for the same compute_arch.  If a weak function or template function is defined in a header and its
                                    behavior depends on <samp class="ph codeph">__CUDA_ARCH__</samp>, then the instances of that function in the objects could conflict if the objects are compiled for different compute arch.
                                    
                                    
                                    <p class="p">For example, if an a.h contains:</p><pre xml:space="preserve">template&lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> T* getptr(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#if __CUDA_ARCH__ == 200</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> NULL; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* no address */</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> T arr[256];
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> arr;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
}
</pre><p class="p">
                                       Then if a.cu and b.cu both include a.h and instantiate getptr for the same type, 
                                       and b.cu expects a non-NULL address, and compile with:
                                       
                                    </p><pre class="pre screen" xml:space="preserve">nvcc arch=compute_20 dc a.cu
nvcc arch=compute_30 dc b.cu
nvcc arch=sm_30 a.o b.o
</pre><p class="p">
                                       At link time only one version of the getptr is used, so the behavior would depend on which version is picked.  
                                       To avoid this, either a.cu and b.cu must be compiled for the same compute arch, 
                                       or <samp class="ph codeph">__CUDA_ARCH__</samp> should not be used in the shared header function.
                                       
                                    </p>
                                 </li>
                              </ol>
                              <p class="p"> The compiler does not guarantee that a diagnostic will be generated
                                 for the unsupported uses of <samp class="ph codeph">__CUDA_ARCH__</samp> 
                                 described above.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="qualifiers"><a name="qualifiers" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#qualifiers" name="qualifiers" shape="rect">H.4.3.&nbsp;Qualifiers</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="device-memory-specifiers"><a name="device-memory-specifiers" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#device-memory-specifiers" name="device-memory-specifiers" shape="rect">H.4.3.1.&nbsp;Device Memory Space Specifiers</a></h3>
                           <div class="body conbody">
                              <p class="p">The <samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__shared__</samp>, <samp class="ph codeph">__managed__</samp> and
                                 <samp class="ph codeph">__constant__</samp> memory space specifiers are not allowed on:
                              </p>
                              <ul class="ul">
                                 <li class="li"><samp class="ph codeph">class</samp>, <samp class="ph codeph">struct</samp>, and
                                    <samp class="ph codeph">union</samp> data members,
                                 </li>
                                 <li class="li">formal parameters,</li>
                                 <li class="li">non-extern variable declarations within a function that executes on the host.</li>
                              </ul>
                              <p class="p">The <samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__constant__</samp> and 
                                 <samp class="ph codeph">__managed__</samp> memory
                                 space specifiers are not allowed on variable declarations that are neither
                                 extern nor static within a function that executes on the device.
                              </p>
                              <p class="p">A <samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__constant__</samp>, <samp class="ph codeph">__managed__</samp>
                                 or
                                 <samp class="ph codeph">__shared__</samp> variable definition cannot have a class type with 
                                 a non-empty constructor or a non-empty destructor. 
                                 A constructor for a class type is considered empty at a point
                                 in the translation unit,  if it is either a trivial constructor or it
                                 satisfies all of the following conditions:
                              </p>
                              <ul class="ul">
                                 <li class="li"> The constructor function has been defined. </li>
                                 <li class="li"> The constructor function has no parameters, the initializer list is
                                    empty and the function body is an empty compound statement. 
                                 </li>
                                 <li class="li"> Its class has no virtual functions, no virtual base classes
                                    	    and no non-static data member initializers. 
                                 </li>
                                 <li class="li"> The default constructors of all base classes of its class can be
                                    considered empty. 
                                 </li>
                                 <li class="li"> For all the nonstatic data members of its class that are of class
                                    type (or array thereof), the default constructors can be considered
                                    empty. 
                                 </li>
                              </ul>
                              <p class="p">A destructor for a class is considered empty at a point in the translation unit, if it is either a trivial destructor or it
                                 satisfies all of the following conditions:
                              </p>
                              <ul class="ul">
                                 <li class="li"> The destructor function has been defined.</li>
                                 <li class="li"> The destructor function body is an empty compound statement.</li>
                                 <li class="li"> Its class has no virtual functions and no virtual base classes.</li>
                                 <li class="li"> The destructors  of all base classes of its class can be considered
                                    empty.
                                 </li>
                                 <li class="li"> For all the nonstatic data members of its class that are of class
                                    type (or array thereof), the destructor can be considered empty.
                                 </li>
                              </ul>
                              <p class="p">When compiling in the whole program compilation mode (see the nvcc user
                                 manual for a description of this mode), <samp class="ph codeph">__device__</samp>,
                                 <samp class="ph codeph">__shared__</samp>, <samp class="ph codeph">__managed__</samp> and <samp class="ph codeph">__constant__</samp> variables
                                 cannot be defined as external using the <samp class="ph codeph">extern</samp> keyword.
                                 The only exception is for dynamically allocated
                                 <samp class="ph codeph">__shared__</samp> variables as described in <a class="xref" href="index.html#shared" shape="rect">__shared__</a>.
                              </p>
                              <p class="p">When compiling in the separate compilation mode (see the nvcc user
                                 manual for a description of this mode), <samp class="ph codeph">__device__</samp>,
                                 <samp class="ph codeph">__shared__</samp>, <samp class="ph codeph">__managed__</samp> and <samp class="ph codeph">__constant__</samp> variables
                                 can be defined as external using the <samp class="ph codeph">extern</samp> keyword.
                                 <samp class="ph codeph">nvlink</samp> will generate an error when it cannot find a
                                 definition for an external variable (unless it is a dynamically allocated
                                 <samp class="ph codeph">__shared__</samp> variable).
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="managed-specifier"><a name="managed-specifier" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#managed-specifier" name="managed-specifier" shape="rect">H.4.3.2.&nbsp;<samp class="ph codeph">__managed__</samp> Memory Space Specifier</a></h3>
                           <div class="body conbody">
                              <p class="p">Variables marked with the <samp class="ph codeph">__managed__</samp> memory space specifier ("managed" variables) have
                                 the following restrictions: 
                              </p>
                              <ul class="ul">
                                 <li class="li"> The address of a managed variable is not a constant expression. </li>
                                 <li class="li"> A managed variable shall not have a const qualified type. </li>
                                 <li class="li"> A managed variable shall not have a reference type. </li>
                                 <li class="li"> The address or value of a managed variable shall not be used when the CUDA runtime may not be in a valid state, including
                                    the following cases:
                                    
                                    <ul class="ul">
                                       <li class="li"> In static/dynamic initialization or destruction of an object with static or thread local storage duration. </li>
                                       <li class="li"> In code that executes after exit() has been called (e.g., a function marked with gcc's "<samp class="ph codeph">__attribute__((destructor))</samp>"). 
                                       </li>
                                       <li class="li"> In code that executes when CUDA runtime may not be initialized (e.g., a function marked with gcc's "<samp class="ph codeph">__attribute__((constructor))</samp>").
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li"> A managed variable cannot be used as an unparenthesized id-expression argument to a <samp class="ph codeph">decltype()</samp> expression.
                                 </li>
                                 <li class="li"> Managed variables have the same coherence and consistency behavior as specified for dynamically allocated managed memory.
                                    
                                 </li>
                                 <li class="li"> When a CUDA program containing managed variables is run on an execution platform with multiple GPUs, the variables are allocated
                                    only once, and not per GPU. 
                                 </li>
                                 <li class="li"> A managed variable declaration without the extern linkage is not allowed within a function that executes on the host.</li>
                                 <li class="li"> A managed variable declaration without the extern or static linkage is not allowed within a function that executes on the
                                    device.
                                 </li>
                              </ul>
                              <p class="p">Here are examples of legal and illegal uses of managed variables: </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx = 10;         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ptr = &amp;xxx;                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: use of managed variable </span>
                                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (xxx) in static initialization</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> field;
  S1_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) : field(xxx) { };
};
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S2_t {
  ~S2_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { xxx = 10; }
};

S1_t temp1;                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: use of managed variable </span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (xxx) in dynamic initialization</span>

S2_t temp2;                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: use of managed variable</span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (xxx) in the destructor of </span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// object with static storage </span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// duration</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> yyy = 10;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: const qualified type</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> &amp;zzz = xxx;      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: reference type</span>

template &lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *addr&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S3_t { };
S3_t&lt;&amp;xxx&gt; temp;                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: address of managed </span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// variable(xxx) not a </span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// constant expression</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ptr)
{
  assert(ptr == &amp;xxx);                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  xxx = 20;                                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) 
{
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ptr = &amp;xxx;                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(ptr);
  cudaDeviceSynchronize();
  xxx++;                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  decltype(xxx) qqq;                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: managed variable(xxx) used</span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// as unparenthized argument to</span>
                                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// decltype</span>
                                            
  decltype((xxx)) zzz = yyy;                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="volatile-qualifier"><a name="volatile-qualifier" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#volatile-qualifier" name="volatile-qualifier" shape="rect">H.4.3.3.&nbsp;Volatile Qualifier</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers
                                 or L1 cache)
                                 as long as it respects the memory ordering semantics of memory fence functions (<a class="xref" href="index.html#memory-fence-functions" shape="rect">Memory Fence Functions</a>) and memory visibility semantics of synchronization functions (<a class="xref" href="index.html#synchronization-functions" shape="rect">Synchronization Functions</a>).
                                 
                              </p>
                              <p class="p">
                                 These optimizations can be disabled using the <samp class="ph codeph">volatile</samp> keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can
                                 be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory
                                 read or write instruction.
                                 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="pointers"><a name="pointers" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#pointers" name="pointers" shape="rect">H.4.4.&nbsp;Pointers</a></h3>
                        <div class="body conbody">
                           <p class="p">Dereferencing a pointer either to global or shared memory in code that is executed on the
                              host, or to host memory in code that is executed on the device results in an undefined
                              behavior, most often in a segmentation fault and application termination.
                              
                           </p>
                           <p class="p"> The address obtained by taking the address of a <samp class="ph codeph">__device__</samp>,
                              <samp class="ph codeph">__shared__</samp> or <samp class="ph codeph">__constant__</samp> variable can only be
                              used in device code. The address of a <samp class="ph codeph">__device__</samp> or
                              <samp class="ph codeph">__constant__</samp> variable obtained through
                              <samp class="ph codeph">cudaGetSymbolAddress()</samp> as described in <a class="xref" href="index.html#device-memory" shape="rect">Device Memory</a> can only be used in host code.
                           </p>
                           <p class="p"></p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="operators"><a name="operators" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#operators" name="operators" shape="rect">H.4.5.&nbsp;Operators</a></h3>
                        <div class="body conbody">
                           <p class="p"></p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="assignment-operator"><a name="assignment-operator" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#assignment-operator" name="assignment-operator" shape="rect">H.4.5.1.&nbsp;Assignment Operator</a></h3>
                           <div class="body conbody">
                              <p class="p"><samp class="ph codeph">__constant__</samp> variables can only be assigned from the host code through runtime
                                 functions (<a class="xref" href="index.html#device-memory" shape="rect">Device Memory</a>); they cannot be assigned from the device code.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">__shared__</samp> variables cannot have an initialization as part of their declaration.
                                 
                              </p>
                              <p class="p">It is not allowed to assign values to any of the built-in variables defined in
                                 <a class="xref" href="index.html#built-in-variables" shape="rect">Built-in Variables</a>.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="address-operator"><a name="address-operator" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#address-operator" name="address-operator" shape="rect">H.4.5.2.&nbsp;Address Operator</a></h3>
                           <div class="body conbody">
                              <p class="p">It is not allowed to take the address of any of the built-in variables defined in <a class="xref" href="index.html#built-in-variables" shape="rect">Built-in Variables</a>.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="rtti"><a name="rtti" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#rtti" name="rtti" shape="rect">H.4.6.&nbsp;Run Time Type Information (RTTI)</a></h3>
                        <div class="body conbody">
                           <p class="p">The following RTTI-related features are supported in host code, but not in device code.</p>
                           <ul class="ul">
                              <li class="li"><samp class="ph codeph">typeid</samp> operator
                              </li>
                              <li class="li"><samp class="ph codeph">std::type_info</samp></li>
                              <li class="li"><samp class="ph codeph">dynamic_cast</samp> operator
                              </li>
                           </ul>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="exception-handling"><a name="exception-handling" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#exception-handling" name="exception-handling" shape="rect">H.4.7.&nbsp;Exception Handling</a></h3>
                        <div class="body conbody">
                           <p class="p">Exception handling is only supported in host code, but not in device code.</p>
                           <p class="p">Exception specification is not supported for <samp class="ph codeph">__global__</samp> functions.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="standard-library"><a name="standard-library" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#standard-library" name="standard-library" shape="rect">H.4.8.&nbsp;Standard Library</a></h3>
                        <div class="body conbody">
                           <p class="p">Standard libraries are only supported in host code, but not in device code, unless specified otherwise.</p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="functions"><a name="functions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#functions" name="functions" shape="rect">H.4.9.&nbsp;Functions</a></h3>
                        <div class="body conbody">
                           <p class="p"></p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="external-linkage"><a name="external-linkage" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#external-linkage" name="external-linkage" shape="rect">H.4.9.1.&nbsp;External Linkage</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 A call within some device code of a function declared with the extern qualifier is only allowed if the function
                                 is defined within the same compilation unit as the device code, i.e., a single file or several files linked together with
                                 relocatable device code and nvlink.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="compiler-generated-functions"><a name="compiler-generated-functions" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#compiler-generated-functions" name="compiler-generated-functions" shape="rect">H.4.9.2.&nbsp;Implicitly-declared and explicitly-defaulted functions</a></h3>
                           <div class="body conbody">
                              <div class="p"> Let <samp class="ph codeph">F</samp> denote a function that is either implicitly-declared or is explicitly-defaulted on its first declaration 
                                 The execution space specifiers (<samp class="ph codeph">__host__</samp>, <samp class="ph codeph">__device__</samp>) for <samp class="ph codeph">F</samp>
                                 are the union of the execution space specifiers of all the functions 
                                 that invoke it (note that a <samp class="ph codeph">__global__</samp> caller will be treated as a <samp class="ph codeph">__device__
                                    </samp> caller for this analysis). For example:
                                 <pre xml:space="preserve">class Base {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x;
public:  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Base(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) : x(10) {}
};

class Derived : public Base {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y;
};

class Other: public Base {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> z;
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
  Derived D1;
  Other D2;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
  Other D3;
}</pre>
                                 Here, the implicitly-declared constructor function "Derived::Derived" will be treated as a <samp class="ph codeph">__device__</samp>
                                 function, since it is invoked only from the <samp class="ph codeph">__device__</samp> function "foo". The implicitly-declared
                                 constructor function "Other::Other" will be treated as a <samp class="ph codeph">__host__ __device__</samp> 
                                 function, since it is invoked both from a <samp class="ph codeph">__device__</samp> function "foo" and a <samp class="ph codeph">__host__</samp>
                                 function "bar". 
                                 
                              </div>
                              <p class="p">
                                 In addition, if <samp class="ph codeph">F</samp> is a virtual destructor, then the execution spaces of each virtual destructor
                                 <samp class="ph codeph">D</samp> overridden by <samp class="ph codeph">F</samp> are added to the set of execution spaces for <samp class="ph codeph">F</samp>,
                                 if <samp class="ph codeph">D</samp> is either not implicitly defined or is explicitly defaulted on a declaration other
                                 than its first declaration. 
                                 
                              </p>
                              <p class="p">For example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> Base1 { virtual <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> ~Base1() { } };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> Derived1 : Base1 { }; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// implicitly-declared virtual destructor</span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ~Derived1 has __host__ __device__ </span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// execution space specifiers</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> Base2 { virtual <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> ~Base2(); };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Base2::~Base2() = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">default</span>;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> Derived2 : Base2 { }; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// implicitly-declared virtual destructor</span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ~Derived2 has __device__ execution </span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// space specifiers </span>
        </pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="function-parameters"><a name="function-parameters" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#function-parameters" name="function-parameters" shape="rect">H.4.9.3.&nbsp;Function Parameters</a></h3>
                           <div class="body conbody">
                              <p class="p"><samp class="ph codeph">__global__</samp> function parameters are passed to the
                                 device via constant memory and are limited to 4 KB.
                              </p>
                              <p class="p"><samp class="ph codeph">__global__</samp> functions
                                 cannot have a variable number of arguments.
                              </p>
                              <p class="p"><samp class="ph codeph">__global__</samp> function parameters cannot be pass-by-reference.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="static-variables-function"><a name="static-variables-function" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#static-variables-function" name="static-variables-function" shape="rect">H.4.9.4.&nbsp;Static Variables within Function</a></h3>
                           <div class="body conbody">
                              <div class="p">Variable memory space specifiers are allowed in the declaration
                                 of a static variable <samp class="ph codeph">V</samp> within the immediate or 
                                 nested block scope of a function <samp class="ph codeph">F</samp> where:
                                 
                                 <ul class="ul">
                                    <li class="li"><samp class="ph codeph">F</samp> is a <samp class="ph codeph">__global__</samp> or 
                                       <samp class="ph codeph">__device__</samp>-only function.
                                    </li>
                                    <li class="li"><samp class="ph codeph">F</samp> is a <samp class="ph codeph">__host__ __device__</samp> 
                                       function and <samp class="ph codeph">__CUDA_ARCH__</samp> is defined 
                                       <a name="fnsrc_17" href="#fntarg_17" shape="rect"><sup>17</sup></a>.
                                    </li>
                                 </ul>
                              </div>
                              <p class="p">If no explicit memory space specifier is present in the declaration
                                 of <samp class="ph codeph">V</samp>, an implicit <samp class="ph codeph">__device__</samp> specifier is assumed
                                 during device compilation.
                                 
                              </p>
                              <p class="p"><samp class="ph codeph">V</samp> has the same initialization restrictions as a variable with the same memory space
                                 specifiers declared in namespace scope e.g. a <samp class="ph codeph">__device__</samp> variable cannot
                                 have a 'non-empty' constructor (see <a class="xref" href="index.html#device-memory-specifiers" shape="rect">Device Memory Space Specifiers</a>).
                                 
                              </p>
                              <p class="p">Examples of legal and illegal uses of function-scope static variables are shown below.</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x;
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S2_t {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> S2_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { x = 10; }
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S3_t {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> S3_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> p) : x(p) { }
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f1() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i1;              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, implicit __device__ memory space specifier</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i2 = 11;         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, implicit __device__ memory space specifier</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> m1;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> d1;   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__constant__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> c1; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> S1_t i3;             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, implicit __device__ memory space specifier</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> S1_t i4 = {22};      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, implicit __device__ memory space specifier</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i5;   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = 33;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i6 = x;          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: dynamic initialization is not allowed</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> S1_t i7 = {x};       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: dynamic initialization is not allowed</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> S2_t i8;             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: dynamic initialization is not allowed</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> S3_t i9(44);         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: dynamic initialization is not allowed</span>
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f2() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i1;              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, implicit __device__ memory space specifier</span>
                              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// during device compilation.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#ifdef __CUDA_ARCH__</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> d1;   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, declaration is only visible during device</span>
                              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compilation  (__CUDA_ARCH__ is defined)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> d0;              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK, declaration is only visible during host</span>
                              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// compilation (__CUDA_ARCH__ is not defined)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif  </span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> d2;   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: __device__ variable inside</span>
                              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// a host function during host compilation</span>
                              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// i.e. when __CUDA_ARCH__ is not defined</span>

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i2;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: __shared__ variable inside</span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// a host function during host compilation</span>
                             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// i.e. when __CUDA_ARCH__ is not defined</span>
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="function-pointers"><a name="function-pointers" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#function-pointers" name="function-pointers" shape="rect">H.4.9.5.&nbsp;Function Pointers</a></h3>
                           <div class="body conbody">
                              <p class="p"> The address of a <samp class="ph codeph">__global__</samp> function taken in host code cannot be used in device code (e.g.
                                 to launch the kernel). Similarly, the address of a <samp class="ph codeph">__global__</samp> function taken in device code
                                 <a name="fnsrc_18" href="#fntarg_18" shape="rect"><sup>18</sup></a> cannot be used in host code. 
                              </p>
                              <p class="p"> It is not allowed to take the address of a <samp class="ph codeph">__device__</samp> function in host code. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="function-recursion"><a name="function-recursion" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#function-recursion" name="function-recursion" shape="rect">H.4.9.6.&nbsp;Function Recursion</a></h3>
                           <div class="body conbody">
                              <p class="p"><samp class="ph codeph">__global__</samp> functions do not support recursion.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="friend-function"><a name="friend-function" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#friend-function" name="friend-function" shape="rect">H.4.9.7.&nbsp;Friend Functions</a></h3>
                           <div class="body conbody">
                              <p class="p"> A <samp class="ph codeph">__global__</samp> function or function template cannot be defined in a friend 
                                 declaration. 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {
  friend <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo1(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: not a definition</span>
  template&lt;typename T&gt;
  friend <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo2(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: not a definition</span>
  
  friend <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo3(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { } <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: definition in friend declaration</span>
  
  template&lt;typename T&gt;
  friend <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo4(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { } <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: definition in friend declaration</span>
};</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="operator-function"><a name="operator-function" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#operator-function" name="operator-function" shape="rect">H.4.9.8.&nbsp;Operator Function</a></h3>
                           <div class="body conbody">
                              <p class="p"> An operator function cannot be a <samp class="ph codeph">__global__</samp> function. 
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="classes"><a name="classes" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#classes" name="classes" shape="rect">H.4.10.&nbsp;Classes</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="data-members"><a name="data-members" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#data-members" name="data-members" shape="rect">H.4.10.1.&nbsp;Data Members</a></h3>
                           <div class="body conbody">
                              <p class="p">Static data members are not supported except for those that are also const-qualified (see <a class="xref" href="index.html#const-variables" shape="rect">Const-qualified variables</a>).
                              </p>
                              <p class="p"></p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="function-members"><a name="function-members" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#function-members" name="function-members" shape="rect">H.4.10.2.&nbsp;Function Members</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 Static member functions cannot be <samp class="ph codeph">__global__</samp> functions.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="virtual-functions"><a name="virtual-functions" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#virtual-functions" name="virtual-functions" shape="rect">H.4.10.3.&nbsp;Virtual Functions</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 When a function in a derived class overrides a virtual function in a base class, the
                                 execution space specifiers (i.e., <samp class="ph codeph">__host__</samp>, <samp class="ph codeph">__device__</samp>)
                                 on the overridden and overriding functions must match.
                                 
                              </p>
                              <p class="p">
                                 It is not allowed to pass as an argument to a <samp class="ph codeph">__global__</samp> function an object
                                 of a class with virtual functions.
                                 
                              </p>
                              <p class="p">If an object is created in host code, invoking a virtual function for that object in device code 
                                 has undefined behavior. 
                              </p>
                              <p class="p">If an object is created in device code, invoking a virtual function for that object in host code
                                 has undefined behavior. 
                              </p>
                              <p class="p"> See <a class="xref" href="index.html#windows-specific" shape="rect">Windows-Specific</a> for additional constraints when using the Microsoft host compiler.
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1 { virtual <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo() { } };

__managed__ S1 *ptr1, *ptr2;

__managed__ __align__(16) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> buf1[128];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern() { 
  ptr1-&gt;foo();     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: virtual function call on a object</span>
                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//        created in host code.</span>
  ptr2 = new(buf1) S1();
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> *buf;
  cudaMallocManaged(&amp;buf, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(S1), cudaMemAttachGlobal);
  ptr1 = new (buf) S1();
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
  cudaDeviceSynchronize();
  ptr2-&gt;foo();  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: virtual function call on an object</span>
                <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//        created in device code.</span>
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="virtual-base-classes"><a name="virtual-base-classes" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#virtual-base-classes" name="virtual-base-classes" shape="rect">H.4.10.4.&nbsp;Virtual Base Classes</a></h3>
                           <div class="body conbody">
                              <p class="p"> It is not allowed to pass as an argument to a <samp class="ph codeph">__global__</samp> function an object
                                 of a class derived from virtual base classes. 
                              </p>
                              <p class="p"> See <a class="xref" href="index.html#windows-specific" shape="rect">Windows-Specific</a> for additional constraints when using the Microsoft host compiler.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="anon-union"><a name="anon-union" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#anon-union" name="anon-union" shape="rect">H.4.10.5.&nbsp;Anonymous Unions</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 Member variables of a namespace scope anonymous union cannot be referenced in a 
                                 <samp class="ph codeph">__global__</samp>  or <samp class="ph codeph">__device__</samp> function.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="windows-specific"><a name="windows-specific" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#windows-specific" name="windows-specific" shape="rect">H.4.10.6.&nbsp;Windows-Specific</a></h3>
                           <div class="body conbody">
                              <div class="p">The CUDA compiler follows the IA64 ABI for class layout, while the Microsoft host compiler does not. Let <samp class="ph codeph">T</samp> denote
                                 a pointer to member type, or a class type that satisfies any of the following conditions:
                                 
                                 <ul class="ul">
                                    <li class="li"><samp class="ph codeph">T</samp> has virtual functions.
                                    </li>
                                    <li class="li"><samp class="ph codeph">T</samp> has a virtual base class.
                                    </li>
                                    <li class="li"><samp class="ph codeph">T</samp> has multiple inheritance with more than one direct or indirect empty base class.
                                    </li>
                                    <li class="li">All direct and indirect base classes <samp class="ph codeph">B</samp> of <samp class="ph codeph">T</samp> are empty and the type of the first field 
                                       <samp class="ph codeph">F</samp> of <samp class="ph codeph">T</samp> uses <samp class="ph codeph">B</samp> in its definition, such that <samp class="ph codeph">B</samp> 
                                       is laid out at offset 0 in the definition of <samp class="ph codeph">F</samp>.
                                    </li>
                                 </ul>
                                 
                                 Let <samp class="ph codeph">C</samp> denote <samp class="ph codeph">T</samp> or a class type that has <samp class="ph codeph">T</samp> as a field type or as a base class type.
                                 The CUDA compiler may compute the class layout and size differently than the Microsoft host compiler for the type <samp class="ph codeph">C</samp>.
                                 
                              </div>
                              <p class="p">As long as the type <samp class="ph codeph">C</samp> is used exclusively in host or device code, the program should work correctly.
                              </p>
                              <p class="p">Passing an object of type <samp class="ph codeph">C</samp> between host and device code has undefined behavior
                                 e.g., as an argument to a <samp class="ph codeph">__global__</samp> function or through <samp class="ph codeph">cudaMemcpy*()</samp> calls. 
                              </p>
                              <p class="p">Accessing an object of type <samp class="ph codeph">C</samp> or any subobject in device code, or invoking a member function in
                                 device code, has undefined behavior if the object is created in host code.
                              </p>
                              <p class="p">Accessing an object of type <samp class="ph codeph">C</samp> or any subobject in host code, or invoking a member function in
                                 host code, has undefined behavior if the object is created in device code
                                 <a name="fnsrc_19" href="#fntarg_19" shape="rect"><sup>19</sup></a>.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="templates"><a name="templates" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#templates" name="templates" shape="rect">H.4.11.&nbsp;Templates</a></h3>
                        <div class="body conbody">
                           <div class="p"> A type or template cannot be used in the type, non-type or template template argument of
                              a <samp class="ph codeph">__global__</samp> function template instantiation or a <samp class="ph codeph">__device__/__constant__</samp>
                              variable instantiation if either:
                              
                              <ul class="ul">
                                 <li class="li"> The type or template is defined within a <samp class="ph codeph">__host__</samp>  or <samp class="ph codeph">__host__ __device__</samp>.
                                 </li>
                                 <li class="li"> The type or template is a class member with  <samp class="ph codeph">private</samp> or <samp class="ph codeph">protected</samp> access and its
                                    parent class is not defined within a <samp class="ph codeph">__device__</samp> or <samp class="ph codeph">__global__</samp> function.
                                 </li>
                                 <li class="li"> The type is unnamed.</li>
                                 <li class="li"> The type is compounded from any of the types above.</li>
                              </ul>
                           </div>
                           <p class="p">Example:</p><pre xml:space="preserve">template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> myKernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { }

class myClass {
private:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> inner_t { }; 
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> launch(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) 
    {
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: inner_t is used in template argument</span>
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// but it is private</span>
       myKernel&lt;inner_t&gt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    }
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// C++14 only</span>
template &lt;typename T&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> T d1;

template &lt;typename T1, typename T2&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> T1 d2;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> fn() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error (C++14 only): S1_t is local to the function fn</span>
  d1&lt;S1_t&gt; = {};

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error (C++14 only): a closure type cannot be used for</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// instantiating a variable template</span>
  d2&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>, decltype(lam1)&gt; = 10;
}
</pre></div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="trigraph-digraph"><a name="trigraph-digraph" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#trigraph-digraph" name="trigraph-digraph" shape="rect">H.4.12.&nbsp;Trigraphs and Digraphs </a></h3>
                        <div class="body conbody">
                           <p class="p"> Trigraphs are not supported on any platform. Digraphs are not supported on Windows.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="const-variables"><a name="const-variables" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#const-variables" name="const-variables" shape="rect">H.4.13.&nbsp;Const-qualified variables </a></h3>
                        <div class="body conbody">
                           <p class="p"> Let 'V' denote a namespace scope variable or a class static member variable that has const qualified 
                              type and does not  have execution space annotations (e.g., <samp class="ph codeph">__device__, __constant__, __shared__</samp>).
                              V is considered to be a  host code variable. 
                              
                           </p>
                           <div class="p"> The value of V may be directly used in device code, if
                              
                              <ul class="ul">
                                 <li class="li">
                                    V has been initialized with a constant expression before the point of use,
                                    
                                 </li>
                                 <li class="li">
                                    the type of V is not volatile-qualified, and
                                    
                                 </li>
                                 <li class="li">
                                    it has one of the following types:
                                    
                                    <ul class="ul">
                                       <li class="li">builtin floating point type except when the Microsoft compiler is used as the host compiler,</li>
                                       <li class="li">builtin integral type.</li>
                                    </ul>
                                 </li>
                              </ul>
                              
                              Device source code cannot contain a reference to V or take the address of V.
                              
                           </div>
                           <p class="p">Example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx = 10;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> yyy = 20; };
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> zzz;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> www = 5.0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> local1[xxx];          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> local2[S1_t::yyy];    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
      
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val1 = xxx;           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
    					
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val2 = S1_t::yyy;     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
    					
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> val3 = zzz;           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: zzz not initialized with constant </span>
                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// expression at the point of use.</span>
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> &amp;val3 = xxx;    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: reference to host variable  </span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *val4 = &amp;xxx;   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: address of host variable</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> val5 = www;   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK except when the Microsoft compiler is used as</span>
                            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the host compiler.</span>
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> zzz = 20;</pre></div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="long-double"><a name="long-double" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#long-double" name="long-double" shape="rect">H.4.14.&nbsp;Long Double </a></h3>
                        <div class="body conbody">
                           <p class="p">The use of <samp class="ph codeph">long double</samp> type is not supported in device code.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="deprecation-annotation"><a name="deprecation-annotation" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#deprecation-annotation" name="deprecation-annotation" shape="rect">H.4.15.&nbsp;Deprecation Annotation </a></h3>
                        <div class="body conbody">
                           <p class="p">nvcc supports the use of <samp class="ph codeph">deprecated</samp> attribute  when using <samp class="ph codeph">gcc</samp>, 
                              <samp class="ph codeph">clang</samp>, <samp class="ph codeph">xlC</samp>, <samp class="ph codeph">icc</samp>
                              or <samp class="ph codeph">pgcc</samp> host compilers, and the use of <samp class="ph codeph">deprecated</samp> declspec when using the 
                              <samp class="ph codeph">cl.exe</samp> host compiler. It also supports the 
                              <samp class="ph codeph">[[deprecated]]</samp> standard attribute when the C++14 dialect has been enabled. 
                              The CUDA frontend compiler will generate a deprecation diagnostic for a reference to a deprecated entity from within the body
                              of a <samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__global__</samp> or <samp class="ph codeph">__host__ __device__</samp> function when 
                              <samp class="ph codeph">__CUDA_ARCH__</samp> is defined (i.e., during device compilation phase). Other references to deprecated entities 
                              will be handled by the host compiler, e.g., a reference from within a <samp class="ph codeph">__host__</samp> function. 
                              
                           </p>
                           <p class="p">The CUDA frontend compiler does not support the <samp class="ph codeph">#pragma gcc diagnostic</samp> or
                              <samp class="ph codeph">#pragma warning</samp> mechanisms supported by various host compilers. Therefore, deprecation diagnostics 
                              generated by the CUDA frontend compiler are not affected by these pragmas, but diagnostics generated by the host compiler
                              will 
                              be affected. The <samp class="ph codeph">nvcc</samp> flag <samp class="ph codeph">-Wno-deprecated-declarations</samp> can be used to suppress all
                              deprecation warnings, and the flag <samp class="ph codeph">-Werror=deprecated-declarations</samp> can be used to turn deprecation
                              warnings into errors.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="noreturn-annotation"><a name="noreturn-annotation" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#noreturn-annotation" name="noreturn-annotation" shape="rect">H.4.16.&nbsp;Noreturn Annotation </a></h3>
                        <div class="body conbody">
                           <p class="p">nvcc supports the use of <samp class="ph codeph">noreturn</samp> attribute when using <samp class="ph codeph">gcc</samp>, <samp class="ph codeph">clang</samp>, 
                              <samp class="ph codeph">xlC</samp>, <samp class="ph codeph">icc</samp> or <samp class="ph codeph">pgcc</samp> host compilers, and the use of <samp class="ph codeph">noreturn</samp> declspec when using the <samp class="ph codeph">cl.exe</samp>
                              host compiler. It also supports the <samp class="ph codeph">[[noreturn]]</samp> standard attribute when the C++11 dialect has been enabled.
                              
                           </p>
                           <p class="p">The attribute/declspec can be used in both host and device code. </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="likely-unlikely-attribute"><a name="likely-unlikely-attribute" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#likely-unlikely-attribute" name="likely-unlikely-attribute" shape="rect">H.4.17.&nbsp;[[likely]] / [[unlikely]] Standard Attributes </a></h3>
                        <div class="body conbody">
                           <p class="p">These attributes are accepted in all configurations that support the C++ standard attribute syntax. The
                              attributes can be used to hint to the device compiler optimizer whether a statement is more or less likely
                              to be executed compared to any alternative path that does not include the statement.
                              
                           </p>
                           <p class="p">Example:</p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x) {

 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (i &lt; 10) [[likely]] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the 'if' block will likely be entered</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 4; 
 }
 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (i &lt; 20) [[unlikely]] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the 'if' block will not likely be entered</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1;
 }
 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre><p class="p">If these attributes are used in host code when <samp class="ph codeph">__CUDA_ARCH__</samp> is undefined, then they will be present
                              in the code parsed by the host compiler, which may generate a warning if the attributes
                              are not supported. E.g., <samp class="ph codeph">clang</samp>11 host compiler will generate an 'unknown attribute' warning.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="cpp11"><a name="cpp11" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#cpp11" name="cpp11" shape="rect">H.4.18.&nbsp;C++11 Features</a></h3>
                        <div class="body conbody">
                           <p class="p">C++11 features that are enabled by default by the host compiler are also
                              supported by nvcc, subject to the restrictions described in this
                              document. In addition, invoking nvcc with <samp class="ph codeph">-std=c++11</samp> flag turns on
                              all C++11 features and also invokes the host preprocessor, compiler
                              and linker with the corresponding C++11 dialect option <a name="fnsrc_20" href="#fntarg_20" shape="rect"><sup>20</sup></a>.
                              
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="lambda-expressions"><a name="lambda-expressions" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#lambda-expressions" name="lambda-expressions" shape="rect">H.4.18.1.&nbsp;Lambda Expressions</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 The execution space specifiers for all member functions<a name="fnsrc_21" href="#fntarg_21" shape="rect"><sup>21</sup></a>
                                 of the closure class associated  with a lambda expression
                                 are derived by the compiler as follows. As described in the C++11 standard, the compiler creates a
                                 closure type in the smallest block scope, class scope or namespace
                                 scope that contains the lambda expression.  The innermost function
                                 scope enclosing the closure type is computed, and the corresponding
                                 function's execution space specifiers are assigned to the
                                 closure class member functions. If there is no enclosing function scope, the
                                 execution space specifier is <samp class="ph codeph">__host__</samp>.
                                 
                              </p>
                              <p class="p">Examples of lambda expressions and computed execution space specifiers
                                 are shown below (in comments).
                              </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> globalVar = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; }; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __host__ </span>
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f1(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> l1 = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; };      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __host__</span>
}
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f2(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> l2 = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 2; };      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __device__</span>
}
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f3(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> l3 = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 3; };      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __host__ __device__</span>
}
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f4(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*fp)() = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 4; } <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* __host__ */</span>) {
}
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f5(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> l5 = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 5; };      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __device__</span>
}
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> f6(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> helper(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*fp)() = [] {<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 6; } <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* __device__ */</span>) {
    }
  };
}</pre><p class="p"> The closure type of a lambda expression cannot be used in the type or
                                 non-type argument of a <samp class="ph codeph">__global__</samp> function template instantiation, unless
                                 the lambda is defined within a <samp class="ph codeph">__device__</samp> 
                                 or <samp class="ph codeph">__global__</samp> function.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(T in) { };
    
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { };
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> temp1 = [] { };
      
  foo<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(temp1);                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: lambda closure type used in</span>
                                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// template type argument</span>
  foo<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( S1_t&lt;decltype(temp1)&gt;()); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: lambda closure type used in </span>
                                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// template type argument</span>
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="initializer-list"><a name="initializer-list" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#initializer-list" name="initializer-list" shape="rect">H.4.18.2.&nbsp;std::initializer_list</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 By default, the CUDA compiler will implicitly consider the member
                                 functions of <samp class="ph codeph">std::initializer_list</samp> to have 
                                 <samp class="ph codeph">__host__ __device__</samp>
                                 execution space specifiers, and therefore they can be invoked directly
                                 from device code. The nvcc flag 
                                 <samp class="ph codeph">--no-host-device-initializer-list</samp> will
                                 disable this behavior; member functions of 
                                 <samp class="ph codeph">std::initializer_list</samp> will
                                 then be considered as <samp class="ph codeph">__host__</samp> functions and will not be directly
                                 invokable from device code.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
#include &lt;initializer_list&gt;
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo(std::initializer_list&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt; in);
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
  {
    foo({4,5,6});   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (a) initializer list containing only </span>
                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// constant expressions.</span>
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 4;
    foo({i,5,6});   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// (b) initializer list with at least one </span>
                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// non-constant element.</span>
                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This form may have better performance than (a). </span>
  }</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="rvalue-references"><a name="rvalue-references" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#rvalue-references" name="rvalue-references" shape="rect">H.4.18.3.&nbsp;Rvalue references</a></h3>
                           <div class="body conbody">
                              <p class="p"> By default, the CUDA compiler will implicitly consider 
                                 <samp class="ph codeph">std::move</samp> and
                                 <samp class="ph codeph">std::forward</samp> function templates to have 
                                 <samp class="ph codeph">__host__ __device__</samp> execution space specifiers, 
                                 and therefore they can be invoked directly from device code. 
                                 The nvcc flag <samp class="ph codeph">--no-host-device-move-forward</samp> will
                                 disable this behavior; <samp class="ph codeph">std::move</samp> and 
                                 <samp class="ph codeph">std::forward</samp> will then be
                                 considered as <samp class="ph codeph">__host__</samp> functions and will not be directly
                                 invokable from device code.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="constexpr-functions"><a name="constexpr-functions" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#constexpr-functions" name="constexpr-functions" shape="rect">H.4.18.4.&nbsp;Constexpr functions and function templates</a></h3>
                           <div class="body conbody">
                              <p class="p"> 
                                 By default, a constexpr function cannot be called from a function with incompatible execution space
                                 <a name="fnsrc_22" href="#fntarg_22" shape="rect"><sup>22</sup></a>. The 
                                 experimental nvcc flag <samp class="ph codeph">--expt-relaxed-constexpr</samp> removes this restriction
                                 <a name="fnsrc_23" href="#fntarg_23" shape="rect"><sup>23</sup></a>. 
                                 When this flag is specified, host code can invoke
                                 a <samp class="ph codeph">__device__</samp> constexpr function and device code can invoke a 
                                 <samp class="ph codeph">__host__</samp> constexpr function. nvcc will define the macro
                                 <samp class="ph codeph">__CUDACC_RELAXED_CONSTEXPR__</samp> when <samp class="ph codeph">--expt-relaxed-constexpr</samp>
                                 has been specified.
                                 Note that a function template instantiation may not be a constexpr function even if the 
                                 corresponding template is marked with the keyword <samp class="ph codeph">constexpr</samp> 
                                 (C++11 Standard Section <samp class="ph codeph">[dcl.constexpr.p6]</samp>).
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="constexpr-variables"><a name="constexpr-variables" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#constexpr-variables" name="constexpr-variables" shape="rect">H.4.18.5.&nbsp;Constexpr variables</a></h3>
                           <div class="body conbody">
                              <p class="p"> Let 'V' denote a namespace scope variable or a class static member variable that has been marked constexpr
                                 and that does not have execution space annotations (e.g., <samp class="ph codeph">__device__, __constant__, __shared__</samp>).
                                 V is considered to be a host code variable. 
                                 
                              </p>
                              <p class="p"> If V is of scalar type <a name="fnsrc_24" href="#fntarg_24" shape="rect"><sup>24</sup></a> other than <samp class="ph codeph">long double</samp> and the type is not volatile-qualified, 
                                 the value of  V can be directly used in device code. In addition, if V is of a non-scalar type then scalar elements of V can
                                 be used inside a constexpr 
                                 <samp class="ph codeph">__device__</samp> or <samp class="ph codeph">__host__ __device__</samp> function, if the call to the function is a
                                 constant expression <a name="fnsrc_25" href="#fntarg_25" shape="rect"><sup>25</sup></a>. 
                                 Device source code cannot contain a reference to V or take the address of V.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
constexpr <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx = 10;
constexpr <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> yyy = xxx + 4;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> constexpr <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> qqq = 100; };

constexpr <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> host_arr[] = { 1, 2, 3};
constexpr <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> get(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> idx) { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> host_arr[idx]; } 
  
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> idx) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> v1 = xxx + yyy + S1_t::qqq;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> &amp;v2 = xxx;             <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: reference to host constexpr </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// variable</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *v3 = &amp;xxx;            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: address of host constexpr </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// variable</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> &amp;v4 = S1_t::qqq;       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: reference to host constexpr </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// variable</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *v5 = &amp;S1_t::qqq;      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: address of host constexpr </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// variable</span>
                                   
  v1 += get(2);                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: 'get(2)' is a constant </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// expression.</span>
  v1 += get(idx);                  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: 'get(idx)' is not a constant </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// expression</span>
  v1 += host_arr[2];               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: 'host_arr' does not have </span>
                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// scalar type.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> v1;
}</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="inline-namespaces"><a name="inline-namespaces" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#inline-namespaces" name="inline-namespaces" shape="rect">H.4.18.6.&nbsp;Inline namespaces</a></h3>
                           <div class="body conbody">
                              <div class="p"> For an input CUDA translation unit, the CUDA compiler may invoke the host compiler for 
                                 compiling the host code within the translation unit. In the  code passed to the host
                                 compiler, the CUDA compiler will inject additional compiler generated code, 
                                 if the input CUDA translation unit contained a definition of any of the following entities:
                                 
                                 <ul class="ul">
                                    <li class="li"><samp class="ph codeph">__global__</samp> function or function template instantiation 
                                    </li>
                                    <li class="li"><samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__constant__</samp></li>
                                    <li class="li">  variables with surface or texture type </li>
                                 </ul>
                                 
                                 The compiler generated code contains a reference to the defined entity. 
                                 If the entity is defined within an inline namespace and another entity of the same name and
                                 type signature is defined in an enclosing namespace, this reference may be considered ambiguous 
                                 by the host compiler and host compilation will fail. 
                                 
                              </div>
                              <p class="p"> This limitation can be avoided by using unique names for such entities defined within 
                                 an inline namespace.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Gvar;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> namespace N1 {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Gvar;  
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// &lt;-- CUDA compiler inserts a reference to "Gvar" at this point in the </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// translation unit. This reference will be considered ambiguous by the </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// host compiler and compilation will fail.</span>
</pre><p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> namespace N1 {
  namespace N2 {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Gvar;
  }
}

namespace N2 {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Gvar;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// &lt;-- CUDA compiler inserts reference to "::N2::Gvar" at this point in </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the translation unit. This reference will be considered ambiguous by </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the host compiler and compilation will fail.</span>
</pre></div>
                           <div class="topic concept nested4" xml:lang="en-US" id="inline-unnamed-namespaces"><a name="inline-unnamed-namespaces" shape="rect">
                                 <!-- --></a><h3 class="title topictitle2"><a href="#inline-unnamed-namespaces" name="inline-unnamed-namespaces" shape="rect">H.4.18.6.1.&nbsp;Inline unnamed namespaces</a></h3>
                              <div class="body conbody">
                                 <div class="p"> The following entities cannot be declared in namespace scope within an inline unnamed namespace:
                                    
                                    <ul class="ul">
                                       <li class="li"><samp class="ph codeph">__managed__</samp>, <samp class="ph codeph">__device__</samp>, <samp class="ph codeph">__shared__</samp> and <samp class="ph codeph">__constant__</samp> variables
                                       </li>
                                       <li class="li"><samp class="ph codeph">__global__</samp> function and function templates
                                       </li>
                                       <li class="li"> variables with surface or texture type</li>
                                    </ul>
                                 </div>
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> namespace {
  namespace N2 {
    template &lt;typename T&gt;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { }         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
    
    template &lt;&gt;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { }    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
      
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x1b;                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__constant__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x2b;                 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x3b;                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error </span>
	
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">texture</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt; q2;                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">surface</span>&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt; s2;                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
  }
};</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="thread-local"><a name="thread-local" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#thread-local" name="thread-local" shape="rect">H.4.18.7.&nbsp;thread_local</a></h3>
                           <div class="body conbody">
                              <p class="p"> The <samp class="ph codeph">thread_local</samp> storage specifier is not allowed in device code. 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="cpp11-global"><a name="cpp11-global" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#cpp11-global" name="cpp11-global" shape="rect">H.4.18.8.&nbsp;<samp class="ph codeph">__global__</samp> functions and function templates</a></h3>
                           <div class="body conbody">
                              <p class="p">If the closure type associated with a lambda expression is used in a template argument of a 
                                 <samp class="ph codeph">__global__</samp> function template instantiation, the lambda expression must either be defined in the
                                 immediate or nested block scope of a <samp class="ph codeph">__device__</samp> or <samp class="ph codeph">__global__</samp> function, or must be an 
                                 <a class="xref" href="index.html#extended-lambda" shape="rect">extended lambda</a>.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
    
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel(T in) { }

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo_device(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// All kernel instantiations in this function</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// are valid, since the lambdas are defined inside</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// a __device__ function.</span>
  
  kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { } );
  kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { } );
  kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( []  { } );
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { };

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo_host(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: instantiated with closure type of an extended __device__ lambda</span>
   kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { } );
   
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: instantiated with closure type of an extended __host__ __device__ </span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// lambda</span>
   kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { } );
 
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: unsupported: instantiated with closure type of a lambda</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// that is not an extended lambda</span>
   kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( []  { } );
   
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: unsupported: instantiated with closure type of a lambda</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// that is not an extended lambda</span>
   kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( lam1);
   
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: unsupported: instantiated with closure type of a lambda</span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// that is not an extended lambda</span>
   kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>( lam2);
}</pre><p class="p"> A <samp class="ph codeph">__global__</samp> function or function template cannot be declared as
                                 <samp class="ph codeph">constexpr</samp>.
                              </p>
                              <p class="p"> A <samp class="ph codeph">__global__</samp> function or function template cannot have a parameter of
                                 type <samp class="ph codeph">std::initializer_list</samp> or <samp class="ph codeph">va_list</samp>.
                              </p>
                              <p class="p"> A <samp class="ph codeph">__global__</samp> function cannot have a parameter of rvalue reference type.
                              </p>
                              <div class="p">A variadic <samp class="ph codeph">__global__</samp> function template has the following restrictions:
                                 
                                 <ul class="ul">
                                    <li class="li">Only a single pack parameter is allowed.</li>
                                    <li class="li">The pack parameter must be listed last in the template parameter list.</li>
                                 </ul>
                              </div>
                              <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ok</span>
template &lt;template &lt;typename...&gt; class Wrapper, typename... Pack&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo1(Wrapper&lt;Pack...&gt;);
    
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: pack parameter is not last in parameter list</span>
template &lt;typename... Pack, template &lt;typename...&gt; class Wrapper&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo2(Wrapper&lt;Pack...&gt;);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: multiple parameter packs</span>
template &lt;typename... Pack1, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>...Pack2, template&lt;typename...&gt; class Wrapper1, 
          template&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>...&gt; class Wrapper2&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo3(Wrapper1&lt;Pack1...&gt;, Wrapper2&lt;Pack2...&gt;);</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="cpp11-device-variable"><a name="cpp11-device-variable" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#cpp11-device-variable" name="cpp11-device-variable" shape="rect">H.4.18.9.&nbsp;<samp class="ph codeph">__managed__</samp> and <samp class="ph codeph">__shared__</samp> variables</a></h3>
                           <div class="body conbody">
                              <p class="p"><samp class="ph codeph">__managed__</samp> and <samp class="ph codeph">__shared__</samp> variables 
                                 cannot be marked with the keyword <samp class="ph codeph">constexpr</samp>.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="cpp11-defaulted-function"><a name="cpp11-defaulted-function" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#cpp11-defaulted-function" name="cpp11-defaulted-function" shape="rect">H.4.18.10.&nbsp;Defaulted functions</a></h3>
                           <div class="body conbody">
                              <p class="p">Execution space specifiers on a function that is explicitly-defaulted on its first declaration are ignored by the CUDA compiler.
                                 
                                 Instead, the CUDA compiler will infer the execution space specifiers as described in <a class="xref" href="index.html#compiler-generated-functions" shape="rect">Implicitly-declared and explicitly-defaulted functions</a>.
                                 
                              </p>
                              <p class="p">Execution space specifiers are not ignored if the function is explicitly-defaulted, but not on its first declaration.</p>
                              <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1 {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// warning: __host__ annotation is ignored on a function that </span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//          is explicitly-defaulted on its first declaration</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> S1() = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">default</span>;
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo1() { 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//note: __device__ execution space is derived for S1::S1 </span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//       based on implicit call from within __device__ function </span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//       foo1</span>
  S1 s1;    
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S2 {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> S2();
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//note: S2::S2 is not defaulted on its first declaration, and </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//      its execution space is fixed to __host__  based on its </span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//      first declaration.</span>
S2::S2() = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">default</span>;  

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo2() {
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: call from __device__ function 'foo2' to </span>
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//        __host__ function 'S2::S2'</span>
   S2 s2;  
}
</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="cpp14"><a name="cpp14" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#cpp14" name="cpp14" shape="rect">H.4.19.&nbsp;C++14 Features</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              C++14 features enabled by default by the host
                              compiler are also supported by nvcc. Passing nvcc <samp class="ph codeph">-std=c++14</samp>
                              flag turns on all C++14 features and also invokes the host preprocessor,
                              compiler and linker with the corresponding C++14 dialect option <a name="fnsrc_26" href="#fntarg_26" shape="rect"><sup>26</sup></a>. 
                              This section describes the restrictions on the supported C++14 features. 
                              
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="return-type-deduction"><a name="return-type-deduction" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#return-type-deduction" name="return-type-deduction" shape="rect">H.4.19.1.&nbsp;Functions with deduced return type</a></h3>
                           <div class="body conbody">
                              <p class="p"> A <samp class="ph codeph">__global__</samp> function cannot have a deduced return type. 
                              </p>
                              <p class="p">
                                 If a <samp class="ph codeph">__device__</samp> function has deduced return type, the CUDA frontend
                                 compiler will change the function declaration to have a <samp class="ph codeph">void</samp> return
                                 type, before invoking the host compiler. This may cause issues for introspecting the
                                 deduced return type of the <samp class="ph codeph">__device__</samp> function in host code.
                                 Thus, the CUDA compiler will issue compile-time errors for referencing such deduced
                                 return type outside device function bodies, except if the reference is absent when
                                 <samp class="ph codeph">__CUDA_ARCH__</samp> is undefined.
                                 
                              </p>
                              <p class="p">Examples:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> fn1(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> x;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> decltype(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span>) fn2(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> x;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> device_fn1() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*p1)(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) = fn1;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: referenced outside device function bodies</span>
decltype(fn1(10)) g1;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_fn1() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: referenced outside device function bodies</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*p1)(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) = fn1;

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S_local_t {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: referenced outside device function bodies</span>
    decltype(fn2(10)) m1;

    S_local_t() : m1(10) { }
  };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: referenced outside device function bodies</span>
template &lt;typename T = decltype(fn2)&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_fn2() { }

template&lt;typename T&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { };

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: referenced outside device function bodies</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_derived_t : S1_t&lt;decltype(fn1)&gt; { };
</pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="variable-templates"><a name="variable-templates" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#variable-templates" name="variable-templates" shape="rect">H.4.19.2.&nbsp;Variable templates</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 A <samp class="ph codeph">__device__/__constant__</samp> variable template cannot have a const
                                 qualified type when using the Microsoft host compiler.
                                 
                              </p>
                              <p class="p">Examples:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: a __device__ variable template cannot</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// have a const qualified type on Windows</span>
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T d1(2);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> x = nullptr;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: a __device__ variable template cannot</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// have a const qualified type on Windows</span>
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> T *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> d2(x);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T *d3;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> fn() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> t1 = d1&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;;

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> t2 = d2&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;;

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *t3 = d3&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;;
}</pre></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="cpp17"><a name="cpp17" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#cpp17" name="cpp17" shape="rect">H.4.20.&nbsp;C++17 Features</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              C++17 features enabled by default by the host
                              compiler are also supported by nvcc. Passing nvcc <samp class="ph codeph">-std=c++17</samp>
                              flag turns on all C++17 features and also invokes the host preprocessor,
                              compiler and linker with the corresponding C++17 dialect option <a name="fnsrc_27" href="#fntarg_27" shape="rect"><sup>27</sup></a>. 
                              This section describes the restrictions on the supported C++17 features. 
                              
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="inline-variable"><a name="inline-variable" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#inline-variable" name="inline-variable" shape="rect">H.4.20.1.&nbsp;Inline Variable</a></h3>
                           <div class="body conbody">
                              <ul class="ul">
                                 <li class="li">
                                    <p class="p">  A namespace scope inline variable declared with <samp class="ph codeph">__device__</samp> or
                                       <samp class="ph codeph">__constant__</samp> or <samp class="ph codeph">__managed__</samp> memory space specifier
                                       must have internal linkage, if the code is compiled with nvcc in whole program
                                       compilation mode.
                                       
                                    </p>
                                    <p class="p">Examples:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//error when compiled with nvcc in</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//whole program compilation mode.</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//ok when compiled with nvcc in</span>
                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//separate compilation mode.</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> yyy0; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ok.</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">static</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> yyy; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ok: internal linkage</span>
namespace {
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">inline</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> zzz; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ok: internal linkage</span>
}</pre></li>
                                 <li class="li">
                                    <p class="p">When using g++ host compiler, 
                                       an inline variable declared with <samp class="ph codeph">__managed__</samp> memory space
                                       specifier may not be visible to the debugger.
                                       
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="structured-binding"><a name="structured-binding" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#structured-binding" name="structured-binding" shape="rect">H.4.20.2.&nbsp;Structured Binding</a></h3>
                           <div class="body conbody">
                              <p class="p">A structured binding cannot be declared with a variable memory space specifier.
                                 
                              </p>
                              <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y; };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> [a1, b1] = S{4,5}; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error</span>
</pre></div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="polymorphic-function-wrappers"><a name="polymorphic-function-wrappers" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#polymorphic-function-wrappers" name="polymorphic-function-wrappers" shape="rect">H.5.&nbsp;Polymorphic Function Wrappers</a></h3>
                     <div class="body conbody">
                        <p class="p"> A polymorphic function wrapper class template <samp class="ph codeph">nvstd::function</samp> is provided 
                           in the <samp class="ph codeph">nvfunctional</samp> header.
                           Instances of this class template can be used to store, copy and invoke any 
                           callable target, e.g., lambda expressions. <samp class="ph codeph">nvstd::function</samp> can be used in both
                           host and device code. 
                           
                        </p>
                        <p class="p">Example:</p><pre xml:space="preserve">
#include &lt;nvfunctional&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo_d() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; }
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo_hd () { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 2; }
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo_h() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 3; }

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *result) {
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn1 = foo_d;  
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn2 = foo_hd;
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn3 =  []() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };

  *result = fn1() + fn2() + fn3();
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> hostdevice_func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *result) {
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn1 = foo_hd;  
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn2 =  []() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };

  *result = fn1() + fn2();
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *result) {
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn1 = foo_h;  
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn2 = foo_hd;  
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn3 =  []() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };

  *result = fn1() + fn2() + fn3();
}</pre><p class="p"> Instances of <samp class="ph codeph">nvstd::function</samp> in host code cannot be initialized with the address of a 
                           <samp class="ph codeph">__device__</samp> function or with a functor whose <samp class="ph codeph">operator()</samp> is a 
                           <samp class="ph codeph">__device__</samp> function. Instances of <samp class="ph codeph">nvstd::function</samp> in device code 
                           cannot be initialized with the address of a <samp class="ph codeph">__host__</samp> function or with a functor whose 
                           <samp class="ph codeph">operator()</samp> is a <samp class="ph codeph">__host__</samp> function.
                           
                        </p>
                        <p class="p"><samp class="ph codeph">nvstd::function</samp> instances cannot be passed from host code to device code (and vice versa) at run time.
                           <samp class="ph codeph">nvstd::function</samp> cannot be used in the parameter type of a <samp class="ph codeph">__global__</samp> function, if the <samp class="ph codeph">__global__</samp> 
                           function is launched from host code. 
                           
                        </p>
                        <p class="p">Example:</p><pre xml:space="preserve">
#include &lt;nvfunctional&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo_d() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; }
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo_h() { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 3; }
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam_h = [] { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; };

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> k(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: initialized with address of __host__ function </span>
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn1 = foo_h;  

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: initialized with address of functor with</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __host__ operator() function </span>
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn2 = lam_h;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern(nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; f1) { }

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: initialized with address of __device__ function </span>
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn1 = foo_d;  

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam_d = [=] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: initialized with address of functor with</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __device__ operator() function </span>
  nvstd::function&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>()&gt; fn2 = lam_d;

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: passing nvstd::function from host to device</span>
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(fn2);
}</pre><p class="p"><samp class="ph codeph">nvstd::function</samp> is defined in the <samp class="ph codeph">nvfunctional</samp> header as follows:
                        </p><pre xml:space="preserve">
namespace nvstd {
  template &lt;class _RetType, class ..._ArgTypes&gt;
  class function&lt;_RetType(_ArgTypes...)&gt; 
  {
    public:
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// constructors</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function() noexcept;
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function(nullptr_t) noexcept;
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> function &amp;);
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function(function &amp;&amp;);

      template&lt;class _F&gt;
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function(_F);

      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// destructor</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  ~function();

      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// assignment operators</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function&amp; operator=(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> function&amp;);
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function&amp; operator=(function&amp;&amp;);
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function&amp; operator=(nullptr_t);
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  function&amp; operator=(_F&amp;&amp;);

      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// swap</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> swap(function&amp;) noexcept;

      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// function capacity</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>  explicit operator bool() <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> noexcept;

      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// function invocation</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> _RetType operator()(_ArgTypes...) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>;
  };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// null pointer comparisons</span>
  template &lt;class _R, class... _ArgTypes&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>
  bool operator==(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;, nullptr_t) noexcept;

  template &lt;class _R, class... _ArgTypes&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>
  bool operator==(nullptr_t, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;) noexcept;

  template &lt;class _R, class... _ArgTypes&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>
  bool operator!=(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;, nullptr_t) noexcept;

  template &lt;class _R, class... _ArgTypes&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>
  bool operator!=(nullptr_t, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;) noexcept;

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// specialized algorithms</span>
  template &lt;class _R, class... _ArgTypes&gt;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> swap(function&lt;_R(_ArgTypes...)&gt;&amp;, function&lt;_R(_ArgTypes...)&gt;&amp;);
}</pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="extended-lambda"><a name="extended-lambda" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#extended-lambda" name="extended-lambda" shape="rect">H.6.&nbsp;Extended Lambdas</a></h3>
                     <div class="body conbody">
                        <p class="p"> The nvcc flag <samp class="ph codeph">'--extended-lambda'</samp> allows explicit execution space annotations 
                           in a lambda expression
                           <a name="fnsrc_28" href="#fntarg_28" shape="rect"><sup>28</sup></a>.
                           The execution space annotations should be present 
                           after the 'lambda-introducer' and before the optional 'lambda-declarator'. nvcc will define
                           the macro <samp class="ph codeph">__CUDACC_EXTENDED_LAMBDA__</samp> when the <samp class="ph codeph">'--extended-lambda'</samp>
                           flag has been specified.
                           
                        </p>
                        <p class="p"> An 'extended <samp class="ph codeph">__device__</samp> lambda' is a lambda expression that is annotated 
                           explicitly with '<samp class="ph codeph">__device__</samp>', and is defined within the immediate or nested block 
                           scope of a <samp class="ph codeph">__host__</samp> or <samp class="ph codeph">__host__ __device__</samp> function.
                           
                        </p>
                        <p class="p"> An 'extended <samp class="ph codeph">__host__ __device__</samp> lambda' is a lambda expression that is annotated 
                           explicitly with  both '<samp class="ph codeph">__host__</samp>' and '<samp class="ph codeph">__device__</samp>', and
                           is defined within the immediate or nested block scope of a <samp class="ph codeph">__host__</samp> or 
                           <samp class="ph codeph">__host__ __device__</samp> function.
                           
                        </p>
                        <p class="p"> An 'extended lambda' denotes either an extended <samp class="ph codeph">__device__</samp> lambda or 
                           an extended <samp class="ph codeph">__host__ __device__</samp> lambda. Extended lambdas can be used in the 
                           type arguments of <a class="xref" href="index.html#cpp11-global" shape="rect">__global__ function template instantiation</a>.
                           
                        </p>
                        <p class="p"> If the execution space annotations are not explicitly specified, they
                           are computed based on the scopes enclosing the closure class
                           associated with the lambda, as described in the section on C++11 support.
                           The execution space annotations are applied to all
                           methods of the closure class associated with the lambda.
                           
                        </p>
                        <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo_host(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// not an extended lambda: no explicit execution space annotations</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// extended __device__ lambda</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// extended __host__ __device__ lambda</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// not an extended lambda: explicitly annotated with only '__host__'</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> { };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo_host_device(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// not an extended lambda: no explicit execution space annotations</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// extended __device__ lambda</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// extended __host__ __device__ lambda</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// not an extended lambda: explicitly annotated with only '__host__'</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> { };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo_device(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// none of the lambdas within this function are extended lambdas, </span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// because the enclosing function is not a __host__ or __host__ __device__</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// function.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> { };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// lam1 and lam2 are not extended lambdas because they are not defined</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// within a __host__ or __host__ __device__ function.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };</pre></div>
                     <div class="topic concept nested2" xml:lang="en-US" id="extended-lambda-traits"><a name="extended-lambda-traits" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#extended-lambda-traits" name="extended-lambda-traits" shape="rect">H.6.1.&nbsp;Extended Lambda Type Traits</a></h3>
                        <div class="body conbody">
                           <p class="p">The compiler provides type traits to detect closure types for extended lambdas at compile time:
                              
                           </p>
                           <p class="p"><samp class="ph codeph">__nv_is_extended_device_lambda_closure_type(type)</samp>: If 'type' is the closure class created for an
                              extended <samp class="ph codeph">__device__</samp> lambda, then the trait is true, otherwise it is false.
                              
                           </p>
                           <p class="p"><samp class="ph codeph">__nv_is_extended_host_device_lambda_closure_type(type)</samp>: If 'type' is the closure class created for an
                              extended <samp class="ph codeph">__host__ __device__</samp> lambda, then the trait is true, otherwise it is false.
                              
                           </p>
                           <p class="p">
                              These traits can be used in all compilation modes, irrespective of whether lambdas or extended lambdas are
                              enabled<a name="fnsrc_29" href="#fntarg_29" shape="rect"><sup>29</sup></a>.  
                              
                           </p>
                           <p class="p">Example:</p><pre xml:space="preserve">
#define IS_D_LAMBDA(X) __nv_is_extended_device_lambda_closure_type(X)
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define IS_HD_LAMBDA(X) __nv_is_extended_host_device_lambda_closure_type(X)</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam0 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] { }; 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// lam0 is not an extended lambda (since defined outside function scope)</span>
  static_assert(!IS_D_LAMBDA(decltype(lam0)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);
  static_assert(!IS_HD_LAMBDA(decltype(lam0)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// lam1 is not an extended lambda (since no execution space annotations)</span>
  static_assert(!IS_D_LAMBDA(decltype(lam1)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);
  static_assert(!IS_HD_LAMBDA(decltype(lam1)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// lam2 is an extended __device__ lambda</span>
  static_assert(IS_D_LAMBDA(decltype(lam2)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);
  static_assert(!IS_HD_LAMBDA(decltype(lam2)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// lam3 is an extended __host__ __device__ lambda</span>
  static_assert(!IS_D_LAMBDA(decltype(lam3)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);
  static_assert(IS_HD_LAMBDA(decltype(lam3)), <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">""</span>);
}</pre></div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="extended-lambda-restrictions"><a name="extended-lambda-restrictions" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#extended-lambda-restrictions" name="extended-lambda-restrictions" shape="rect">H.6.2.&nbsp;Extended Lambda Restrictions</a></h3>
                        <div class="body conbody">
                           <p class="p">The CUDA compiler will replace an extended lambda expression with an instance of a
                              placeholder type defined in namespace scope, before invoking the host compiler. The
                              template argument of the placeholder type 
                              requires taking the address of a function enclosing the original extended lambda
                              expression. This is required for the
                              correct execution of any <samp class="ph codeph">__global__ </samp> function template whose template argument
                              involves the closure type of an extended lambda. The <dfn class="term">enclosing function</dfn> is 
                              computed as follows.
                              
                           </p>
                           <p class="p">By definition, the extended lambda
                              is present within the immediate or nested block scope of a <samp class="ph codeph">__host__</samp> or 
                              <samp class="ph codeph">__host__ __device__</samp> function. If this function is not the 
                              <samp class="ph codeph">operator()</samp> of a lambda expression, then it
                              is considered the enclosing function for the extended lambda. Otherwise,
                              the extended lambda is defined within the immediate or nested block scope of
                              the <samp class="ph codeph">operator()</samp> of one or more enclosing lambda expressions. If the outermost
                              such lambda expression is defined in the immediate or nested block scope of
                              a function <samp class="ph codeph">F</samp>, then <samp class="ph codeph">F</samp> is the computed enclosing function, else the enclosing function
                              does not exist.
                              
                           </p>
                           <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// enclosing function for lam1 is "foo"</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] {
     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [] {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// enclosing function for lam4 is "foo"</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
     };
  };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam6 = [] {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// enclosing function for lam7 does not exist</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam7 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
};</pre><p class="p">Here are the restrictions on extended lambdas:</p>
                           <ol class="ol">
                              <li class="li">An extended lambda cannot be defined inside another extended lambda expression.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span>  {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: extended lambda defined within another extended lambda</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  };
}
</pre></li>
                              <li class="li">An extended lambda cannot be defined inside a generic lambda expression.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span>) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: extended lambda defined within a generic lambda</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  };
}
</pre></li>
                              <li class="li">If an extended lambda is defined within the immediate or nested block scope of
                                 one or more nested lambda expression, the outermost such lambda expression must
                                 be defined inside the immediate or nested block scope of a function.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = []  {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// error: outer enclosing lambda is not defined within a</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// non-lambda-operator() function. </span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
};
</pre></li>
                              <li class="li"> The enclosing function for the extended lambda must be named and its address can be taken. If the
                                 enclosing function is a class member, then the following conditions must be satisfied:
                                 
                                 <ul class="ul">
                                    <li class="li">All classes enclosing the member function must have a name.</li>
                                    <li class="li">The member function must not have private or protected access within its parent class.</li>
                                    <li class="li">All enclosing classes must not have private or protected access within their respective parent classes.</li>
                                 </ul>
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; };
  {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; };
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; };
  }
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {
  S1_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: cannot take address of enclosing function</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; }; 
  }
};

class C0_t {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: enclosing function has private access in parent class</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> temp1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
  }
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S2_t {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: enclosing class S2_t has private access in its </span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// parent class</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> temp1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
    }
  };
};
</pre></li>
                              <li class="li">
                                 It must be possible to take the address of the enclosing routine unambigously, at the point where the 
                                 extended lambda has been defined. This may not be feasible in some cases e.g. when a class typedef shadows
                                 a template type argument of the same name.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
template &lt;typename&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> A {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Bar;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> test();
};

template&lt;&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> A&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>&gt; { };

template &lt;typename Bar&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> A&lt;Bar&gt;::test() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* In code sent to host compiler, nvcc will inject an
     address expression here, of the form: 
     (void (A&lt; Bar&gt; ::*)(void))(&amp;A::test))
 
     However, the class typedef 'Bar' (to void) shadows the
     template argument 'Bar', causing the address 
     expression in A&lt;int&gt;::test to actually refer to:
     (void (A&lt; void&gt; ::*)(void))(&amp;A::test))
    
     ..which doesn't take the address of the enclosing
     routine 'A&lt;int&gt;::test' correctly.
  */</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 4; };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
  A&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt; xxx;
  xxx.test();
}
</pre></li>
                              <li class="li"> An extended  lambda cannot be defined in a class that is local to a function.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: bar is member of a class that is local to a function.</span>
      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; }; 
    }
  };
}</pre></li>
                              <li class="li"> The enclosing function for an extended lambda cannot have deduced return type.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: the return type of foo is deduced.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; }; 
}
</pre></li>
                              <li class="li"> __host__ __device__ extended lambdas cannot be generic lambdas.
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: __host__ __device__ extended lambdas cannot be</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// generic lambdas.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> i) { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> i; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: __host__ __device__ extended lambdas cannot be</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// generic lambdas.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> ...i) {
               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>...(i);
              };
}
</pre></li>
                              <li class="li">If the enclosing function is an instantiation of a function template or
                                 a member function template, and/or the function is a member of a class template,
                                 the template(s) must satisfy the following constraints:
                                 
                                 <ul class="ul">
                                    <li class="li">The template must have at most one variadic parameter, and it must
                                       be listed last in the template parameter list.
                                       
                                    </li>
                                    <li class="li">The template parameters must be named. </li>
                                    <li class="li">The template instantiation argument types cannot involve types that are either local
                                       to a function (except for closure types for extended lambdas), 
                                       or are private or protected class members.
                                    </li>
                                 </ul>
                                 <p class="p">Example:</p><pre xml:space="preserve">
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern(T in) { in(); }

template &lt;typename... T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> foo {};

template &lt; template &lt;typename...&gt; class T, typename... P1, 
          typename... P2&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar1(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T&lt;P1...&gt;, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T&lt;P2...&gt;) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: enclosing function has multiple parameter packs</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 =  [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
}

template &lt; template &lt;typename...&gt; class T, typename... P1, 
          typename T2&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar2(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> T&lt;P1...&gt;, T2) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: for enclosing function, the</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// parameter pack is not last in the template parameter list.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 =  [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
}

template &lt;typename T, T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar3(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: for enclosing function, the second template</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// parameter is not named.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 =  [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
  foo&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>&gt; f1;
  foo&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt; f2;
  bar1(f1, f2);
  bar2(f1, 10);
  bar3&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>, 10&gt;();
}</pre><p class="p">Example:</p><pre xml:space="preserve">
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern(T in) { in(); }

template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar4(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 =  [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam1);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> C1_t { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { }; friend <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>); };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: enclosing function for device lambda in bar4</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// is instantiated with a type local to main.</span>
  bar4&lt;S1_t&gt;();

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: enclosing function for device lambda in bar4</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// is instantiated with a type that is a private member</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// of a class.</span>
  bar4&lt;C1_t::S1_t&gt;();
}
</pre></li>
                              <li class="li">
                                 With Visual Studio 2013 and later Visual Studio host compilers, the enclosing function
                                 must have external linkage. The restriction is present because
                                 this host compiler does not support using the address of non-extern linkage
                                 functions as template arguments, which is needed by the CUDA compiler 
                                 transformations to support extended lambdas.
                                 
                              </li>
                              <li class="li">An extended lambda has the following restrictions on captured variables:
                                 
                                 <ul class="ul">
                                    <li class="li">In the code sent to the host compiler, the variable may be passed by value to a sequence of helper
                                       functions before being used to direct-initialize the field of the class type used to represent the
                                       closure type for the extended lambda<a name="fnsrc_30" href="#fntarg_30" shape="rect"><sup>30</sup></a>.
                                    </li>
                                    <li class="li">A variable can only be captured by value. </li>
                                    <li class="li">A variable of array type cannot be captured if the number of array dimensions is greater than 7.</li>
                                    <li class="li">For a variable of array type, in the code sent to the host compiler, the closure type's array field
                                       is first default-initialized, and then each element of the array field is copy-assigned from the corresponding
                                       element of the captured array variable. Therefore, the array element type must be default-constructible and
                                       copy-assignable in host code. 
                                    </li>
                                    <li class="li">A function parameter that is an element of a variadic argument pack cannot
                                       be captured.
                                    </li>
                                    <li class="li">The type of the captured variable cannot involve types that are either local
                                       to a function (except for closure types of extended lambdas), 
                                       or are private or protected class members.
                                    </li>
                                    <li class="li">For a __host__ __device__ extended lambda, the types used in the return or parameter types of 
                                       the lambda expression's <samp class="ph codeph">operator()</samp> cannot involve types that are either local
                                       to a function (except for closure types of extended lambdas), 
                                       or are private or protected class members.
                                    </li>
                                    <li class="li">Init-capture is not supported for __host__ __device__ extended lambdas.
                                       Init-capture is supported for __device__ extended lambdas, except when the init-capture
                                       is of array type or of type <samp class="ph codeph">std::initializer_list</samp>.
                                    </li>
                                    <li class="li">The function call operator for an extended lambda is not constexpr. The closure type for an
                                       extended lambda is not a literal type. The constexpr specifier
                                       cannot be used in the declaration of an extended lambda.
                                    </li>
                                 </ul>
                                 <p class="p">Example</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: an init-capture is allowed for an</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// extended __device__ lambda.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [x = 1] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> x; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: an init-capture is not allowed for</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// an extended __host__ __device__ lambda.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [x = 1] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> x; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> a = 1;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: an extended __device__ lambda cannot capture</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// variables by reference.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [&amp;a] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> a; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: by-reference capture is not allowed</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// for an extended __device__ lambda.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam4 = [&amp;x = a] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> x; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { };
  S1_t s1;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: a type local to a function cannot be used in the type</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// of a captured variable.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam6 = [s1] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: an init-capture cannot be of type std::initializer_list.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam7 = [x = {11}] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { };

  std::initializer_list&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt; b = {11,22,33};
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: an init-capture cannot be of type std::initializer_list.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam8 = [x = b] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> () { }; 
}
</pre></li>
                              <li class="li">When parsing a function, the CUDA compiler assigns
                                 a counter value to each extended lambda within that function. 
                                 This counter value is used in the
                                 substituted named type passed to the host compiler. Hence, whether or not
                                 an extended lambda is defined within a function should not depend 
                                 on a particular value of <samp class="ph codeph">__CUDA_ARCH__</samp>, or on <samp class="ph codeph">__CUDA_ARCH__</samp> being undefined.
                                 
                                 <p class="p">Example</p><pre xml:space="preserve">
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel(T in) { in(); }

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: the number and relative declaration</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// order of extended lambdas depends on</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// __CUDA_ARCH__</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#if defined(__CUDA_ARCH__)</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0; };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1b = [] __host___ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 4; };
  kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam2);
}
</pre></li>
                              <li class="li">As described above, the CUDA compiler replaces a <samp class="ph codeph">__device__</samp> extended lambda
                                 defined in a host function with a placeholder type defined in namespace scope.
                                 This placeholder type does not define a <samp class="ph codeph">operator()</samp> function equivalent to the
                                 original lambda declaration. An attempt to
                                 determine the return type or parameter types of the <samp class="ph codeph">operator()</samp> function
                                 may therefore work incorrectly in host code, as the code processed by the host compiler
                                 will be semantically different than the input code processed by the CUDA
                                 compiler. However, it is ok to introspect the return type or parameter types
                                 of the <samp class="ph codeph">operator()</samp> function within device code. Note that this restriction
                                 does not apply to <samp class="ph codeph"> __host__ __device__ </samp> extended lambdas.
                                 
                                 <p class="p">Example</p><pre xml:space="preserve">
#include &lt;type_traits&gt;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: attempt to extract the return type</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// of a __device__ lambda in host code</span>
  std::result_of&lt;decltype(lam1)()&gt;::type xx1 = 1;


  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span>  { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK : lam2 represents a __host__ __device__ extended lambda</span>
  std::result_of&lt;decltype(lam2)()&gt;::type xx2 = 1;
}
</pre></li>
                              <li class="li">If the functor object represented by an extended lambda is passed from host to device
                                 code (e.g., as the argument of a <samp class="ph codeph">__global__</samp> function), then any expression in the body of the
                                 lambda expression that captures variables must be remain unchanged irrespective of whether the
                                 <samp class="ph codeph">__CUDA_ARCH__</samp> macro is defined, and whether the macro has a particular value.
                                 This restriction arises because the lambda's closure
                                 class layout depends on the order in which captured variables are encountered when the 
                                 compiler processes the lambda expression; the program may execute incorrectly if the
                                 closure class layout differs in device and host compilation.
                                 
                                 
                                 <p class="p">Example</p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> result;
   
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel(T in) { result = in(); }
   
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x1 = 1;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: "x1" is only captured when __CUDA_ARCH__ is defined.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#ifdef __CUDA_ARCH__</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> x1 + 1;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#else	</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 10; 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif       </span>
  };
  kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam1);
}
</pre></li>
                              <li class="li">
                                 As described previously, the CUDA compiler replaces an extended <samp class="ph codeph"> __device__</samp> lambda expression with an instance of
                                 a placeholder type in the code sent to the host compiler. This placeholder type does not define a pointer-to-function
                                 conversion operator in host code, however the conversion operator is provided in device code. Note that this restriction
                                 does not apply to <samp class="ph codeph">__host__ __device__</samp> extended lambdas.
                                 
                                 
                                 <p class="p">Example</p><pre xml:space="preserve">
template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kern(T in) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*fp)(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>) = in;

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: conversion in device code is supported</span>
  fp(0);
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [](<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>) { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; };

  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: conversion in device code is supported</span>
  fp = lam1;
  fp(0);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam_d = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>) { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; };
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam_hd = [] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>) { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 1; };
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam_d);
  kern<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam_hd);
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK : conversion for __host__ __device__ lambda is supported</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// in host code</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*fp)(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>) = lam_hd;
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: conversion for __device__ lambda is not supported in</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// host code.</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> (*fp2)(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>) = lam_d;
}
</pre></li>
                           </ol>
                           <p class="p">The CUDA compiler will generate compiler diagnostics for a subset of cases
                              described in 1-11; no diagnostic will be generated for cases 12-15, but the host
                              compiler may fail to compile the generated code.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="host-device-lambda-notes"><a name="host-device-lambda-notes" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#host-device-lambda-notes" name="host-device-lambda-notes" shape="rect">H.6.3.&nbsp;Notes on <samp class="ph codeph">__host__ __device__ </samp> lambdas</a></h3>
                        <div class="body conbody">
                           <p class="p">Unlike <samp class="ph codeph">__device__</samp> lambdas, <samp class="ph codeph">__host__ __device__</samp> lambdas can be called from host code. 
                              As described earlier, the CUDA compiler replaces an extended lambda expression defined in
                              host code with an instance of a named placeholder type. The placeholder type for an extended
                              <samp class="ph codeph">__host__ __device__</samp> lambda invokes the orignal lambda's <samp class="ph codeph">operator()</samp> with an indirect
                              function call <a name="fnsrc_31" href="#fntarg_31" shape="rect"><sup>31</sup></a>.
                              
                           </p>
                           <p class="p">
                              The presence of the indirect function call may cause an extended <samp class="ph codeph">__host__ __device__</samp> lambda
                              to be less optimized  by the host compiler than
                              lambdas that are implicitly or explicitly <samp class="ph codeph">__host__</samp> only. In the latter case, the host compiler
                              can easily inline the body of the lambda into the calling context. But in case of an extended <samp class="ph codeph">__host__
                                 __device__</samp> lambda, the host compiler encounters the indirect function call and may not 
                              be able to easily inline the original <samp class="ph codeph">__host__ __device__</samp> lambda body.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="star-this-capture"><a name="star-this-capture" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#star-this-capture" name="star-this-capture" shape="rect">H.6.4.&nbsp;*this Capture By Value</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              When a lambda is defined within a non-static class member function, and the body of the lambda refers to 
                              a class member variable, C++11/C++14 rules require that the <samp class="ph codeph">this</samp> pointer of the class is captured by value,
                              instead of the referenced member variable. If the lambda is an extended <samp class="ph codeph">__device__</samp> or
                              <samp class="ph codeph">__host__</samp><samp class="ph codeph">__device__</samp> lambda defined in a host function,
                              and the lambda is executed on the GPU, accessing the referenced member variable on the GPU will cause a
                              run time error if the <samp class="ph codeph">this</samp> pointer points to host memory.
                              
                           </p>
                           <p class="p">Example:</p><pre xml:space="preserve">
#include &lt;cstdio&gt;

template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(T in) { printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"\n value = %d"</span>, in()); }

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> S1_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) : xxx(10) { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> doit(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { 
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// reference to "xxx" causes </span>
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the 'this' pointer (S1_t*) to be captured by value</span>
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx + 1; 
      
    };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel launch fails at run time because 'this-&gt;xxx'</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// is not accessible from the GPU</span>
    foo<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam1);
    cudaDeviceSynchronize();
  }
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  S1_t s1;
  s1.doit();
}
</pre><p class="p">
                              C++17 solves this problem by adding a new "*this" capture mode. In this
                              mode, the compiler makes a copy of the object denoted by "*this"
                              instead of capturing the pointer <samp class="ph codeph">this</samp> by value. The "*this" capture
                              mode is described in more detail here:
                              <samp class="ph codeph">http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html
                                 </samp>
                              . 
                              
                           </p>
                           <p class="p">
                              The CUDA compiler supports
                              the "*this" capture mode for lambdas defined within <samp class="ph codeph">__device__</samp> 
                              and <samp class="ph codeph">__global__</samp>
                              functions and for extended <samp class="ph codeph">__device__</samp> lambdas defined in host code, when the 
                              <samp class="ph codeph">--extended-lambda</samp> nvcc flag is used.
                              
                           </p>
                           <p class="p">
                              Here's the above example modified to use "*this" capture mode:
                              
                           </p><pre xml:space="preserve">
#include &lt;cstdio&gt;

template &lt;typename T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(T in) { printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"\n value = %d"</span>, in()); }

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> S1_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) : xxx(10) { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> doit(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// note the "*this" capture specification</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { 
      
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// reference to "xxx" causes </span>
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// the object denoted by '*this' to be captured by</span>
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// value, and the GPU code will access copy_of_star_this-&gt;xxx</span>
       <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx + 1; 
      
    };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Kernel launch succeeds</span>
    foo<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>1,1<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(lam1);
    cudaDeviceSynchronize();
  }
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
  S1_t s1;
  s1.doit();
}
</pre><p class="p">
                              "*this" capture mode is not allowed for unannotated lambdas defined in host code,
                              or for extended <samp class="ph codeph">__host__</samp><samp class="ph codeph">__device__</samp> lambdas. Examples of supported and unsupported
                              usage:
                              
                           </p><pre xml:space="preserve">
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { 
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> xxx;
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> S1_t(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) : xxx(10) { };
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: use in an extended __device__ lambda</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: use in an extended __host__ __device__ lambda</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: use in an unannotated lambda in host function</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [=, *this]  { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
  }
  
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> device_func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: use in a lambda defined in a __device__ function</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: use in a lambda defined in a __device__ function</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: use in a lambda defined in a __device__ function</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [=, *this]  { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
  }
  
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> host_device_func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) {
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// OK: use in an extended __device__ lambda</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: use in an extended __host__ __device__ lambda</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam2 = [=, *this] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__host__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error: use in an unannotated lambda in a __host__ __device__ function</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam3 = [=, *this]  { <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> xxx; };
  }
};
</pre></div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="extended-lambda-notes"><a name="extended-lambda-notes" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#extended-lambda-notes" name="extended-lambda-notes" shape="rect">H.6.5.&nbsp;Additional Notes</a></h3>
                        <div class="body conbody">
                           <ol class="ol">
                              <li class="li"><samp class="ph codeph">ADL Lookup</samp>: As described earlier, the CUDA compiler will replace 
                                 an extended lambda expression with an instance of
                                 a placeholder type, before invoking the host compiler. One template argument of the placeholder type
                                 uses the address of the function enclosing the original lambda expression. This may cause additional
                                 namespaces to participate in argument
                                 dependent lookup (ADL), for any host function call whose argument types involve the closure type
                                 of the extended lambda expression. This may cause an incorrect function to be selected by the host
                                 compiler.
                                 
                                 
                                 <p class="p">Example:</p><pre xml:space="preserve">
namespace N1 {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> S1_t { };
  template &lt;typename T&gt;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(T);
};
 
namespace N2 {
  template &lt;typename T&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> foo(T);
 
  template &lt;typename T&gt;  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> doit(T in) {     foo(in);  }
}
 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> bar(N1::S1_t in) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">/* extended __device__ lambda. In the code sent to the host compiler, this 
     is replaced with the placeholder type instantiation expression
     ' __nv_dl_wrapper_t&lt; __nv_dl_tag&lt;void (*)(N1::S1_t in),(&amp;bar),1&gt; &gt; { }'
   
     As a result, the namespace 'N1' participates in ADL lookup of the 
     call to "foo" in the body of N2::doit, causing ambiguity.
  */</span>
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">auto</span> lam1 = [=] <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> { };
  N2::doit(lam1);
}
</pre><p class="p">
                                    In the example above, the CUDA compiler replaced the extended lambda with a placeholder type
                                    that involves the <samp class="ph codeph">N1</samp> namespace. As a result, the namespace <samp class="ph codeph">N1</samp>
                                    participates in the ADL lookup for <samp class="ph codeph">foo(in)</samp> in the body of <samp class="ph codeph">N2::doit</samp>, and host
                                    compilation fails because multiple overload candidates <samp class="ph codeph">N1::foo</samp> and <samp class="ph codeph">N2::foo</samp> are found.
                                    
                                 </p>
                              </li>
                           </ol>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="code-samples"><a name="code-samples" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#code-samples" name="code-samples" shape="rect">H.7.&nbsp;Code Samples</a></h3>
                     <div class="topic reference nested2" xml:lang="en-US" id="data-aggregation-class"><a name="data-aggregation-class" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#data-aggregation-class" name="data-aggregation-class" shape="rect">H.7.1.&nbsp;Data Aggregation Class</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">class PixelRGBA {
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> PixelRGBA(): r_(0), g_(0), b_(0), a_(0) { }
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> PixelRGBA(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> r, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> g,
                         <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> b, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> a = 255):
                         r_(r), g_(g), b_(b), a_(a) { }
    
private:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> r_, g_, b_, a_;
    
    friend PixelRGBA operator+(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> PixelRGBA&amp;, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> PixelRGBA&amp;);
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> 
PixelRGBA operator+(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> PixelRGBA&amp; p1, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> PixelRGBA&amp; p2)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> PixelRGBA(p1.r_ + p2.r_, p1.g_ + p2.g_, 
                     p1.b_ + p2.b_, p1.a_ + p2.a_);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> func(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>)
{
    PixelRGBA p1, p2;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ...      // Initialization of p1 and p2 here</span>
    PixelRGBA p3 = p1 + p2;
}</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="derived-class"><a name="derived-class" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#derived-class" name="derived-class" shape="rect">H.7.2.&nbsp;Derived Class</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* operator new(size_t bytes, MemoryPool&amp; p);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> operator delete(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*, MemoryPool&amp; p);
class Shape {
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Shape(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>) { }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> putThis(PrintBuffer *p) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> virtual <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Draw(PrintBuffer *p) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> {
         p-&gt;put(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Shapeless"</span>); 
    }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> virtual ~Shape() {}
};
class Point : public Shape {
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Point() : x(0), y(0) {}
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Point(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> ix, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> iy) : x(ix), y(iy) { }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> PutCoord(PrintBuffer *p) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> Draw(PrintBuffer *p) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> ~Point() {}
private:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, y;
};
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> Shape* GetPointObj(MemoryPool&amp; pool)
{
    Shape* shape = new(pool) Point(rand(-20,10), rand(-100,-20));
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> shape;
}</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="class-template"><a name="class-template" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#class-template" name="class-template" shape="rect">H.7.3.&nbsp;Class Template</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">template &lt;class T&gt;
class myValues {
    T values[MAX_VALUES];
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> myValues(T clear) { ... }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> setValue(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> Idx, T value) { ... }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> putToMemory(T* valueLocation) { ... }
};

template &lt;class T&gt;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> useValues(T* memoryBuffer) {
    myValues&lt;T&gt; myLocation(0);
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* buffer;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    ...
    useValues&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>blocks, threads<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(buffer);
    ...
}</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="function-template"><a name="function-template" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#function-template" name="function-template" shape="rect">H.7.4.&nbsp;Function Template</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">template &lt;typename T&gt; 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> bool func(T x) 
{
   ...
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> (...);
}

template &lt;&gt; 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> bool func&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>&gt;(T x) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Specialization</span>
{
   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> true;
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Explicit argument specification</span>
bool result = func&lt;<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">double</span>&gt;(0.5);

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Implicit argument deduction</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x = 1;
bool result = func(x);</pre></div>
                        </div>
                     </div>
                     <div class="topic reference nested2" xml:lang="en-US" id="functor-class"><a name="functor-class" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#functor-class" name="functor-class" shape="rect">H.7.5.&nbsp;Functor Class</a></h3>
                        <div class="body refbody">
                           <div class="section refsyn"><pre xml:space="preserve">class Add {
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> operator() (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> b) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>
    {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> a + b;
    }
};

class Sub {
public:
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> operator() (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> b) <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span>
    {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> a - b;
    }
};

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
template&lt;class O&gt; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> VectorOperation(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> * A, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> * B, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> * C,
                     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N, O op)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> iElement = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span>.x * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockIdx</span>.x + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (iElement &lt; N)
        C[iElement] = op(A[iElement], B[iElement]);
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    ...
    VectorOperation<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>blocks, threads<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(v1, v2, v3, N, Add());
    ...
}</pre></div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="texture-fetching"><a name="texture-fetching" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#texture-fetching" name="texture-fetching" shape="rect">I.&nbsp;Texture Fetching</a></h2>
                  <div class="body conbody">
                     <p class="p">This appendix gives the formula used to compute the value returned by
                        the texture functions of <a class="xref" href="index.html#texture-functions" shape="rect">Texture Functions</a> depending on
                        the various attributes of the texture reference (see <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>).
                     </p>
                     <p class="p">The texture bound to the texture reference is represented as an array
                        <em class="ph i">T</em> of 
                     </p>
                     <ul class="ul">
                        <li class="li"><em class="ph i">N</em> texels for a one-dimensional texture,
                        </li>
                        <li class="li"><em class="ph i">N x M</em> texels for a two-dimensional texture,
                        </li>
                        <li class="li"><em class="ph i">N x M x L</em> texels for a three-dimensional texture.
                        </li>
                     </ul>
                     <p class="p">It is fetched using non-normalized texture coordinates <em class="ph i">x</em>,
                        <em class="ph i">y</em>, and <em class="ph i">z</em>, or the normalized texture coordinates <em class="ph i">x/N</em>,
                        <em class="ph i">y/M</em>, and <em class="ph i">z/L</em> as described in <a class="xref" href="index.html#texture-memory" shape="rect">Texture Memory</a>. In this appendix, the coordinates are
                        assumed to be in the valid range. <a class="xref" href="index.html#texture-memory" shape="rect">Texture Memory</a>
                        explained how out-of-range coordinates are remapped to the valid range
                        based on the addressing mode.
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="nearest-point-sampling"><a name="nearest-point-sampling" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#nearest-point-sampling" name="nearest-point-sampling" shape="rect">I.1.&nbsp;Nearest-Point Sampling</a></h3>
                     <div class="body conbody">
                        <p class="p">In this filtering mode, the value returned by the texture fetch is</p>
                        <ul class="ul">
                           <li class="li"><em class="ph i">tex(x)=T[i]</em> for a one-dimensional texture,
                           </li>
                           <li class="li"><em class="ph i">tex(x,y)=T[i,j]</em> for a two-dimensional texture,
                           </li>
                           <li class="li"><em class="ph i">tex(x,y,z)=T[i,j,k]</em> for a three-dimensional texture,
                           </li>
                        </ul>
                        <p class="p">where <em class="ph i">i=floor(x)</em>, <em class="ph i">j=floor(y)</em>, and <em class="ph i">k=floor(z)</em>.
                        </p>
                        <p class="p"><a class="xref" href="index.html#nearest-point-sampling__nearest-point-sampling-fig" title="Nearest-point sampling of a one-dimensional texture of four texels." shape="rect">Figure 13</a>
                           illustrates nearest-point sampling for a one-dimensional texture with
                           <em class="ph i">N=4</em>.
                        </p>
                        <p class="p">For integer textures, the value returned by the texture fetch can be
                           optionally remapped to [0.0, 1.0] (see <a class="xref" href="index.html#texture-memory" shape="rect">Texture Memory</a>).
                        </p>
                        <div class="fig fignone" id="nearest-point-sampling__nearest-point-sampling-fig"><a name="nearest-point-sampling__nearest-point-sampling-fig" shape="rect">
                              <!-- --></a><span class="figcap">Figure 13. Nearest-Point Sampling Filtering Mode</span>. <span class="desc figdesc">Nearest-point sampling of a one-dimensional texture of four
                              texels.</span><br clear="none"></br><img class="image" src="graphics/nearest-point-sampling-of-1-d-texture-of-4-texels.png" alt="Nearest-Point Sampling of a One-Dimensional Texture of Four Texels."></img><br clear="none"></br></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="linear-filtering"><a name="linear-filtering" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#linear-filtering" name="linear-filtering" shape="rect">I.2.&nbsp;Linear Filtering</a></h3>
                     <div class="body conbody">
                        <p class="p">In this filtering mode, which is only available for floating-point
                           textures, the value returned by the texture fetch is
                        </p>
                        <ul class="ul">
                           <li class="li"><em class="ph i">tex(x)=(1)T[i]+T[i+1]</em> for a one-dimensional texture,
                           </li>
                           <li class="li"><em class="ph i">tex(x,y)=(1)(1)T[i,j]+(1)T[i+1,j]+(1)T[i,j+1]+T[i+1,j+1]</em>
                              for a two-dimensional texture,
                           </li>
                           <li class="li">
                              <p class="p"><em class="ph i">tex(x,y,z)</em> =
                              </p>
                              <p class="p"><em class="ph i">(1)(1)(1)T[i,j,k]+(1)(1)T[i+1,j,k]+</em></p>
                              <p class="p"><em class="ph i">(1)(1)T[i,j+1,k]+(1)T[i+1,j+1,k]+</em></p>
                              <p class="p"><em class="ph i">(1)(1)T[i,j,k+1]+(1)T[i+1,j,k+1]+</em></p>
                              <p class="p"><em class="ph i">(1)T[i,j+1,k+1]+T[i+1,j+1,k+1]</em></p>
                              <p class="p">for a three-dimensional texture,</p>
                           </li>
                        </ul>
                        <p class="p">where:</p>
                        <ul class="ul">
                           <li class="li"><em class="ph i">i=floor(x<sub class="ph sub">B</sub>)</em>, <em class="ph i">=frac(x<sub class="ph sub">B</sub>)</em>,
                              <em class="ph i">x<sub class="ph sub">B</sub>=x-0.5,</em></li>
                           <li class="li"><em class="ph i">j=floor(y<sub class="ph sub">B</sub>)</em>, <em class="ph i">=frac(y<sub class="ph sub">B</sub>)</em>,
                              <em class="ph i">y<sub class="ph sub">B</sub>=y-0.5</em>,
                           </li>
                           <li class="li"><em class="ph i">k=floor(z<sub class="ph sub">B</sub>)</em>, <em class="ph i">=frac(z<sub class="ph sub">B</sub>)</em>,
                              <em class="ph i">z<sub class="ph sub">B</sub>= z-0.5</em>,
                           </li>
                        </ul>
                        <p class="p"><em class="ph i"></em>, <em class="ph i"></em>, and <em class="ph i"></em> are stored in 9-bit fixed point format
                           with 8 bits of fractional value (so 1.0 is exactly represented).
                        </p>
                        <p class="p"><a class="xref" href="index.html#linear-filtering__linear-filtering-of-1-d-texture-of-4-texels" title="Linear filtering of a one-dimensional texture of four texels in clamp addressing mode." shape="rect">Figure 14</a>
                           illustrates linear filtering of a one-dimensional texture with
                           <em class="ph i">N=4</em>.
                        </p>
                        <div class="fig fignone" id="linear-filtering__linear-filtering-of-1-d-texture-of-4-texels"><a name="linear-filtering__linear-filtering-of-1-d-texture-of-4-texels" shape="rect">
                              <!-- --></a><span class="figcap">Figure 14. Linear Filtering Mode</span>. <span class="desc figdesc">Linear filtering of a one-dimensional texture of four texels in
                              clamp addressing mode.</span><br clear="none"></br><img class="image" src="graphics/linear-filtering-of-1-d-texture-of-4-texels.png" alt="Linear         Filtering of a One-Dimensional Texture of Four Texels in Clamp         Addressing Mode."></img><br clear="none"></br></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="table-lookup"><a name="table-lookup" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#table-lookup" name="table-lookup" shape="rect">I.3.&nbsp;Table Lookup</a></h3>
                     <div class="body conbody">
                        <p class="p">A table lookup <em class="ph i">TL(x)</em> where <em class="ph i">x</em> spans the interval
                           <em class="ph i">[0,R]</em> can be implemented as <em class="ph i">TL(x)=tex((N-1)/R)x+0.5)</em> in
                           order to ensure that <em class="ph i">TL(0)=T[0]</em> and <em class="ph i">TL(R)=T[N-1]</em>.
                        </p>
                        <p class="p"><a class="xref" href="index.html#table-lookup__1-d-table-lookup-using-linear-filtering" shape="rect">Figure 15</a>
                           illustrates the use of texture filtering to implement a table lookup with
                           <em class="ph i">R=4</em> or <em class="ph i">R=1</em> from a one-dimensional texture with
                           <em class="ph i">N=4</em>.
                        </p>
                        <div class="fig fignone" id="table-lookup__1-d-table-lookup-using-linear-filtering"><a name="table-lookup__1-d-table-lookup-using-linear-filtering" shape="rect">
                              <!-- --></a><span class="figcap">Figure 15. One-Dimensional Table Lookup Using Linear Filtering</span><br clear="none"></br><img class="image" src="graphics/1-d-table-lookup-using-linear-filtering.png" alt="One-Dimensional Table Lookup Using Linear Filtering."></img><br clear="none"></br></div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="compute-capabilities"><a name="compute-capabilities" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#compute-capabilities" name="compute-capabilities" shape="rect">J.&nbsp;Compute Capabilities</a></h2>
                  <div class="body conbody">
                     <p class="p">
                        The general specifications and features of a compute device depend on its compute capability (see <a class="xref" href="index.html#compute-capability" shape="rect">Compute Capability</a>).
                        
                     </p>
                     <p class="p"><a class="xref" href="index.html#features-and-technical-specifications__feature-support-per-compute-capability" shape="rect">Table 14</a> and 
                        <a class="xref" href="index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">Table 15</a> show
                        the features and technical specifications associated with each compute capability that is currently supported.
                        
                     </p>
                     <p class="p"><a class="xref" href="index.html#floating-point-standard" shape="rect">Floating-Point Standard</a> reviews the compliance with the IEEE floating-point standard.
                        
                     </p>
                     <p class="p"> Sections <a class="xref" href="index.html#compute-capability-3-0" shape="rect">Compute Capability 3.x</a>, 
                        <a class="xref" href="index.html#compute-capability-5-x" shape="rect">Compute Capability 5.x</a>,
                        <a class="xref" href="index.html#compute-capability-6-x" shape="rect">Compute Capability 6.x</a>,
                        <a class="xref" href="index.html#compute-capability-7-x" shape="rect">Compute Capability 7.x</a> and 
                        <a class="xref" href="index.html#compute-capability-8-x" shape="rect">Compute Capability 8.x</a> give more details on the architecture of
                        devices of compute capability 3.x, 5.x, 6.x, 7.x and 8.x respectively. 
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="features-and-technical-specifications"><a name="features-and-technical-specifications" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#features-and-technical-specifications" name="features-and-technical-specifications" shape="rect">J.1.&nbsp;Features and Technical Specifications</a></h3>
                     <div class="body conbody">
                        <div class="tablenoborder"><a name="features-and-technical-specifications__feature-support-per-compute-capability" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="features-and-technical-specifications__feature-support-per-compute-capability" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 14. Feature Support per Compute Capability</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="50%" id="d225e31753" rowspan="1" colspan="1">Feature Support</th>
                                    <th class="entry" colspan="5" align="center" valign="middle" id="d225e31756" rowspan="1">
                                       Compute Capability
                                       
                                    </th>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="50%" id="d225e31762" rowspan="1" colspan="1">
                                       (Unlisted features are
                                       supported for all compute capabilities)
                                       
                                    </th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d225e31765" rowspan="1" colspan="1">3.5, 3.7, 5.0, 5.2</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d225e31768" rowspan="1" colspan="1">5.3</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d225e31771" rowspan="1" colspan="1">6.x</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d225e31774" rowspan="1" colspan="1">7.x</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d225e31778" rowspan="1" colspan="1">8.x</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Atomic functions operating on 32-bit integer values in global memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Atomic functions operating on 32-bit integer values in shared memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Atomic functions operating on 64-bit integer values in global memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Atomic functions operating on 64-bit integer values in shared memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Atomic addition operating on 32-bit floating point values in global
                                       and shared memory (<a class="xref" href="index.html#atomicadd" shape="rect">atomicAdd()</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Atomic addition operating on 64-bit floating point values in global
                                       memory and shared memory (<a class="xref" href="index.html#atomicadd" shape="rect">atomicAdd()</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768" rowspan="1">No</td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e31756 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Warp vote functions (<a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a>)
                                       
                                    </td>
                                    <td class="entry" rowspan="6" colspan="5" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774 d225e31778">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Memory fence functions (<a class="xref" href="index.html#memory-fence-functions" shape="rect">Memory Fence Functions</a>)
                                       
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       
                                       Synchronization functions (<a class="xref" href="index.html#synchronization-functions" shape="rect">Synchronization Functions</a>)
                                       
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Surface functions (<a class="xref" href="index.html#surface-functions" shape="rect">Surface Functions</a>)
                                       
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">Unified Memory Programming (<a class="xref" href="index.html#um-unified-memory-programming-hd" shape="rect">Unified Memory Programming</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">Dynamic Parallelism (<a class="xref" href="index.html#cuda-dynamic-parallelism" shape="rect">CUDA Dynamic Parallelism</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Half-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion
                                       
                                    </td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e31756 d225e31765" rowspan="1">No</td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e31756 d225e31768 d225e31771 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Bfloat16-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion
                                       
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e31756 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">
                                       Tensor Cores
                                       
                                    </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771" rowspan="1">No</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e31756 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1">Mixed Precision Warp-Matrix Functions
                                       (<a class="xref" href="index.html#wmma" shape="rect">Warp matrix functions</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771" rowspan="1">No</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e31756 d225e31774 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1"> Hardware-accelerated <samp class="ph codeph">memcpy_async</samp>
                                       (<a class="xref" href="index.html#memcpy_async" shape="rect">Asynchronous Data Copies</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e31756 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1"> Hardware-accelerated Split Arrive/Wait Barrier
                                       (<a class="xref" href="index.html#aw-barrier" shape="rect">Asynchronous Barrier</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e31756 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d225e31753 d225e31762" rowspan="1" colspan="1"> L2 Cache Residency Management
                                       (<a class="xref" href="index.html#L2_access_intro" shape="rect">Device Memory L2 Access Management</a>)
                                       
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e31756 d225e31765 d225e31768 d225e31771 d225e31774" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e31756 d225e31778" rowspan="1">Yes</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>
                        <p class="p"> Note that the KB and K units used in the following table correspond to 1024 bytes (i.e., a KiB)
                           and 1024 respectively.
                           
                        </p>
                        <div class="tablenoborder"><a name="features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="features-and-technical-specifications__technical-specifications-per-compute-capability" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 15. Technical Specifications per Compute Capability</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="27.77777777777778%" id="d225e32066" rowspan="1" colspan="1">&nbsp;</th>
                                    <th class="entry" colspan="13" align="center" valign="middle" id="d225e32068" rowspan="1">
                                       Compute
                                       Capability
                                       
                                    </th>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="27.77777777777778%" id="d225e32074" rowspan="1" colspan="1">Technical Specifications</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32077" rowspan="1" colspan="1">3.5</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32080" rowspan="1" colspan="1">3.7</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32083" rowspan="1" colspan="1">5.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32086" rowspan="1" colspan="1">5.2</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32090" rowspan="1" colspan="1">5.3</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32093" rowspan="1" colspan="1">6.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32096" rowspan="1" colspan="1">6.1</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32099" rowspan="1" colspan="1">6.2</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32102" rowspan="1" colspan="1">7.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32105" rowspan="1" colspan="1">7.2</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32109" rowspan="1" colspan="1">7.5</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32112" rowspan="1" colspan="1">8.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.555555555555555%" id="d225e32115" rowspan="1" colspan="1">8.6</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of resident grids per device (<a class="xref" href="index.html#concurrent-kernel-execution" shape="rect">Concurrent Kernel Execution</a>) 
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32090" rowspan="1">16</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32093" rowspan="1">128</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32096" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32099" rowspan="1">16</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32102" rowspan="1">128</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32105" rowspan="1">16</td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32109 d225e32112 d225e32115" rowspan="1">128</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum dimensionality of grid of thread blocks </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">3</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum x-dimension of a grid of thread blocks </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">
                                       2<sup class="ph sup">31</sup>-1
                                       
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum y- or z-dimension of a grid of thread blocks</td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">65535</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum dimensionality of a thread block </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">3</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum x- or y-dimension of a block </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">1024</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum z-dimension of a block </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">64</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of threads per block </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">1024</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Warp size</td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">32</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of resident blocks per SM </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1">16</td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32109" rowspan="1">16</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32112" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32115" rowspan="1">16</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of resident warps per SM </td>
                                    <td class="entry" colspan="10" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105" rowspan="1">64</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32109" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32112" rowspan="1">64</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32115" rowspan="1">48</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of resident threads per SM </td>
                                    <td class="entry" colspan="10" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105" rowspan="1">2048</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32109" rowspan="1">1024</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32112" rowspan="1">2048</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32115" rowspan="1">1536</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Number of 32-bit registers per SM
                                       
                                    </td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32077" rowspan="1">64 K</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32080" rowspan="1">128 K</td>
                                    <td class="entry" colspan="11" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">64 K</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of 32-bit registers per thread block
                                       
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086" rowspan="1">64 K</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32090" rowspan="1">32 K</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096" rowspan="1">64 K</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32099" rowspan="1">32 K</td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">64 K</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of 32-bit registers per thread</td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">255</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum amount of shared memory per SM </td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32077" rowspan="1">48 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32080" rowspan="1">112 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32083" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32086" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32090 d225e32093" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32096" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32099" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32102 d225e32105" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32109" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32112" rowspan="1">164 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32115" rowspan="1">100 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum amount of shared memory per thread block
                                       <a name="fnsrc_32" href="#fntarg_32" shape="rect"><sup>32</sup></a></td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099" rowspan="1">48 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32102" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32105" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32109" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32112" rowspan="1">163 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32115" rowspan="1">99 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Number of shared memory banks </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">32</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum amount of local memory per thread </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">512 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Constant memory size </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">64 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Cache working set per SM for constant memory </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1">8 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32093" rowspan="1">4 KB</td>
                                    <td class="entry" colspan="7" align="center" valign="middle" headers="d225e32068 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">8 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Cache working set per SM for texture memory </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> Between 12 KB and 48 KB </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099" rowspan="1">Between 24 KB and 48 KB</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32102 d225e32105" rowspan="1">32 ~ 128 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32109" rowspan="1">32 or 64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32112" rowspan="1">28KB ~ 192 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32115" rowspan="1">28KB ~ 128 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum width for a 1D texture reference bound to a CUDA array </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1">65536</td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">131072</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum width for a 1D texture reference bound to linear memory </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1">2<sup class="ph sup">27</sup></td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32093" rowspan="1">2<sup class="ph sup">28</sup></td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32096 d225e32099" rowspan="1">2<sup class="ph sup">27</sup></td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32102" rowspan="1">2<sup class="ph sup">28</sup></td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d225e32068 d225e32105" rowspan="1">2<sup class="ph sup">27</sup></td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32109 d225e32112 d225e32115" rowspan="1">2<sup class="ph sup">28</sup></td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum width and number of layers for a 1D layered texture reference </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 x 2048 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width and height for a 2D texture reference bound to a CUDA
                                       array
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> 65536 x 65536 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">131072 x 65536</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width and height for a 2D texture reference bound to linear
                                       memory
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 65000 x 65000 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 65536 x 65536 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 131072 x 65000 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width and height for a 2D texture reference bound to a CUDA
                                       array supporting texture gather
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 x 16384 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 x 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width, height, and number of layers for a 2D layered texture
                                       reference
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 x 16384 x 2048 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 x 32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width, height, and depth for a 3D texture reference bound to
                                       a CUDA array
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> 4096 x 4096 x 4096 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 16384 x 16384 x 16384 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum width (and height) for a cubemap texture reference </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width (and height) and number of layers for a cubemap layered
                                       texture reference
                                       
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090" rowspan="1">16384 x 2046 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 x 2046 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of textures that can be bound to a kernel </td>
                                    <td class="entry" colspan="13" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">256</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum width for a 1D surface reference bound to a CUDA array </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 65536 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum width and number of layers for a 1D layered surface reference </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 65536 x 2048 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 x 2048 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width and height for a 2D surface reference bound to a CUDA
                                       array
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 65536 x 32768 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 65536 x 65536 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 131072 x 65536 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width, height, and number of layers for a 2D layered surface
                                       reference
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1">65536 x 32768 x 2048 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 x 16384 x 2048 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 x 32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width, height, and depth for a 3D surface reference bound to
                                       a CUDA array
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 65536 x 32768 x 2048 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 4096 x 4096 x 4096 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 16384 x 16384 x 16384 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width (and height) for a cubemap surface reference bound to a
                                       CUDA array
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 32768 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">
                                       Maximum width (and height) and number of layers for a cubemap layered
                                       surface reference
                                       
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080" rowspan="1"> 32768 x 2046 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d225e32068 d225e32083 d225e32086 d225e32090" rowspan="1"> 16384 x 2046 </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32093 d225e32096 d225e32099 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1"> 32768 x 2046 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="27.77777777777778%" headers="d225e32066 d225e32074" rowspan="1" colspan="1">Maximum number of surfaces that can be bound to a kernel </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d225e32068 d225e32077 d225e32080 d225e32083 d225e32086 d225e32090 d225e32093 d225e32096 d225e32099" rowspan="1">16</td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d225e32068 d225e32102 d225e32105 d225e32109 d225e32112 d225e32115" rowspan="1">32</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="floating-point-standard"><a name="floating-point-standard" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#floating-point-standard" name="floating-point-standard" shape="rect">J.2.&nbsp;Floating-Point Standard</a></h3>
                     <div class="body conbody">
                        <p class="p">All compute devices follow the IEEE 754-2008 standard for binary floating-point arithmetic with the following deviations:
                           
                        </p>
                        <ul class="ul">
                           <li class="li">There is no dynamically configurable rounding mode; however, most of the operations support multiple IEEE rounding modes,
                              exposed via device intrinsics.
                              
                           </li>
                           <li class="li">There is no mechanism for detecting that a floating-point exception has occurred and all operations behave as if the IEEE-754
                              exceptions are always masked, and deliver the masked response as defined by IEEE-754 if there is an exceptional event. For
                              the same reason, while SNaN encodings are supported, they are not signaling and are handled as quiet.
                              
                           </li>
                           <li class="li">The result of a single-precision floating-point operation involving one or more input NaNs is the quiet NaN of bit pattern
                              0x7fffffff.
                              
                           </li>
                           <li class="li">Double-precision floating-point absolute value and negation are not compliant with IEEE-754 with respect to NaNs; these are
                              passed through unchanged.
                              
                           </li>
                        </ul>
                        <p class="p">Code must be compiled with <samp class="ph codeph">-ftz=false</samp>, <samp class="ph codeph">-prec-div=true</samp>, and <samp class="ph codeph">-prec-sqrt=true</samp> to ensure IEEE compliance (this is the default setting; see the <samp class="ph codeph">nvcc</samp> user manual for description of these compilation flags).
                           
                        </p>
                        <p class="p">Regardless of the setting of the compiler flag <samp class="ph codeph">-ftz</samp>,
                        </p>
                        <ul class="ul">
                           <li class="li">atomic single-precision floating-point adds on global memory always operate in flush-to-zero mode, i.e., behave equivalent
                              to <samp class="ph codeph">FADD.F32.FTZ.RN</samp>,
                              
                           </li>
                           <li class="li">atomic single-precision floating-point adds on shared memory always operate with denormal support, i.e., behave equivalent
                              to <samp class="ph codeph">FADD.F32.RN</samp>.
                              
                           </li>
                        </ul>
                        <p class="p">In accordance to the IEEE-754R standard, if one of the input parameters to <samp class="ph codeph">fminf()</samp>, <samp class="ph codeph">fmin()</samp>, <samp class="ph codeph">fmaxf()</samp>, or <samp class="ph codeph">fmax()</samp> is NaN, but not the other, the result is the non-NaN parameter.
                           
                        </p>
                        <p class="p">The conversion of a floating-point value to an integer value in the case where the floating-point value falls outside the
                           range of the integer format is left undefined by IEEE-754. For compute devices, the behavior is to clamp to the end of the
                           supported range. This is unlike the x86 architecture behavior.
                           
                        </p>
                        <p class="p">
                           The behavior of integer division by zero and integer overflow is left undefined by IEEE-754. For compute devices, there is
                           no mechanism for detecting that such integer operation exceptions have occurred. Integer division by zero yields an unspecified,
                           machine-specific value.
                           
                        </p>
                        <p class="p"><a class="xref" href="http://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus" target="_blank" shape="rect">http://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus</a> includes more information on the floating point accuracy and compliance of NVIDIA GPUs.
                           
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-capability-3-0"><a name="compute-capability-3-0" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-capability-3-0" name="compute-capability-3-0" shape="rect">J.3.&nbsp;Compute Capability 3.x</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="architecture-3-0"><a name="architecture-3-0" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#architecture-3-0" name="architecture-3-0" shape="rect">J.3.1.&nbsp;Architecture</a></h3>
                        <div class="body conbody">
                           <div class="p">An SM consists of:
                              
                              <ul class="ul">
                                 <li class="li">192 CUDA cores for arithmetic operations (see <a class="xref" href="index.html#arithmetic-instructions" shape="rect">Arithmetic Instructions</a> for throughputs of arithmetic operations),
                                    
                                 </li>
                                 <li class="li">32 special function units for
                                    single-precision floating-point transcendental functions,
                                    
                                 </li>
                                 <li class="li">4 warp schedulers.</li>
                              </ul>
                           </div>
                           <p class="p">When an SM is given warps to execute, it first distributes them among the four schedulers. Then, at every instruction issue
                              time, each scheduler issues two independent instructions for one of its assigned warps that is ready to execute, if any.
                              
                           </p>
                           <p class="p">An SM has a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space,
                              which resides in device memory.
                              
                           </p>
                           <p class="p">There is an L1 cache for each SM and an L2 cache shared by all SMs.
                              The L1 cache is used to cache accesses to local memory, including temporary register spills. The L2 cache is used to cache
                              accesses to local and global memory. The cache behavior (e.g., whether reads are cached in both L1 and L2 or in L2 only) can
                              be partially configured on a per-access basis using modifiers to the load or store instruction.  Some devices of compute capability
                              3.5 and devices of compute capability 3.7 allow opt-in to caching of global memory in both L1 and L2 via compiler options.
                              
                           </p>
                           <p class="p">The same on-chip memory is used for both L1 and shared memory: It can be configured as 48 KB of shared memory and 16 KB of
                              L1 cache or as 16 KB of shared memory and 48 KB of L1 cache or as 32 KB of shared memory and 32 KB of L1 cache, using <samp class="ph codeph">cudaFuncSetCacheConfig()</samp>/<samp class="ph codeph">cuFuncSetCacheConfig()</samp>:
                              
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel()
{
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Runtime API</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cudaFuncCachePreferShared: shared memory is 48 KB</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cudaFuncCachePreferEqual: shared memory is 32 KB</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cudaFuncCachePreferL1: shared memory is 16 KB</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cudaFuncCachePreferNone: no preference</span>
cudaFuncSetCacheConfig(MyKernel, cudaFuncCachePreferShared)</pre><p class="p">The default cache configuration is "prefer none", meaning "no preference". If a kernel is configured to have no preference,
                              then it will default to the preference of the current thread/context, which is set using <samp class="ph codeph">cudaDeviceSetCacheConfig()</samp>/<samp class="ph codeph">cuCtxSetCacheConfig()</samp> (see the reference manual for details). If the current thread/context also has no preference (which is again the default
                              setting), then whichever cache configuration was most recently used for any kernel will be the one that is used, unless a
                              different cache configuration is required to launch the kernel (e.g., due to shared memory requirements). The initial configuration
                              is 48 KB of shared memory and 16 KB of L1 cache.
                              
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span> Devices of compute capability 3.7 add an additional 64 KB of shared memory to each of the above configurations, yielding
                              112 KB, 96 KB, and 80 KB shared memory per SM, respectively.  However, the maximum shared memory per thread block remains
                              48 KB.
                           </div>
                           <p class="p">Applications may query the L2 cache size by checking the <samp class="ph codeph">l2CacheSize</samp> device property (see
                              <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>). The maximum L2 cache size is 1.5 MB. 
                           </p>
                           <p class="p">
                              Each SM has a read-only data cache of 48 KB to speed up reads from device memory.
                              It accesses this cache either directly (for devices of compute capability 3.5 or 3.7), or via a texture unit that implements
                              the various addressing modes and data filtering mentioned in <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>.
                              When accessed via the texture unit, the read-only data cache is also referred to as texture cache.
                              
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="global-memory-3-0"><a name="global-memory-3-0" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#global-memory-3-0" name="global-memory-3-0" shape="rect">J.3.2.&nbsp;Global Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Global memory accesses for devices of compute capability 3.x are cached in L2 and for devices of compute capability 3.5 or
                              3.7, may also be cached in the read-only data cache described in the previous section; they are normally not cached in L1.
                              Some devices of compute capability 3.5 and devices of compute capability 3.7 allow opt-in to caching of global memory accesses
                              in L1 via the <samp class="ph codeph">-Xptxas -dlcm=ca</samp> option to <samp class="ph codeph">nvcc</samp>.
                           </p>
                           <p class="p">A cache line is 128 bytes and maps to a 128 byte aligned segment in device memory. Memory accesses that are cached in both
                              L1 and L2 are serviced with 128-byte memory transactions, whereas memory accesses that are cached in L2 only are serviced
                              with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered
                              memory accesses.
                           </p>
                           <div class="p">If the size of the words accessed by each thread is more than 4 bytes, a memory request by a warp is first split into separate
                              128-byte memory requests that are issued independently:
                              
                              <ul class="ul">
                                 <li class="li">Two memory requests, one for each half-warp, if the size is 8 bytes,</li>
                                 <li class="li">Four memory requests, one for each quarter-warp, if the size is 16 bytes.</li>
                              </ul>
                           </div>
                           <p class="p">Each memory request is then broken down into cache line requests that are issued independently. A cache line request is serviced
                              at the throughput of L1 or L2 cache in case of a cache hit, or at the throughput of device memory, otherwise.
                           </p>
                           <p class="p">Note that threads can access any words in any order, including the same words.</p>
                           <p class="p">If a non-atomic instruction executed by a warp writes to the same location in global memory for more than one of the threads
                              of the warp, only one thread performs a write and which thread does it is undefined.
                           </p>
                           <p class="p">
                              Data that is read-only for the entire lifetime of the kernel can also be cached in the read-only data cache described in the
                              previous section by reading it using the <samp class="ph codeph">__ldg()</samp> function (see <a class="xref" href="index.html#ldg-function" shape="rect">Read-Only Data Cache Load Function</a>).
                              When the compiler detects that the read-only condition is satisfied for some data, it will use <samp class="ph codeph">__ldg()</samp> to read it.
                              The compiler might not always be able to detect that the read-only condition is satisfied for some data.
                              Marking pointers used for loading such data with both the <samp class="ph codeph">const</samp> and <samp class="ph codeph">__restrict__</samp> qualifiers increases the likelihood that the compiler will detect the read-only condition.
                              
                           </p>
                           <p class="p"><a class="xref" href="index.html#global-memory-3-0__examples-of-global-memory-accesses" title="Examples of Global Memory Accesses by a Warp, 4-Byte Word per Thread, and Associated Memory Transactions for Compute Capabilities 3.x and Beyond" shape="rect">Figure 16</a> shows some examples of global memory accesses and corresponding memory transactions.
                           </p>
                           <div class="fig fignone" id="global-memory-3-0__examples-of-global-memory-accesses"><a name="global-memory-3-0__examples-of-global-memory-accesses" shape="rect">
                                 <!-- --></a><span class="figcap">Figure 16. Examples of Global Memory Accesses</span>. <span class="desc figdesc">Examples of Global Memory Accesses by a Warp, 4-Byte Word per Thread, and Associated Memory Transactions for Compute Capabilities
                                 3.x and Beyond</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/examples-of-global-memory-accesses.png" alt="Examples of Global Memory Accesses. Examples of Global Memory Accesses by a Warp, 4-Byte Word per Thread, and Associated Memory Transactions Based on Compute Capability."></img></div><br clear="none"></br></div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory-3-0"><a name="shared-memory-3-0" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-3-0" name="shared-memory-3-0" shape="rect">J.3.3.&nbsp;Shared Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Shared memory has 32 banks with two addressing modes that are described
                              below.
                           </p>
                           <p class="p">The addressing mode can be queried using
                              <samp class="ph codeph">cudaDeviceGetSharedMemConfig()</samp> and set using
                              <samp class="ph codeph">cudaDeviceSetSharedMemConfig()</samp> (see reference manual for
                              more details). Each bank has a bandwidth of 64 bits per clock cycle. 
                           </p>
                           <p class="p"><a class="xref" href="index.html#shared-memory-5-x__examples-of-strided-shared-memory-accesses" title="Examples for devices of compute capability 3.x (in 32-bit mode) or compute capability 5.x and 6.x" shape="rect">Figure 17</a>
                              shows some examples of strided access.
                           </p>
                           <p class="p"><a class="xref" href="index.html#shared-memory-5-x__examples-of-irregular-shared-memory-accesses" title="Examples for devices of compute capability 3.x, 5.x, or 6.x." shape="rect">Figure 18</a>
                              shows some examples of memory read accesses that involve the broadcast
                              mechanism.
                           </p>
                           <div class="section">
                              <h4 class="title sectiontitle">64-Bit Mode</h4>
                              <p class="p">Successive 64-bit words map to successive banks.</p>
                              <p class="p">A shared memory request for a warp does not generate a bank conflict
                                 between two threads that access any sub-word within the same 64-bit
                                 word (even though the addresses of the two sub-words fall in the same
                                 bank). In that case, for read accesses, the 64-bit word is broadcast to
                                 the requesting threads and for write accesses, each sub-word is written
                                 by only one of the threads (which thread performs the write is
                                 undefined).
                              </p>
                           </div>
                           <div class="section">
                              <h4 class="title sectiontitle">32-Bit Mode</h4>
                              <p class="p">Successive 32-bit words map to successive banks.</p>
                              <p class="p">A shared memory request for a warp does not generate a bank conflict
                                 between two threads that access any sub-word within the same 32-bit
                                 word or within two 32-bit words whose indices <em class="ph i">i</em> and <em class="ph i">j</em> are
                                 in the same 64-word aligned segment (i.e., a segment whose first index
                                 is a multiple of 64) and such that <em class="ph i">j=i+32</em> (even though the
                                 addresses of the two sub-words fall in the same bank). In that case,
                                 for read accesses, the 32-bit words are broadcast to the requesting
                                 threads and for write accesses, each sub-word is written by only one of
                                 the threads (which thread performs the write is undefined).
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-capability-5-x"><a name="compute-capability-5-x" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-capability-5-x" name="compute-capability-5-x" shape="rect">J.4.&nbsp;Compute Capability 5.x</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="architecture-5-x"><a name="architecture-5-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#architecture-5-x" name="architecture-5-x" shape="rect">J.4.1.&nbsp;Architecture</a></h3>
                        <div class="body conbody">
                           <div class="p">An SM consists of:
                              
                              <ul class="ul">
                                 <li class="li">128 CUDA cores for arithmetic operations (see <a class="xref" href="index.html#arithmetic-instructions" shape="rect">Arithmetic Instructions</a> for throughputs of arithmetic operations),
                                    
                                 </li>
                                 <li class="li">32 special function units for
                                    single-precision floating-point transcendental functions,
                                    
                                 </li>
                                 <li class="li">4 warp schedulers.</li>
                              </ul>
                           </div>
                           <p class="p">When an SM is given warps to execute, it first distributes them among the four schedulers. Then, at every instruction issue
                              time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
                              
                           </p>
                           <div class="p">An SM has:
                              
                              <ul class="ul">
                                 <li class="li">
                                    a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which
                                    resides in device memory,
                                    
                                 </li>
                                 <li class="li">
                                    a unified L1/texture cache of 24 KB used to cache reads from global memory,
                                    
                                 </li>
                                 <li class="li">64 KB of shared memory for devices of compute capability 5.0 or 96 KB of shared memory for devices of compute capability 5.2.</li>
                              </ul>
                           </div>
                           <p class="p">
                              The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering
                              mentioned in <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>.
                              
                           </p>
                           <p class="p">
                              There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary
                              register spills.
                              Applications may query the L2 cache size by checking the <samp class="ph codeph">l2CacheSize</samp> device property (see
                              <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>).
                              
                           </p>
                           <p class="p">The cache behavior (e.g., whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially
                              configured on a per-access basis using modifiers to the load instruction.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="global-memory-5-x"><a name="global-memory-5-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#global-memory-5-x" name="global-memory-5-x" shape="rect">J.4.2.&nbsp;Global Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              Global memory accesses are always cached in L2 and caching in L2 behaves in the same way as for devices of compute capability
                              3.x (see <a class="xref" href="index.html#global-memory-3-0" shape="rect">Global Memory</a>).
                              
                           </p>
                           <p class="p">
                              Data that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture cache described
                              in the previous section by reading it using the <samp class="ph codeph">__ldg()</samp> function (see <a class="xref" href="index.html#ldg-function" shape="rect">Read-Only Data Cache Load Function</a>).
                              When the compiler detects that the read-only condition is satisfied for some data, it will use <samp class="ph codeph">__ldg()</samp> to read it.
                              The compiler might not always be able to detect that the read-only condition is satisfied for some data.
                              Marking pointers used for loading such data with both the <samp class="ph codeph">const</samp> and <samp class="ph codeph">__restrict__</samp> qualifiers increases the likelihood that the compiler will detect the read-only condition.
                              
                           </p>
                           <div class="p">
                              Data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified L1/texture cache for devices
                              of compute capability 5.0.
                              For devices of compute capability 5.2, it is, by default, not cached in the unified L1/texture cache, but caching may be enabled
                              using the following mechanisms:
                              
                              <ul class="ul">
                                 <li class="li">Perform the read using inline assembly with the appropriate modifier as described in the PTX reference manual;</li>
                                 <li class="li">
                                    Compile with the <samp class="ph codeph">-Xptxas -dlcm=ca</samp> compilation flag, in which case all reads are cached, except reads that are performed using inline assembly with a modifier
                                    that disables caching;
                                    
                                 </li>
                                 <li class="li">
                                    Compile with the <samp class="ph codeph">-Xptxas -fscm=ca</samp> compilation flag, in which case all reads are cached, including reads that are performed using inline assembly regardless
                                    of the modifier used.
                                    
                                 </li>
                              </ul>
                              
                              When caching is enabled using one of the three mechanisms listed above, devices of compute capability
                              5.2 will cache global memory reads in the unified L1/texture cache for all kernel launches except for
                              the kernel launches for which thread blocks consume too much of the SM's register file.
                              These exceptions are reported by the profiler.
                              
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory-5-x"><a name="shared-memory-5-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-5-x" name="shared-memory-5-x" shape="rect">J.4.3.&nbsp;Shared Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks. Each bank has a bandwidth
                              of 32 bits per clock cycle.
                           </p>
                           <p class="p">A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the
                              same 32-bit word 
                              (even though the two addresses fall in the same bank). In that case, for read accesses, the word is broadcast to the requesting
                              threads 
                              and for write accesses, each address is written by only one of the threads (which thread performs the write is undefined).
                           </p>
                           <p class="p"><a class="xref" href="index.html#shared-memory-5-x__examples-of-strided-shared-memory-accesses" title="Examples for devices of compute capability 3.x (in 32-bit mode) or compute capability 5.x and 6.x" shape="rect">Figure 17</a>
                              shows some examples of strided access.
                           </p>
                           <p class="p"><a class="xref" href="index.html#shared-memory-5-x__examples-of-irregular-shared-memory-accesses" title="Examples for devices of compute capability 3.x, 5.x, or 6.x." shape="rect">Figure 18</a>
                              shows some examples of memory read accesses that involve the broadcast
                              mechanism.
                           </p>
                           <div class="section">
                              <div class="fig fignone" id="shared-memory-5-x__examples-of-strided-shared-memory-accesses"><a name="shared-memory-5-x__examples-of-strided-shared-memory-accesses" shape="rect">
                                    <!-- --></a><span class="figcap">Figure 17. Strided Shared Memory Accesses</span>. <span class="desc figdesc">Examples for devices of compute capability 3.x (in 32-bit
                                    mode) or compute capability 5.x and 6.x</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/examples-of-strided-shared-memory-accesses.png" alt="Examples of           Strided Shared Memory Accesses for Devices of Compute Capability 3.x,           in 32-bit mode."></img></div><br clear="none"></br><dl class="dl">
                                    <dt class="dt dlterm">Left</dt>
                                    <dd class="dd">Linear addressing with a stride of one 32-bit word (no bank
                                       conflict).
                                    </dd>
                                    <dt class="dt dlterm">Middle</dt>
                                    <dd class="dd">Linear addressing with a stride of two 32-bit words (two-way bank
                                       conflict).
                                    </dd>
                                    <dt class="dt dlterm">Right</dt>
                                    <dd class="dd">Linear addressing with a stride of three 32-bit words (no bank
                                       conflict).
                                    </dd>
                                 </dl>
                              </div>
                              <div class="fig fignone" id="shared-memory-5-x__examples-of-irregular-shared-memory-accesses"><a name="shared-memory-5-x__examples-of-irregular-shared-memory-accesses" shape="rect">
                                    <!-- --></a><span class="figcap">Figure 18. Irregular Shared Memory Accesses</span>. <span class="desc figdesc">Examples for devices of compute capability 3.x, 5.x, or 6.x.</span><br clear="none"></br><div class="imagecenter"><img class="image imagecenter" src="graphics/examples-of-irregular-shared-memory-accesses.png" alt="Examples           of Irregular Shared Memory Accesses for Devices of Compute Capability           3.x."></img></div><br clear="none"></br><dl class="dl">
                                    <dt class="dt dlterm">Left</dt>
                                    <dd class="dd">Conflict-free access via random permutation.</dd>
                                    <dt class="dt dlterm">Middle</dt>
                                    <dd class="dd">Conflict-free access since threads 3, 4, 6, 7, and 9 access the
                                       same word within bank 5.
                                    </dd>
                                    <dt class="dt dlterm">Right</dt>
                                    <dd class="dd">Conflict-free broadcast access (threads access the same word
                                       within a bank).
                                    </dd>
                                 </dl>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-capability-6-x"><a name="compute-capability-6-x" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-capability-6-x" name="compute-capability-6-x" shape="rect">J.5.&nbsp;Compute Capability 6.x</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="architecture-6-x"><a name="architecture-6-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#architecture-6-x" name="architecture-6-x" shape="rect">J.5.1.&nbsp;Architecture</a></h3>
                        <div class="body conbody">
                           <div class="p"> An SM consists of:
                              
                              <ul class="ul">
                                 <li class="li">64 (compute capability 6.0) or 128 (6.1 and 6.2) CUDA cores for arithmetic operations,
                                    
                                 </li>
                                 <li class="li">16 (6.0) or 32 (6.1 and 6.2) special function units for
                                    single-precision floating-point transcendental functions,
                                    
                                 </li>
                                 <li class="li">  
                                    2 (6.0) or 4 (6.1 and 6.2) warp schedulers.
                                    
                                 </li>
                              </ul>
                           </div>
                           <p class="p">When an SM is given warps to execute, it first distributes them among its schedulers. Then, at every instruction issue time,
                              each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
                              
                           </p>
                           <div class="p">An SM has:
                              
                              <ul class="ul">
                                 <li class="li">
                                    a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which
                                    resides in device memory,
                                    
                                 </li>
                                 <li class="li">
                                    
                                    a unified L1/texture cache for reads from global memory of size 24 KB (6.0 and 6.2) or 48 KB (6.1),
                                    
                                 </li>
                                 <li class="li">
                                    
                                    a shared memory of size 64 KB (6.0 and 6.2) or 96 KB (6.1).
                                    
                                 </li>
                              </ul>
                           </div>
                           <p class="p">
                              The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering
                              mentioned in <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>.
                              
                           </p>
                           <p class="p">
                              There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary
                              register spills.
                              Applications may query the L2 cache size by checking the <samp class="ph codeph">l2CacheSize</samp> device property (see
                              <a class="xref" href="index.html#device-enumeration" shape="rect">Device Enumeration</a>).
                              
                           </p>
                           <p class="p">The cache behavior (e.g., whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially
                              configured on a per-access basis using modifiers to the load instruction.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="global-memory-6-x"><a name="global-memory-6-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#global-memory-6-x" name="global-memory-6-x" shape="rect">J.5.2.&nbsp;Global Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Global memory behaves the same way as in devices of compute capability 5.x (See <a class="xref" href="index.html#global-memory-5-x" shape="rect">Global Memory</a>). 
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory-6-x"><a name="shared-memory-6-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-6-x" name="shared-memory-6-x" shape="rect">J.5.3.&nbsp;Shared Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Shared memory behaves the same way as in devices of compute capability 5.x (See <a class="xref" href="index.html#shared-memory-5-x" shape="rect">Shared Memory</a>). 
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-capability-7-x"><a name="compute-capability-7-x" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-capability-7-x" name="compute-capability-7-x" shape="rect">J.6.&nbsp;Compute Capability 7.x</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="architecture-7-x"><a name="architecture-7-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#architecture-7-x" name="architecture-7-x" shape="rect">J.6.1.&nbsp;Architecture</a></h3>
                        <div class="body conbody">
                           <div class="p">An SM consists of:
                              
                              <ul class="ul">
                                 <li class="li">64 FP32 cores for single-precision arithmetic operations,</li>
                                 <li class="li">
                                    32 FP64 cores for double-precision arithmetic operations,
                                    <a name="fnsrc_33" href="#fntarg_33" shape="rect"><sup>33</sup></a></li>
                                 <li class="li">64 INT32 cores for integer math,</li>
                                 <li class="li">
                                    8 mixed-precision Tensor Cores for deep learning matrix arithmetic
                                    
                                 </li>
                                 <li class="li">16 special function units for single-precision floating-point transcendental functions,</li>
                                 <li class="li">4 warp schedulers.</li>
                              </ul>
                           </div>
                           <p class="p">An SM statically distributes its warps among its schedulers. Then, at every instruction issue time, each scheduler issues
                              one instruction for one of its assigned warps that is ready to execute, if any.
                           </p>
                           <div class="p">An SM has:
                              
                              <ul class="ul">
                                 <li class="li">a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which
                                    resides in device memory,
                                 </li>
                                 <li class="li">a unified data cache and shared memory with a total size of 128 KB (<dfn class="term">Volta</dfn>) or 96 KB (<dfn class="term">Turing</dfn>).
                                 </li>
                              </ul>
                           </div>
                           <p class="p">Shared memory is partitioned out of unified data cache, and can be configured to various sizes
                              (See <a class="xref" href="index.html#shared-memory-7-x" shape="rect">Shared Memory</a>.)
                              The remaining data cache serves as an L1 cache and is also used by the texture unit
                              that implements the various addressing and data filtering modes mentioned in
                              <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="independent-thread-scheduling-7-x"><a name="independent-thread-scheduling-7-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#independent-thread-scheduling-7-x" name="independent-thread-scheduling-7-x" shape="rect">J.6.2.&nbsp;Independent Thread Scheduling</a></h3>
                        <div class="body conbody">
                           <p class="p">The <dfn class="term">Volta</dfn> architecture introduces <dfn class="term">Independent Thread Scheduling</dfn> among
                              threads in a warp, enabling intra-warp synchronization patterns previously unavailable
                              and simplifying code changes when porting CPU code. However, this can lead to a rather
                              different set of threads participating in the executed code than intended if the
                              developer made assumptions about warp-synchronicity of previous hardware architectures.
                           </p>
                           <p class="p">Below are code patterns of concern and suggested corrective actions for Volta-safe code.</p>
                           <ol class="ol">
                              <li class="li">
                                 <p class="p">For applications using warp intrinsics (<samp class="ph codeph">__shfl*</samp>,
                                    <samp class="ph codeph">__any</samp>, <samp class="ph codeph">__all</samp>, <samp class="ph codeph">__ballot</samp>),
                                    it is necessary that developers port their code to the new, safe,
                                    synchronizing counterpart, with the <samp class="ph codeph">*_sync</samp> suffix. The new
                                    warp intrinsics take in a mask of threads that explicitly define which lanes
                                    (threads of a warp) must participate in the warp intrinsic. See
                                    <a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a> and
                                    <a class="xref" href="index.html#warp-shuffle-functions" shape="rect">Warp Shuffle Functions</a> for details.
                                 </p>
                                 <div class="p">Since the intrinsics are available with CUDA 9.0+, (if necessary) code can
                                    be executed conditionally with the following preprocessor macro:
                                    <pre xml:space="preserve">
#<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> defined(CUDART_VERSION) &amp;&amp; CUDART_VERSION &gt;= 9000
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// *_sync intrinsic</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#endif</span>
                </pre></div>
                                 <p class="p">These intrinsics are available on all architectures, not just <dfn class="term">Volta</dfn> or
                                    <dfn class="term">Turing</dfn>, and in most cases a single code-base will suffice for all architectures.
                                    Note, however, that for <dfn class="term">Pascal</dfn> and earlier architectures, all threads in
                                    mask must execute the same warp intrinsic instruction in convergence,
                                    and the union of all values in mask must be equal to the warp's active mask.
                                    The following code pattern is valid on <dfn class="term">Volta</dfn>, but not on <dfn class="term">Pascal</dfn>
                                    or earlier architectures.
                                 </p><pre xml:space="preserve">    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (tid % warpSize &lt; 16) {
        ...
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> swapped = __shfl_xor_sync(0xffffffff, val, 16);
        ...
    } <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">else</span> {
        ...
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> swapped = __shfl_xor_sync(0xffffffff, val, 16);
        ...
    }</pre><p class="p">The replacement for <samp class="ph codeph">__ballot(1)</samp> is <samp class="ph codeph">
                                       __activemask()</samp>. Note that threads within a warp can diverge
                                    even within a single code path. As a result, <samp class="ph codeph">__activemask()
                                       </samp> and <samp class="ph codeph">__ballot(1)</samp> may return only a subset
                                    of the threads on the current code path. The following invalid code
                                    example sets bit <samp class="ph codeph">i</samp> of <samp class="ph codeph">output</samp> to 1
                                    when <samp class="ph codeph">data[i]</samp> is greater than <samp class="ph codeph">threshold</samp>.
                                    <samp class="ph codeph">__activemask()</samp> is used in an attempt to enable cases
                                    where <samp class="ph codeph">dataLen</samp> is not a multiple of 32.
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Sets bit in output[] to 1 if the correspond element in data[i]</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// is greater than threshold, using 32 threads in a warp.</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = warpLane; i &lt; dataLen; i += warpSize) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> active = __activemask();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> bitPack = __ballot_sync(active, data[i] &gt; threshold);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (warpLane == 0) {
        output[i / 32] = bitPack;
    }
}</pre><p class="p">This code is invalid because CUDA does not guarantee that the warp
                                    will diverge ONLY at the loop condition. When divergence happens for
                                    other reasons, conflicting results will be computed for the same
                                    32-bit output element by different subsets of threads in the warp.
                                    A correct code might use a non-divergent loop condition together with
                                    <samp class="ph codeph">__ballot_sync()</samp> to safely enumerate the set of threads
                                    in the warp participating in the threshold calculation as follows.
                                 </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = warpLane; i - warpLane &lt; dataLen; i += warpSize) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> active = __ballot_sync(0xFFFFFFFF, i &lt; dataLen);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (i &lt; dataLen) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> bitPack = __ballot_sync(active, data[i] &gt; threshold);
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (warpLane == 0) {
            output[i / 32] = bitPack;
        }
    }
}</pre><p class="p"><a class="xref" href="index.html#discovery-pattern-cg" shape="rect">Discovery Pattern</a>
                                    demonstrates a valid use case for <samp class="ph codeph">__activemask()</samp>.
                                 </p>
                              </li>
                              <li class="li">
                                 <p class="p">If applications have warp-synchronous codes, they will need to insert the
                                    new <samp class="ph codeph">__syncwarp()</samp> warp-wide barrier synchronization
                                    instruction between any steps where data is exchanged between threads via
                                    global or shared memory. Assumptions that code is executed in
                                    lockstep or that reads/writes from separate threads are visible
                                    across a warp without synchronization are invalid.
                                 </p><pre xml:space="preserve">    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> s_buff[BLOCK_SIZE];
    s_buff[tid] = val;
    __syncthreads();

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Inter-warp reduction</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = BLOCK_SIZE / 2; i &gt;= 32; i /= 2) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (tid &lt; i) {
            s_buff[tid] += s_buff[tid+i];
        }
        __syncthreads();
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Intra-warp reduction</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Butterfly reduction simplifies syncwarp mask</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (tid &lt; 32) {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> temp;
        temp = s_buff[tid ^ 16]; __syncwarp();
        s_buff[tid] += temp;     __syncwarp();
        temp = s_buff[tid ^ 8];  __syncwarp();
        s_buff[tid] += temp;     __syncwarp();
        temp = s_buff[tid ^ 4];  __syncwarp();
        s_buff[tid] += temp;     __syncwarp();
        temp = s_buff[tid ^ 2];  __syncwarp();
        s_buff[tid] += temp;     __syncwarp();
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (tid == 0) {
        *output = s_buff[0] + s_buff[1];
    }
    __syncthreads();</pre></li>
                              <li class="li">
                                 <p class="p">Although <samp class="ph codeph">__syncthreads()</samp> has been consistently  documented as synchronizing all threads in the thread block, <dfn class="term">Pascal</dfn> and prior architectures could only enforce synchronization at the warp level. In certain cases, this allowed a barrier to
                                    succeed without being executed by every thread as long as at least some thread in every warp reached the barrier. Starting
                                    with <dfn class="term">Volta</dfn>, the CUDA built-in <samp class="ph codeph">__syncthreads()</samp> and PTX instruction <samp class="ph codeph">bar.sync</samp> (and their derivatives) are enforced per thread and thus will not succeed until reached by all non-exited threads in the
                                    block. Code exploiting the previous behavior will likely deadlock and must be modified to ensure that all non-exited threads
                                    reach the barrier.
                                 </p>
                              </li>
                           </ol>
                           <p class="p">The <samp class="ph codeph">racecheck</samp> and <samp class="ph codeph">synccheck</samp> tools provided by <samp class="ph codeph">cuda-memcheck</samp> can aid in locating violations of points 2 and 3.
                           </p>
                           <p class="p">To aid migration while implementing the above-mentioned corrective actions, developers can opt-in to the <dfn class="term">Pascal</dfn> scheduling model that does not support independent thread scheduling. See <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a> for details.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="global-memory-7-x"><a name="global-memory-7-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#global-memory-7-x" name="global-memory-7-x" shape="rect">J.6.3.&nbsp;Global Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Global memory behaves the same way as in devices of compute capability 5.x (See <a class="xref" href="index.html#global-memory-5-x" shape="rect">Global Memory</a>).
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory-7-x"><a name="shared-memory-7-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-7-x" name="shared-memory-7-x" shape="rect">J.6.4.&nbsp;Shared Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Similar to the <a class="xref" href="index.html#architecture-3-0" shape="rect">Kepler architecture</a>, the amount
                              of the unified data cache reserved for shared memory is configurable on a per kernel basis.
                              For the <dfn class="term">Volta</dfn> architecture (compute capability 7.0), the unified data cache has a size of 128 KB,
                              and the shared memory capacity can be set to 0, 8, 16, 32, 64 or 96 KB. For the <dfn class="term">Turing</dfn>
                              architecture (compute capability 7.5), the unified data cache has a size of 96 KB, and the shared memory
                              capacity can be set to either 32 KB or 64 KB.
                              Unlike <dfn class="term">Kepler</dfn>, the driver automatically configures the shared memory capacity for each
                              kernel to avoid shared memory occupancy bottlenecks while also allowing concurrent execution
                              with already launched kernels where possible. In most cases, the driver's default behavior
                              should provide optimal performance.
                           </p>
                           <p class="p">Because the driver is not always aware of the full workload, it is sometimes
                              useful for applications to provide additional hints regarding the desired shared memory
                              configuration. For example, a kernel with little or no shared memory use may request a
                              larger carveout in order to encourage concurrent execution with later kernels that require
                              more shared memory. The new <samp class="ph codeph">cudaFuncSetAttribute()</samp> API allows applications to
                              set a preferred shared memory capacity, or <samp class="ph codeph">carveout</samp>, as a percentage of the
                              maximum supported shared memory capacity (96 KB for <dfn class="term">Volta</dfn>, and 64 KB for <dfn class="term">Turing</dfn>).
                           </p>
                           <p class="p"><samp class="ph codeph">cudaFuncSetAttribute()</samp> relaxes enforcement of the preferred
                              shared capacity compared to the legacy <samp class="ph codeph">cudaFuncSetCacheConfig()</samp> API
                              introduced with <a class="xref" href="index.html#architecture-3-0" shape="rect">Kepler</a>. The legacy API treated shared
                              memory capacities as hard requirements for kernel launch. As a result, interleaving kernels
                              with different shared memory configurations would needlessly serialize launches behind
                              shared memory reconfigurations. With the new API, the carveout is treated as a hint. The
                              driver may choose a different configuration if required to execute the function or to avoid
                              thrashing.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel(...)
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__shared__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> buffer[BLOCK_DIM];
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> carveout = 50; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// prefer shared memory capacity 50% of maximum</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Named Carveout Values:</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// carveout = cudaSharedmemCarveoutDefault;   //  (-1)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// carveout = cudaSharedmemCarveoutMaxL1;     //   (0)</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// carveout = cudaSharedmemCarveoutMaxShared; // (100)</span>
cudaFuncSetAttribute(MyKernel, cudaFuncAttributePreferredSharedMemoryCarveout, carveout);
MyKernel <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>gridDim, BLOCK_DIM<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);</pre><p class="p">In addition to an integer percentage, several convenience enums are provided as
                              listed in the code comments above. Where a chosen integer percentage does not map exactly to
                              a supported capacity (SM 7.0 devices support shared capacities of 0, 8, 16, 32, 64, or 96 KB),
                              the next larger capacity is used. For instance, in the example above, 50% of the 96 KB maximum
                              is 48 KB, which is not a supported shared memory capacity. Thus, the preference is rounded up
                              to 64 KB.
                           </p>
                           <p class="p">Compute capability 7.x devices allow a single thread block to address the full capacity of shared memory:
                              96 KB on <dfn class="term">Volta</dfn>, 64 KB on <dfn class="term">Turing</dfn>.
                              Kernels relying on shared memory allocations over 48 KB per block are architecture-specific,
                              as such they must use dynamic shared memory (rather than statically sized arrays) and require
                              an explicit opt-in using <samp class="ph codeph">cudaFuncSetAttribute()</samp> as follows.
                           </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Device code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> MyKernel(...)
{
    ...
}

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Host code</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> maxbytes = 98304; <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 96 KB</span>
cudaFuncSetAttribute(MyKernel, cudaFuncAttributeMaxDynamicSharedMemorySize, maxbytes);
MyKernel <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>gridDim, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">blockDim</span><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(...);</pre><p class="p">Otherwise, shared memory behaves the same way as for devices of compute capability 5.x (See <a class="xref" href="index.html#shared-memory-5-x" shape="rect">Shared Memory</a>).
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="compute-capability-8-x"><a name="compute-capability-8-x" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#compute-capability-8-x" name="compute-capability-8-x" shape="rect">J.7.&nbsp;Compute Capability 8.x</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="architecture-8-x"><a name="architecture-8-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#architecture-8-x" name="architecture-8-x" shape="rect">J.7.1.&nbsp;Architecture</a></h3>
                        <div class="body conbody">
                           <div class="p">A Streaming Multiprocessor (SM) consists of:
                              
                              <ul class="ul">
                                 <li class="li">64 FP32 cores for single-precision arithmetic operations in devices of compute capability 8.0 and
                                    128 FP32 cores in devices of compute capability 8.6,
                                 </li>
                                 <li class="li">32 FP64 cores for double-precision arithmetic operations in devices of compute capability 8.0 and 
                                    2 FP64 cores in devices of compute capability 8.6
                                 </li>
                                 <li class="li">64 INT32 cores for integer math,</li>
                                 <li class="li">4 mixed-precision Third Generation Tensor Cores supporting half-precision (fp16), 
                                    <samp class="ph codeph">__nv_bfloat16</samp>, <samp class="ph codeph">tf32</samp>,
                                    sub-byte and double precision (fp64) matrix arithmetic 
                                    (see <a class="xref" href="index.html#wmma" shape="rect">Warp matrix functions</a> for details),
                                    
                                 </li>
                                 <li class="li">16 special function units for single-precision floating-point transcendental functions,</li>
                                 <li class="li">4 warp schedulers.</li>
                              </ul>
                           </div>
                           <p class="p">An SM statically distributes its warps among its schedulers. Then, at every
                              instruction issue time, each scheduler issues one instruction for one of its assigned
                              warps that is ready to execute, if any.
                              
                           </p>
                           <div class="p">An SM has:
                              
                              <ul class="ul">
                                 <li class="li">a read-only constant cache that is shared by all functional units and speeds up reads
                                    from the constant memory space, which resides in device memory,
                                 </li>
                                 <li class="li">a unified data cache and shared memory with a total size of 192 KB for devices of compute capability 8.0 (1.5x <dfn class="term">Volta</dfn>'s
                                    128 KB capacity) and 128 KB for devices of compute capability 8.6.
                                 </li>
                              </ul>
                           </div>
                           <p class="p">Shared memory is partitioned out of the unified data cache, and can be configured to various sizes
                              (see <a class="xref" href="index.html#shared-memory-8-x" shape="rect">Shared Memory</a> section).
                              The remaining data cache serves as an L1 cache and is also used by the texture unit
                              that implements the various addressing and data filtering modes mentioned in
                              <a class="xref" href="index.html#texture-and-surface-memory" shape="rect">Texture and Surface Memory</a>.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="global-memory-8-x"><a name="global-memory-8-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#global-memory-8-x" name="global-memory-8-x" shape="rect">J.7.2.&nbsp;Global Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Global memory behaves the same way as for devices of compute capability 5.x (See <a class="xref" href="index.html#global-memory-5-x" shape="rect">Global Memory</a>).
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory-8-x"><a name="shared-memory-8-x" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-8-x" name="shared-memory-8-x" shape="rect">J.7.3.&nbsp;Shared Memory</a></h3>
                        <div class="body conbody">
                           <p class="p">Similar to the <a class="xref" href="index.html#architecture-7-x" shape="rect">Volta architecture</a>, the amount
                              of the unified data cache reserved for shared memory is configurable on a per kernel basis.
                              For the <dfn class="term">NVIDIA Ampere GPU architecture</dfn>, 
                              the unified data cache has a size of 192 KB for devices of compute capability 8.0 and 128 KB for devices of compute capability
                              8.6.
                              The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132 or 164 KB for devices of compute capability 8.0,
                              and to 0, 8, 16, 32, 64 or 100 KB for devices of compute capability 8.6.
                              
                           </p>
                           <p class="p">
                              An application can set the <samp class="ph codeph">carveout</samp>, i.e., the preferred shared memory capacity,
                              with the <samp class="ph codeph">cudaFuncSetAttribute()</samp>.
                           </p><pre xml:space="preserve">
cudaFuncSetAttribute(kernel_name, cudaFuncAttributePreferredSharedMemoryCarveout, carveout);
</pre><p class="p">
                              The API can specify the carveout either as an
                              integer percentage of the maximum supported shared memory capacity of 164 KB for devices of compute capability 8.0
                              and 100 KB for devices of compute capability 8.6 respectively, or as one of the following values:
                              <samp class="ph codeph">{cudaSharedmemCarveoutDefault</samp>, <samp class="ph codeph">cudaSharedmemCarveoutMaxL1</samp>, or
                              <samp class="ph codeph">cudaSharedmemCarveoutMaxShared</samp>.
                              
                              When using a percentage, the carveout is rounded up to the nearest supported
                              shared memory capacity. For example, for devices of compute capability 8.0, 50% will map to a 100 KB carveout instead of an
                              82 KB one.
                              Setting the <samp class="ph codeph">cudaFuncAttributePreferredSharedMemoryCarveout</samp> is considered
                              a hint by the driver; the driver may choose a different configuration, if needed.
                              
                           </p>
                           <p class="p">
                              Devices of compute capability 8.0 allow a single thread block to address up to 163 KB of shared memory,
                              while devices of compute capability 8.6 allow up to 99 KB of shared memory.
                              Kernels relying on shared memory allocations over 48 KB per block are architecture-specific,
                              and must use dynamic shared memory rather than statically sized shared memory arrays.
                              These kernels require an explicit opt-in by using 
                              <samp class="ph codeph">cudaFuncSetAttribute()</samp> to set the <samp class="ph codeph">cudaFuncAttributeMaxDynamicSharedMemorySize</samp>;
                              see <a class="xref" href="topics/compute-capabilities.html#shared-memory-7.x" shape="rect">Shared Memory</a> for the Volta architecture.
                              
                           </p>
                           <p class="p">Note that the maximum amount of shared memory per thread block is smaller than the
                              maximum shared memory partition available per SM. The 1 KB of shared memory not made available to a thread block
                              is reserved for system use.
                              
                           </p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="driver-api"><a name="driver-api" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#driver-api" name="driver-api" shape="rect">K.&nbsp;Driver API</a></h2>
                  <div class="body conbody">
                     <p class="p">This appendix assumes knowledge of the concepts described in <a class="xref" href="index.html#cuda-c-runtime" shape="rect">CUDA Runtime</a>.
                     </p>
                     <p class="p">The driver API is implemented in the <samp class="ph codeph">cuda</samp> dynamic
                        library (<samp class="ph codeph">cuda.dll</samp> or <samp class="ph codeph">cuda.so</samp>) which is
                        copied on the system during the installation of the device driver. All
                        its entry points are prefixed with cu.
                     </p>
                     <p class="p">It is a handle-based, imperative API: Most objects are referenced by
                        opaque handles that may be specified to functions to manipulate the
                        objects.
                     </p>
                     <p class="p">The objects available in the driver API are summarized in <a class="xref" href="index.html#driver-api__objects-available-in-cuda-driver-api" shape="rect">Table 16</a>.
                     </p>
                     <div class="tablenoborder"><a name="driver-api__objects-available-in-cuda-driver-api" shape="rect">
                           <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="driver-api__objects-available-in-cuda-driver-api" class="table" frame="border" border="1" rules="all">
                           <caption><span class="tablecap">Table 16. Objects Available in the CUDA Driver API</span></caption>
                           <thead class="thead" align="left">
                              <tr class="row">
                                 <th class="entry" valign="top" width="33.33333333333333%" id="d225e33972" rowspan="1" colspan="1">Object</th>
                                 <th class="entry" valign="top" width="16.666666666666664%" id="d225e33975" rowspan="1" colspan="1">Handle</th>
                                 <th class="entry" valign="top" width="50%" id="d225e33978" rowspan="1" colspan="1">Description</th>
                              </tr>
                           </thead>
                           <tbody class="tbody">
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Device</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUdevice</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">CUDA-enabled device</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Context</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUcontext</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Roughly equivalent to a CPU process</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Module</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUmodule</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Roughly equivalent to a dynamic library</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Function</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUfunction</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Kernel</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Heap memory</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUdeviceptr</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Pointer to device memory</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">CUDA array</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUarray</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Opaque container for one-dimensional or two-dimensional data on the
                                    device, readable via texture or surface references
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Texture reference</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUtexref</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Object that describes how to interpret texture memory data</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Surface reference</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUsurfref</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Object that describes how to read or write CUDA arrays</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Stream</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUstream</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Object that describes a CUDA stream</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d225e33972" rowspan="1" colspan="1">Event</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d225e33975" rowspan="1" colspan="1">CUevent</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e33978" rowspan="1" colspan="1">Object that describes a CUDA event</td>
                              </tr>
                           </tbody>
                        </table>
                     </div>
                     <p class="p">The driver API must be initialized with <samp class="ph codeph">cuInit()</samp>
                        before any function from the driver API is called. A CUDA context must
                        then be created that is attached to a specific device and made current
                        to the calling host thread as detailed in <a class="xref" href="index.html#context" shape="rect">Context</a>.
                     </p>
                     <p class="p">Within a CUDA context, kernels are explicitly loaded as PTX or binary
                        objects by the host code as described in  <a class="xref" href="index.html#module" shape="rect">Module</a>.
                        Kernels written in C++ must therefore be compiled separately into
                        <dfn class="term">PTX</dfn> or binary objects. Kernels are launched using API
                        entry points as described in  <a class="xref" href="index.html#kernel-execution" shape="rect">Kernel Execution</a>.
                     </p>
                     <p class="p">Any application that wants to run on future device architectures must
                        load <dfn class="term">PTX</dfn>, not binary code. This is because binary code is
                        architecture-specific and therefore incompatible with future
                        architectures, whereas <dfn class="term">PTX</dfn> code is compiled to binary code
                        at load time by the device driver.
                     </p>
                     <p class="p">Here is the host code of the sample from <a class="xref" href="index.html#kernels" shape="rect">Kernels</a>
                        written using the driver API:
                     </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
{
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> N = ...;
    size_t size = N * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate input vectors h_A and h_B in host memory</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* h_A = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)malloc(size);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* h_B = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)malloc(size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize input vectors</span>
    ...

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Initialize</span>
    cuInit(0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get number of devices supporting CUDA</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> deviceCount = 0;
    cuDeviceGetCount(&amp;deviceCount);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (deviceCount == 0) {
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"There is no device supporting CUDA.\n"</span>);
        exit (0);
    }

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get handle for device 0</span>
    CUdevice cuDevice;
    cuDeviceGet(&amp;cuDevice, 0);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create context</span>
    CUcontext cuContext;
    cuCtxCreate(&amp;cuContext, 0, cuDevice);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create module from binary file</span>
    CUmodule cuModule;
    cuModuleLoad(&amp;cuModule, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"VecAdd.ptx"</span>);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate vectors in device memory</span>
    CUdeviceptr d_A;
    cuMemAlloc(&amp;d_A, size);
    CUdeviceptr d_B;
    cuMemAlloc(&amp;d_B, size);
    CUdeviceptr d_C;
    cuMemAlloc(&amp;d_C, size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Copy vectors from host memory to device memory</span>
    cuMemcpyHtoD(d_A, h_A, size);
    cuMemcpyHtoD(d_B, h_B, size);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get function handle from module</span>
    CUfunction vecAdd;
    cuModuleGetFunction(&amp;vecAdd, cuModule, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"VecAdd"</span>);

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Invoke kernel</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> threadsPerBlock = 256;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* args[] = { &amp;d_A, &amp;d_B, &amp;d_C, &amp;N };
    cuLaunchKernel(vecAdd,
                   blocksPerGrid, 1, 1, threadsPerBlock, 1, 1,
                   0, 0, args, 0);

    ...
}</pre><p class="p">Full code can be found in the <samp class="ph codeph">vectorAddDrv</samp> CUDA 
                        sample.
                     </p>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="context"><a name="context" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#context" name="context" shape="rect">K.1.&nbsp;Context</a></h3>
                     <div class="body conbody">
                        <p class="p">A CUDA context is analogous to a CPU process. All resources and actions
                           performed within the driver API are encapsulated inside a CUDA context,
                           and the system automatically cleans up these resources when the context
                           is destroyed. Besides objects such as modules and texture or surface
                           references, each context has its own distinct address space. As a result,
                           <samp class="ph codeph">CUdeviceptr</samp> values from different contexts reference
                           different memory locations.
                        </p>
                        <p class="p">A host thread may have only one device context current at a time. When a
                           context is created with <samp class="ph codeph">cuCtxCreate(</samp>), it is made
                           current to the calling host thread. CUDA functions that operate in a
                           context (most functions that do not involve device enumeration or context
                           management) will return <samp class="ph codeph">CUDA_ERROR_INVALID_CONTEXT</samp> if a
                           valid context is not current to the thread.
                        </p>
                        <p class="p">Each host thread has a stack of current contexts.
                           <samp class="ph codeph">cuCtxCreate()</samp> pushes the new context onto the top of the
                           stack. <samp class="ph codeph">cuCtxPopCurrent()</samp> may be called to detach the
                           context from the host thread. The context is then "floating" and may be
                           pushed as the current context for any host thread.
                           <samp class="ph codeph">cuCtxPopCurrent()</samp> also restores the previous current
                           context, if any.
                        </p>
                        <p class="p">A usage count is also maintained for each context.
                           <samp class="ph codeph">cuCtxCreate()</samp> creates a context with a usage count of 1.
                           <samp class="ph codeph">cuCtxAttach()</samp> increments the usage count and
                           <samp class="ph codeph">cuCtxDetach()</samp> decrements it. A context is destroyed when
                           the usage count goes to 0 when calling <samp class="ph codeph">cuCtxDetach()</samp> or
                           <samp class="ph codeph">cuCtxDestroy()</samp>.
                        </p>
                        <p class="p">The driver API is interoperable with the runtime and it is possible to access the <dfn class="term">primary context</dfn> (see <a class="xref" href="index.html#initialization" shape="rect">Initialization</a>) managed by the runtime from the driver API via <samp class="ph codeph">cuDevicePrimaryCtxRetain()</samp>.
                        </p>
                        <p class="p">Usage count facilitates interoperability between third party authored
                           code operating in the same context. For example, if three libraries are
                           loaded to use the same context, each library would call
                           <samp class="ph codeph">cuCtxAttach()</samp> to increment the usage count and
                           <samp class="ph codeph">cuCtxDetach()</samp> to decrement the usage count when the
                           library is done using the context. For most libraries, it is expected
                           that the application will have created a context before loading or
                           initializing the library; that way, the application can create the
                           context using its own heuristics, and the library simply operates on the
                           context handed to it. Libraries that wish to create their own contexts -
                           unbeknownst to their API clients who may or may not have created contexts
                           of their own - would use <samp class="ph codeph">cuCtxPushCurrent()</samp> and
                           <samp class="ph codeph">cuCtxPopCurrent()</samp> as illustrated in <a class="xref" href="index.html#context__library-context-management" shape="rect">Figure 19</a>.
                        </p>
                        <div class="fig fignone" id="context__library-context-management"><a name="context__library-context-management" shape="rect">
                              <!-- --></a><span class="figcap">Figure 19. Library Context Management</span><br clear="none"></br><img class="image" src="graphics/library-context-management.png" alt="Library Context Management."></img><br clear="none"></br></div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="module"><a name="module" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#module" name="module" shape="rect">K.2.&nbsp;Module</a></h3>
                     <div class="body conbody">
                        <p class="p">Modules are dynamically loadable packages of device code and data, akin to DLLs in Windows, that are output by nvcc (see <a class="xref" href="index.html#compilation-with-nvcc" shape="rect">Compilation with NVCC</a>). The names for all symbols, including functions, global variables, and texture or surface references, are maintained at
                           module scope so that modules written by independent third parties may interoperate in the same CUDA context.
                        </p>
                        <p class="p">This code sample loads a module and retrieves a handle to some kernel:</p><pre xml:space="preserve">CUmodule cuModule;
cuModuleLoad(&amp;cuModule, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"myModule.ptx"</span>);
CUfunction myKernel;
cuModuleGetFunction(&amp;myKernel, cuModule, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"MyKernel"</span>);</pre><p class="p">This code sample compiles and loads a new module from PTX code and parses compilation errors:</p><pre xml:space="preserve">
  
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define BUFFER_SIZE 8192</span>
CUmodule cuModule;
CUjit_option options[3];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* values[3];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* PTXCode = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"some PTX code"</span>;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> error_log[BUFFER_SIZE];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> err;
options[0] = CU_JIT_ERROR_LOG_BUFFER;
values[0]  = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)error_log;
options[1] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
values[1]  = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)BUFFER_SIZE;
options[2] = CU_JIT_TARGET_FROM_CUCONTEXT;
values[2]  = 0;
err = cuModuleLoadDataEx(&amp;cuModule, PTXCode, 3, options, values);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (err != CUDA_SUCCESS)
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Link error:\n%s\n"</span>, error_log);
</pre><p class="p">This code sample compiles, links, and loads a new module from multiple PTX codes and parses link and compilation errors:</p><pre xml:space="preserve">
        
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define BUFFER_SIZE 8192</span>
CUmodule cuModule;
CUjit_option options[6];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* values[6];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> walltime;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> error_log[BUFFER_SIZE], info_log[BUFFER_SIZE];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* PTXCode0 = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"some PTX code"</span>;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span>* PTXCode1 = <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"some other PTX code"</span>;
CUlinkState linkState;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> err;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* cubin;
size_t cubinSize;
options[0] = CU_JIT_WALL_TIME;
values[0] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)&amp;walltime;
options[1] = CU_JIT_INFO_LOG_BUFFER;
values[1] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)info_log;
options[2] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
values[2] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)BUFFER_SIZE;
options[3] = CU_JIT_ERROR_LOG_BUFFER;
values[3] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)error_log;
options[4] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
values[4] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)BUFFER_SIZE;
options[5] = CU_JIT_LOG_VERBOSE;
values[5] = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)1;
cuLinkCreate(6, options, values, &amp;linkState);
err = cuLinkAddData(linkState, CU_JIT_INPUT_PTX,
                    (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)PTXCode0, strlen(PTXCode0) + 1, 0, 0, 0, 0);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (err != CUDA_SUCCESS)
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Link error:\n%s\n"</span>, error_log);
err = cuLinkAddData(linkState, CU_JIT_INPUT_PTX,
                    (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>*)PTXCode1, strlen(PTXCode1) + 1, 0, 0, 0, 0);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (err != CUDA_SUCCESS)
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Link error:\n%s\n"</span>, error_log);
cuLinkComplete(linkState, &amp;cubin, &amp;cubinSize);
printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Link completed in %fms. Linker Output:\n%s\n"</span>, walltime, info_log);
cuModuleLoadData(cuModule, cubin);
cuLinkDestroy(linkState);

      </pre><p class="p">
                           Full code can be found in the <samp class="ph codeph">ptxjit</samp> CUDA 
                           sample.
                           
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="kernel-execution"><a name="kernel-execution" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#kernel-execution" name="kernel-execution" shape="rect">K.3.&nbsp;Kernel Execution</a></h3>
                     <div class="body conbody">
                        <p class="p"><samp class="ph codeph">cuLaunchKernel()</samp> launches a kernel with a given
                           execution configuration.
                        </p>
                        <p class="p">Parameters are passed either as an array of pointers (next to last
                           parameter of <samp class="ph codeph">cuLaunchKernel()</samp>) where the nth pointer
                           corresponds to the nth parameter and points to a region of memory from
                           which the parameter is copied, or as one of the extra options (last
                           parameter of <samp class="ph codeph">cuLaunchKernel()</samp>).
                        </p>
                        <p class="p">When parameters are passed as an extra option (the
                           <samp class="ph codeph">CU_LAUNCH_PARAM_BUFFER_POINTER</samp> option), they are passed
                           as a pointer to a single buffer where parameters are assumed to be
                           properly offset with respect to each other by matching the alignment
                           requirement for each parameter type in device code.
                        </p>
                        <p class="p">Alignment requirements in device code for the built-in vector types are
                           listed in <a class="xref" href="index.html#vector-types__alignment-requirements-in-device-code" shape="rect">Table 4</a>. For all
                           other basic types, the alignment requirement in device code matches the
                           alignment requirement in host code and can therefore be obtained using
                           <samp class="ph codeph">__alignof()</samp>. The only exception is when the host
                           compiler aligns <samp class="ph codeph">double</samp> and <samp class="ph codeph">long long</samp>
                           (and <samp class="ph codeph">long</samp> on a 64-bit system) on a one-word boundary
                           instead of a two-word boundary (for example, using <samp class="ph codeph">gcc</samp>'s
                           compilation flag <samp class="ph codeph">-mno-align-double</samp>) since in device code
                           these types are always aligned on a two-word boundary.
                        </p>
                        <p class="p"><samp class="ph codeph">CUdeviceptr</samp> is an integer, but represents a pointer, so
                           its alignment requirement is <samp class="ph codeph">__alignof(void*)</samp>.
                        </p>
                        <p class="p">The following code sample uses a macro (<samp class="ph codeph">ALIGN_UP()</samp>) to
                           adjust the offset of each parameter to meet its alignment requirement and
                           another macro (<samp class="ph codeph">ADD_TO_PARAM_BUFFER()</samp>) to add each
                           parameter to the parameter buffer passed to the
                           <samp class="ph codeph">CU_LAUNCH_PARAM_BUFFER_POINTER</samp> option.
                        </p><pre xml:space="preserve">#define ALIGN_UP(offset, alignment) \
      (offset) = ((offset) + (alignment) - 1) &amp; ~((alignment) - 1)

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> paramBuffer[1024];
size_t paramBufferSize = 0;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#define ADD_TO_PARAM_BUFFER(value, alignment)                   \
    do {                                                        \
        paramBufferSize = ALIGN_UP(paramBufferSize, alignment); \
        memcpy(paramBuffer + paramBufferSize,                   \
               &amp;(value), sizeof(value));                        \
        paramBufferSize += sizeof(value);                       \
    } while (0)</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i;
ADD_TO_PARAM_BUFFER(i, __alignof(i));
float4 f4;
ADD_TO_PARAM_BUFFER(f4, 16); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// float4's alignment is 16</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> c;
ADD_TO_PARAM_BUFFER(c, __alignof(c));
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span> f;
ADD_TO_PARAM_BUFFER(f, __alignof(f));
CUdeviceptr devPtr;
ADD_TO_PARAM_BUFFER(devPtr, __alignof(devPtr));
float2 f2;
ADD_TO_PARAM_BUFFER(f2, 8); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// float2's alignment is 8</span>

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>* extra[] = {
    CU_LAUNCH_PARAM_BUFFER_POINTER, paramBuffer,
    CU_LAUNCH_PARAM_BUFFER_SIZE,    &amp;paramBufferSize,
    CU_LAUNCH_PARAM_END
};
cuLaunchKernel(cuFunction,
               blockWidth, blockHeight, blockDepth,
               gridWidth, gridHeight, gridDepth,
               0, 0, 0, extra);</pre><p class="p">The alignment requirement of a structure is equal to the maximum of the
                           alignment requirements of its fields. The alignment requirement of a
                           structure that contains built-in vector types,
                           <samp class="ph codeph">CUdeviceptr</samp>, or non-aligned <samp class="ph codeph">double</samp> and
                           <samp class="ph codeph">long long</samp>, might therefore differ between device code
                           and host code. Such a structure might also be padded differently.  The
                           following structure, for example, is not padded at all in host code, but
                           it is padded in device code with 12 bytes after field <samp class="ph codeph">f</samp>
                           since the alignment requirement for field <samp class="ph codeph">f4</samp> is 16.
                        </p><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">struct</span> {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>  f;
    float4 f4;
} myStruct;</pre></div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="interoperability-between-runtime-and-driver-apis"><a name="interoperability-between-runtime-and-driver-apis" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#interoperability-between-runtime-and-driver-apis" name="interoperability-between-runtime-and-driver-apis" shape="rect">K.4.&nbsp;Interoperability between Runtime and Driver APIs</a></h3>
                     <div class="body conbody">
                        <p class="p">An application can mix runtime API code with driver API code.</p>
                        <p class="p">If a context is created and made current via the driver API, subsequent
                           runtime calls will pick up this context instead of creating a new
                           one.
                        </p>
                        <p class="p">If the runtime is initialized (implicitly as mentioned in <a class="xref" href="index.html#cuda-c-runtime" shape="rect">CUDA Runtime</a>), <samp class="ph codeph">cuCtxGetCurrent()</samp> can be
                           used to retrieve the context created during initialization. This context
                           can be used by subsequent driver API calls.
                        </p>
                        <p class="p">The implicitly created context from the runtime is called the <dfn class="term">primary context</dfn> (see <a class="xref" href="index.html#initialization" shape="rect">Initialization</a>). It can be managed from the driver API with the <a class="xref" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PRIMARY__CTX.html" target="_blank" shape="rect">Primary Context Management</a> functions.
                        </p>
                        <p class="p">Device memory can be allocated and freed using either API.
                           <samp class="ph codeph">CUdeviceptr</samp> can be cast to regular pointers and
                           vice-versa:
                        </p><pre xml:space="preserve">CUdeviceptr devPtr;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>* d_data;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocation using driver API</span>
cuMemAlloc(&amp;devPtr, size);
d_data = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">float</span>*)devPtr;

<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocation using runtime API</span>
cudaMalloc(&amp;d_data, size);
devPtr = (CUdeviceptr)d_data;</pre><p class="p">In particular, this means that applications written using the driver API
                           can invoke libraries written using the runtime API (such as cuFFT,
                           cuBLAS, ...).
                        </p>
                        <p class="p">All functions from the device and version management sections of the
                           reference manual can be used interchangeably.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="driver-entry-point-access"><a name="driver-entry-point-access" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#driver-entry-point-access" name="driver-entry-point-access" shape="rect">K.5.&nbsp;Driver Entry Point Access</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="introduction-driver-entry-point-access"><a name="introduction-driver-entry-point-access" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#introduction-driver-entry-point-access" name="introduction-driver-entry-point-access" shape="rect">K.5.1.&nbsp;Introduction</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              The <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DRIVER__ENTRY__POINT.html" target="_blank" shape="rect">Driver Entry Point Access APIs</a></samp>
                              provide a way to retrieve the address of a CUDA driver function. Starting from CUDA 11.3, users
                              can call into available CUDA driver APIs using function pointers obtained from these APIs.
                              
                           </p>
                           <div class="p">
                              These APIs provide functionality similar to their counterparts, dlsym on POSIX platforms and
                              GetProcAddress on Windows. The provided APIs will let users:
                              
                              <ul class="ul">
                                 <li class="li"> Retrieve the address of a driver function using the <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DRIVER__ENTRY__POINT.html" target="_blank" shape="rect">CUDA Driver API.</a></samp></li>
                                 <li class="li"> Retrieve the address of a driver function using the <samp class="ph codeph"><a class="xref" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DRIVER__ENTRY__POINT.html" target="_blank" shape="rect">CUDA Runtime API.</a></samp></li>
                                 <li class="li"> Request <dfn class="term">per-thread default stream</dfn> version of a CUDA driver function.
                                    For more details, see <a class="xref" href="index.html#retrieve-per-thread-default-stream-versions" shape="rect">Retrieve per-thread default stream versions</a></li>
                                 <li class="li"> Access new CUDA features on older toolkits but with a newer driver.</li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="driver-function-typedefs"><a name="driver-function-typedefs" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#driver-function-typedefs" name="driver-function-typedefs" shape="rect">K.5.2.&nbsp;Driver Function Typedefs</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              To help retrieve the CUDA Driver API entry points, the CUDA Toolkit provides access to
                              headers containing the function pointer definitions for all CUDA driver APIs.
                              These headers are installed with the CUDA Toolkit and are made available in the toolkit's
                              <samp class="ph codeph">include/</samp> directory. The table below summarizes the header files containing
                              the <samp class="ph codeph">typedefs</samp> for each CUDA API header file.
                              
                           </p>
                           <div class="tablenoborder"><a name="driver-function-typedefs__driver-api-typedefs-header-files" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="driver-function-typedefs__driver-api-typedefs-header-files" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 17. Typedefs header files for CUDA driver APIs</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="50%" id="d225e34526" rowspan="1" colspan="1">API header file</th>
                                       <th class="entry" valign="top" width="50%" id="d225e34529" rowspan="1" colspan="1">API Typedef header file</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cuda.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaGL.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaGLTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaProfiler.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaProfilerTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaVDPAU.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaVDPAUTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaEGL.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaEGLTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D9.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D9Typedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D10.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D10Typedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d225e34526" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D11.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d225e34529" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D11Typedefs.h</samp></td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>
                           <p class="p">
                              The above headers do not define actual function pointers themselves; they define the
                              typedefs for function pointers. For example, <samp class="ph codeph">cudaTypedefs.h</samp>
                              has the below typedefs for the driver API <samp class="ph codeph">cuMemAlloc</samp>:
                              
                           </p><pre xml:space="preserve">
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> CUresult (CUDAAPI *PFN_cuMemAlloc_v3020)(CUdeviceptr_v2 *dptr, size_t bytesize);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> CUresult (CUDAAPI *PFN_cuMemAlloc_v2000)(CUdeviceptr_v1 *dptr, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">unsigned</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> bytesize);
  </pre><p class="p">
                              CUDA driver symbols have a version based naming scheme with a <samp class="ph codeph">_v*</samp> extension in its name
                              except for the first version. When the signature or the semantics of a specific CUDA driver API
                              changes, we increment the version number of the corresponding driver symbol. In the case of the
                              <samp class="ph codeph">cuMemAlloc</samp> driver API, the first driver symbol name is <samp class="ph codeph">cuMemAlloc</samp>
                              and the next symbol name is <samp class="ph codeph">cuMemAlloc_v2</samp>. The typedef for the first
                              version which was introduced in CUDA 2.0 (2000) is <samp class="ph codeph">PFN_cuMemAlloc_v2000</samp>.
                              The typedef for the next version which was introduced in CUDA 3.2 (3020) is <samp class="ph codeph">PFN_cuMemAlloc_v3020</samp>.
                              
                           </p>
                           <p class="p">
                              The <samp class="ph codeph">typedefs</samp> can be used to more easily define a function pointer of the
                              appropriate type in code:
                              
                           </p><pre xml:space="preserve">
    PFN_cuMemAlloc_v3020 pfn_cuMemAlloc_v2;
    PFN_cuMemAlloc_v2000 pfn_cuMemAlloc_v1;
  </pre><p class="p">
                              The above method is preferable if users are interested in a specific version of the API.
                              Additionally, the headers have predefined macros for the latest version of all driver symbols
                              that were available when the installed CUDA toolkit was released; these typedefs do not have
                              a <samp class="ph codeph">_v*</samp> suffix. For CUDA 11.3 toolkit, <samp class="ph codeph">cuMemAlloc_v2</samp> was
                              the latest version and so we can also define its function pointer as below:
                              
                           </p><pre xml:space="preserve">
    PFN_cuMemAlloc pfn_cuMemAlloc;
  </pre></div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="driver-function-retrieval"><a name="driver-function-retrieval" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#driver-function-retrieval" name="driver-function-retrieval" shape="rect">K.5.3.&nbsp;Driver Function Retrieval</a></h3>
                        <div class="body conbody">
                           <p class="p">
                              Using the Driver Entry Point Access APIs and the appropriate typedef, we can get the function
                              pointer to any CUDA driver API.
                              
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="using-the-driver-API"><a name="using-the-driver-API" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#using-the-driver-API" name="using-the-driver-API" shape="rect">K.5.3.1.&nbsp;Using the driver API</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 The driver API requires CUDA version as an argument to get the ABI compatible version for the
                                 requested driver symbol. CUDA Driver APIs have a per-function ABI denoted with a <samp class="ph codeph">_v*</samp>
                                 extension. For example, consider the versions of <samp class="ph codeph">cuStreamBeginCapture</samp> and
                                 their corresponding <samp class="ph codeph">typedefs</samp> from <samp class="ph codeph">cudaTypedefs.h</samp>:
                                 
                              </p><pre xml:space="preserve">
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cuda.h</span>
    CUresult CUDAAPI cuStreamBeginCapture(CUstream hStream);
    CUresult CUDAAPI cuStreamBeginCapture_v2(CUstream hStream, CUstreamCaptureMode mode);
            
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cudaTypedefs.h</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> CUresult (CUDAAPI *PFN_cuStreamBeginCapture_v10000)(CUstream hStream);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> CUresult (CUDAAPI *PFN_cuStreamBeginCapture_v10010)(CUstream hStream, CUstreamCaptureMode mode);
  </pre><p class="p">
                                 From the above <samp class="ph codeph">typedefs</samp> in the code snippet, version suffixes <samp class="ph codeph">_v10000</samp>
                                 and <samp class="ph codeph">_v10010</samp> indicate that the above APIs were introduced in CUDA 10.0 and CUDA 10.1
                                 respectively.
                                 
                              </p><pre xml:space="preserve">
    #include &lt;cudaTypedefs.h&gt;
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Declare the entry points for cuStreamBeginCapture</span>
    PFN_cuStreamBeginCapture_v10000 pfn_cuStreamBeginCapture_v1;
    PFN_cuStreamBeginCapture_v10010 pfn_cuStreamBeginCapture_v2;
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get the function pointer to the cuStreamBeginCapture driver symbol</span>
    cuGetProcAddress(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"cuStreamBeginCapture"</span>, &amp;pfn_cuStreamBeginCapture_v1, 10000, CU_GET_PROC_ADDRESS_DEFAULT);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get the function pointer to the cuStreamBeginCapture_v2 driver symbol</span>
    cuGetProcAddress(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"cuStreamBeginCapture"</span>, &amp;pfn_cuStreamBeginCapture_v2, 10010, CU_GET_PROC_ADDRESS_DEFAULT);
  </pre><p class="p">
                                 Referring to the code snippet above, to retrieve the address to the <samp class="ph codeph">_v1</samp> version of
                                 the driver API <samp class="ph codeph">cuStreamBeginCapture</samp>, the CUDA version argument should be exactly
                                 10.0 (10000). Similarly, the CUDA version for retrieving the address to the <samp class="ph codeph">_v2</samp>
                                 version of the API should be 10.1 (10010). Specifying a higher CUDA version for retrieving a specific version
                                 of a driver API might not always be portable. For example, using 11030 here would still return the <samp class="ph codeph">_v2</samp>
                                 symbol, but if a hypothetical <samp class="ph codeph">_v3</samp> version is released in CUDA 11.3, the <samp class="ph codeph">cuGetProcAddress</samp>
                                 API would start returning the newer <samp class="ph codeph">_v3</samp> symbol instead when paired with a CUDA 11.3
                                 driver. Since the ABI and function signatures of the <samp class="ph codeph">_v2</samp> and <samp class="ph codeph">_v3</samp>
                                 symbols might differ, calling the <samp class="ph codeph">_v3</samp> function using the <samp class="ph codeph">_v10010</samp>
                                 typedef intended for the <samp class="ph codeph">_v2</samp> symbol would exhibit undefined behavior.
                                 
                              </p>
                              <p class="p">
                                 To retrieve the latest version of a driver API for a given CUDA Toolkit, we can also specify
                                 CUDA_VERSION as the <samp class="ph codeph">version</samp> argument and use the unversioned typedef to
                                 define the function pointer. Since <samp class="ph codeph">_v2</samp> is the latest version of the
                                 driver API <samp class="ph codeph">cuStreamBeginCapture</samp> in CUDA 11.3, the below code snippet
                                 shows a different method to retrieve it.
                                 
                              </p><pre xml:space="preserve">
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Assuming we are using CUDA 11.3 Toolkit</span>

    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-directive">#include &lt;cudaTypedefs.h&gt;</span>
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Declare the entry point</span>
    PFN_cuStreamBeginCapture pfn_cuStreamBeginCapture_latest;
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Intialize the entry point. Specifying CUDA_VERSION will give the function pointer to the</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// cuStreamBeginCapture_v2 symbol since it is latest version on CUDA 11.3.</span>
    cuGetProcAddress(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"cuStreamBeginCapture"</span>, &amp;pfn_cuStreamBeginCapture_latest, CUDA_VERSION, CU_GET_PROC_ADDRESS_DEFAULT);
  </pre><p class="p">
                                 Note that requesting a driver API with an invalid CUDA version will return an error
                                 <samp class="ph codeph">CUDA_ERROR_NOT_FOUND</samp>. In the above code examples, passing in a
                                 version less than 10000 (CUDA 10.0) would be invalid.
                                 
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="using-the-runtime-API"><a name="using-the-runtime-API" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#using-the-runtime-API" name="using-the-runtime-API" shape="rect">K.5.3.2.&nbsp;Using the runtime API</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 The runtime API uses the CUDA runtime version to get the ABI compatible version for the
                                 requested driver symbol. In the below code snippet, the minimum CUDA runtime version
                                 required would be CUDA 11.2 as <samp class="ph codeph">cuMemAllocAsync</samp> was introduced then.
                                 
                              </p><pre xml:space="preserve">
    #include &lt;cudaTypedefs.h&gt;
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Declare the entry point</span>
    PFN_cuMemAllocAsync pfn_cuMemAllocAsync;
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Intialize the entry point. Assuming CUDA runtime version &gt;= 11.2</span>
    cudaGetDriverEntryPoint(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"cuMemAllocAsync"</span>, &amp;pfn_cuMemAllocAsync, cudaEnableDefault);
    
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Call the entry point</span>
    pfn_cuMemAllocAsync(...);
  </pre></div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="retrieve-per-thread-default-stream-versions"><a name="retrieve-per-thread-default-stream-versions" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#retrieve-per-thread-default-stream-versions" name="retrieve-per-thread-default-stream-versions" shape="rect">K.5.3.3.&nbsp;Retrieve per-thread default stream versions</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 Some CUDA driver APIs can be configured to have <dfn class="term">default stream</dfn> or
                                 <dfn class="term">per-thread default stream</dfn> semantics. Driver APIs having
                                 <dfn class="term">per-thread default stream</dfn> semantics are suffixed with
                                 <dfn class="term">_ptsz</dfn> or <dfn class="term">_ptds</dfn> in their name. For example,
                                 <samp class="ph codeph">cuLaunchKernel</samp> has a <dfn class="term">per-thread default stream</dfn> variant
                                 named <samp class="ph codeph">cuLaunchKernel_ptsz</samp>. With the Driver Entry Point Access APIs,
                                 users can request for the <dfn class="term">per-thread default stream</dfn> version of the driver API
                                 <samp class="ph codeph">cuLaunchKernel</samp> instead of the <dfn class="term">default stream</dfn> version.
                                 Configuring the CUDA driver APIs for <dfn class="term">default stream</dfn> or <dfn class="term">per-thread default stream</dfn>
                                 semantics affects the synchronization behavior. More details can be found
                                 <a class="xref" href="https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html#stream-sync-behavior__default-stream" target="_blank" shape="rect">here</a>.
                                 
                              </p>
                              <div class="p">
                                 The <dfn class="term">default stream</dfn> or <dfn class="term">per-thread default stream</dfn> versions of a
                                 driver API can be obtained by one of the following ways:
                                 
                                 <ul class="ul">
                                    <li class="li"> Use the compilation flag <samp class="ph codeph">--default-stream per-thread</samp> or define the
                                       marco <samp class="ph codeph">CUDA_API_PER_THREAD_DEFAULT_STREAM</samp> to get <dfn class="term">per-thread default
                                          stream</dfn> behavior. 
                                    </li>
                                    <li class="li"> Force <dfn class="term">default stream</dfn> or <dfn class="term">per-thread default stream</dfn> behavior using the flags
                                       <samp class="ph codeph">CU_GET_PROC_ADDRESS_LEGACY_STREAM/cudaEnableLegacyStream</samp> or
                                       <samp class="ph codeph">CU_GET_PROC_ADDRESS_PER_THREAD_DEFAULT_STREAM/cudaEnablePerThreadDefaultStream</samp>
                                       respectively. 
                                    </li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="access-new-CUDA-features"><a name="access-new-CUDA-features" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#access-new-CUDA-features" name="access-new-CUDA-features" shape="rect">K.5.3.4.&nbsp;Access new CUDA features</a></h3>
                           <div class="body conbody">
                              <p class="p">
                                 It is always recommended to install the latest CUDA toolkit to access new CUDA driver features,
                                 but if for some reason, a user does not want to update or does not have access to the latest toolkit,
                                 the API can be used to access new CUDA features with only an updated CUDA driver. For discussion,
                                 let us assume the user is on CUDA 11.3 and wants to use a new driver API <samp class="ph codeph">cuFoo</samp>
                                 available in the CUDA 12.0 driver. The below code snippet illustrates this use-case:
                                 
                              </p><pre xml:space="preserve">
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main()
    {
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Assuming we have CUDA 12.0 driver installed.</span>

        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Manually define the prototype as cudaTypedefs.h in CUDA 11.3 does not have the cuFoo typedef</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">typedef</span> CUresult (CUDAAPI *PFN_cuFoo)(...);
        PFN_cuFoo pfn_cuFoo = NULL;
        
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Get the address for cuFoo API using cuGetProcAddress. Specify CUDA version as</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// 12000 since cuFoo was introduced then or get the driver version dynamically</span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// using cuDriverGetVersion </span>
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> driverVersion;
        cuDriverGetVersion(&amp;driverVersion);
        cuGetProcAddress(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"cuFoo"</span>, &amp;pfn_cuFoo, driverVersion, CU_GET_PROC_ADDRESS_DEFAULT);
        
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">if</span> (pfn_cuFoo) {
            pfn_cuFoo(...);
        }
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">else</span> {
            printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Cannot retrieve the address to cuFoo. Check if the latest driver for CUDA 12.0 is installed.\n"</span>);
            assert(0);
        }
        
        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// rest of code here</span>
        
    }
  </pre></div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="env-vars"><a name="env-vars" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#env-vars" name="env-vars" shape="rect">L.&nbsp;CUDA Environment Variables</a></h2>
                  <div class="body conbody">
                     <p class="p">Environment variables related to the Multi-Process Service are
                        documented in the Multi-Process Service section of the GPU Deployment and
                        Management guide.
                     </p>
                     <div class="tablenoborder"><a name="env-vars__cuda-environment-variables" shape="rect">
                           <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="env-vars__cuda-environment-variables" class="table" frame="border" border="1" rules="all">
                           <caption><span class="tablecap">Table 18. CUDA Environment Variables</span></caption>
                           <thead class="thead" align="left">
                              <tr class="row">
                                 <th class="entry" valign="top" width="30%" id="d225e34962" rowspan="1" colspan="1">Variable</th>
                                 <th class="entry" valign="top" width="20%" id="d225e34965" rowspan="1" colspan="1">Values</th>
                                 <th class="entry" valign="top" width="50%" id="d225e34968" rowspan="1" colspan="1">Description</th>
                              </tr>
                           </thead>
                           <tbody class="tbody">
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d225e34962 d225e34965 d225e34968" rowspan="1"><strong class="ph b">Device Enumeration and Properties</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_VISIBLE_DEVICES </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">A comma-separated sequence of GPU identifiers<br clear="none"></br> MIG support:
                                    <samp class="ph codeph">MIG-&lt;GPU-UUID&gt;/&lt;GPU instance ID&gt;/&lt;compute instance ID&gt;</samp></td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">GPU identifiers are given as integer indices or as UUID strings. GPU UUID
                                    strings should follow the same format as given by <dfn class="term">nvidia-smi</dfn>, such as
                                    GPU-8932f937-d72c-4106-c12f-20bd9faed9f6. However, for convenience, abbreviated
                                    forms are allowed; simply specify enough digits from the beginning of the GPU UUID
                                    to uniquely identify that GPU in the target system. For example,
                                    CUDA_VISIBLE_DEVICES=GPU-8932f937 may be a valid way to refer to the above GPU UUID,
                                    assuming no other GPU in the system shares this prefix.<br clear="none"></br> Only the devices whose
                                    index is present in the sequence are visible to CUDA applications and they are
                                    enumerated in the order of the sequence. If one of the indices is invalid, only the
                                    devices whose index precedes the invalid index are visible to CUDA applications. For
                                    example, setting CUDA_VISIBLE_DEVICES to 2,1 causes device 0 to be invisible and
                                    device 2 to be enumerated before device 1. Setting CUDA_VISIBLE_DEVICES to 0,2,-1,1
                                    causes devices 0 and 2 to be visible and device 1 to be invisible.<br clear="none"></br> MIG format
                                    starts with MIG keyword and GPU UUID should follow the same format as given by
                                    <dfn class="term">nvidia-smi</dfn>. For example,
                                    MIG-GPU-8932f937-d72c-4106-c12f-20bd9faed9f6/1/2. Only single MIG instance
                                    enumeration is supported.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_MANAGED_FORCE_DEVICE_ALLOC </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1 (default is 0) </td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Forces the driver to place all managed allocations in device memory.</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_DEVICE_ORDER </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST) </td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">FASTEST_FIRST causes CUDA to enumerate the available devices in fastest to
                                    slowest order using a simple heuristic. PCI_BUS_ID orders devices by PCI bus ID in
                                    ascending order.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d225e34962 d225e34965 d225e34968" rowspan="1"><strong class="ph b">Compilation</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_CACHE_DISABLE </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Disables caching (when set to 1) or enables caching (when set to 0) for
                                    just-in-time-compilation. When disabled, no binary code is added to or retrieved
                                    from the cache.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_CACHE_PATH </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">filepath </td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Specifies the folder where the just-in-time compiler caches binary codes; the
                                    default values are: 
                                    <ul class="ul">
                                       <li class="li">on Windows, <samp class="ph codeph">%APPDATA%\NVIDIA\ComputeCache</samp></li>
                                       <li class="li">on Linux, <samp class="ph codeph">~/.nv/ComputeCache</samp></li>
                                    </ul>
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_CACHE_MAXSIZE</td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">integer (default is 268435456 (256 MiB) and maximum is 4294967296 (4
                                    GiB))
                                 </td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Specifies the size in bytes of the cache used by the just-in-time compiler.
                                    Binary codes whose size exceeds the cache size are not cached. Older binary codes
                                    are evicted from the cache to make room for newer binary codes if needed. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_FORCE_PTX_JIT </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">When set to 1, forces the device driver to ignore any binary code embedded in
                                    an application (see <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a>) and to just-in-time
                                    compile embedded <dfn class="term">PTX</dfn> code instead. If a kernel does not have embedded
                                    <dfn class="term">PTX</dfn> code, it will fail to load. This environment variable can be used
                                    to validate that <dfn class="term">PTX</dfn> code is embedded in an application and that its
                                    just-in-time compilation works as expected to guarantee application forward
                                    compatibility with future architectures (see <a class="xref" href="index.html#just-in-time-compilation" shape="rect">Just-in-Time Compilation</a>).
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_DISABLE_PTX_JIT </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">When set to 1, disables the just-in-time compilation of embedded
                                    <dfn class="term">PTX</dfn> code and use the compatible binary code embedded in an
                                    application (see <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a>). If a kernel does not have embedded binary code or the embedded binary was
                                    compiled for an incompatible architecture, then it will fail to load. This
                                    environment variable can be used to validate that an application has the compatible
                                    <dfn class="term">SASS</dfn> code generated for each kernel.(see <a class="xref" href="index.html#binary-compatibility" shape="rect">Binary Compatibility</a>).
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d225e34962 d225e34965 d225e34968" rowspan="1"><strong class="ph b">Execution</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_LAUNCH_BLOCKING </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Disables (when set to 1) or enables (when set to 0) asynchronous kernel
                                    launches. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_DEVICE_MAX_CONNECTIONS </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">1 to 32 (default is 8)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Sets the number of compute and copy engine concurrent connections (work queues)
                                    from the host to each device of compute capability 3.5 and above. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_AUTO_BOOST </td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Overrides the autoboost behavior set by the --auto-boost-default option of
                                    nvidia-smi. If an application requests via this environment variable a behavior that
                                    is different from nvidia-smi's, its request is honored if there is no other
                                    application currently running on the same GPU that successfully requested a
                                    different behavior, otherwise it is ignored. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d225e34962 d225e34965 d225e34968" rowspan="1"><strong class="ph b">cuda-gdb (on Linux platform)</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_DEVICE_WAITS_ON_EXCEPTION</td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">When set to 1, a CUDA application will halt when a device exception occurs,
                                    allowing a debugger to be attached for further debugging.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d225e34962 d225e34965 d225e34968" rowspan="1"><strong class="ph b">MPS service (on Linux platform)</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d225e34962" rowspan="1" colspan="1">CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT</td>
                                 <td class="entry" valign="top" width="20%" headers="d225e34965" rowspan="1" colspan="1">Percentage value (between 0 - 100, default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d225e34968" rowspan="1" colspan="1">Devices of compute capability 8.x allow, a portion of L2 cache to be set-aside
                                    for persisting data accesses to global memory. When using CUDA MPS service, the
                                    set-aside size can only be controlled using this environment variable, before
                                    starting CUDA MPS control daemon. I.e., the environment variable should be set
                                    before running the command <samp class="ph codeph">nvidia-cuda-mps-control -d</samp>.
                                 </td>
                              </tr>
                           </tbody>
                        </table>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="um-unified-memory-programming-hd"><a name="um-unified-memory-programming-hd" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#um-unified-memory-programming-hd" name="um-unified-memory-programming-hd" shape="rect">M.&nbsp;Unified Memory Programming</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="um-introduction"><a name="um-introduction" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#um-introduction" name="um-introduction" shape="rect">M.1.&nbsp;Unified Memory Introduction</a></h3>
                     <div class="body conbody">
                        <div class="p">Unified Memory is a component of the CUDA programming model, first introduced in CUDA 6.0, that
                           defines a <dfn class="term">managed</dfn> memory space in which all processors see a single
                           coherent memory image with a common address space.
                           <div class="note note"><span class="notetitle">Note:</span> A <dfn class="term">processor</dfn> refers
                              to any independent execution unit with a dedicated MMU. This includes both CPUs and
                              GPUs of any type and architecture.
                           </div>
                           The underlying system manages data access
                           and locality within a CUDA program without need for explicit memory copy calls. This
                           benefits GPU programming in two primary ways:<a name="um-introduction__ul_yhd_kmp_vm" shape="rect">
                              <!-- --></a><ul class="ul" id="um-introduction__ul_yhd_kmp_vm">
                              <li class="li">GPU programming is simplified by unifying memory spaces coherently across all
                                 GPUs and CPUs in the system and by providing tighter and more straightforward
                                 language integration for CUDA programmers.
                              </li>
                              <li class="li">Data access speed is maximized by transparently migrating data towards the
                                 processor using it.
                              </li>
                           </ul>
                        </div>
                        <p class="p">In simple terms, Unified Memory eliminates the need for explicit data movement via the
                           <samp class="ph codeph">cudaMemcpy*()</samp> routines without the performance penalty incurred by
                           placing all data into zero-copy memory. Data movement, of course, still takes place, so
                           a programs run time typically does not decrease; Unified Memory instead enables the
                           writing of simpler and more maintainable code.
                        </p>
                        <p class="p">Unified Memory offers a single-pointer-to-data model that is conceptually similar to CUDAs
                           zero-copy memory. One key difference between the two is that with zero-copy allocations
                           the physical location of memory is pinned in CPU system memory such that a program may
                           have fast or slow access to it depending on where it is being accessed from. Unified
                           Memory, on the other hand, decouples memory and execution spaces so that all data
                           accesses are fast.
                        </p>
                        <p class="p">The term <em class="ph i">Unified Memory</em> describes a system that provides memory management
                           services to a wide range of programs, from those targeting the Runtime API down to those
                           using the Virtual ISA (PTX). Part of this system defines the managed memory space that
                           opts in to Unified Memory services.
                        </p>
                        <p class="p">Managed memory is interoperable and interchangeable with device-specific allocations, such as
                           those created using the <samp class="ph codeph">cudaMalloc()</samp> routine. All CUDA operations that
                           are valid on device memory are also valid on managed memory; the primary difference is
                           that the host portion of a program is able to reference and access the memory as
                           well.
                        </p>
                        <div class="note note"><span class="notetitle">Note:</span> 
                           Unified memory is not supported on discrete GPUs attached to Tegra.
                           
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-requirements"><a name="um-requirements" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-requirements" name="um-requirements" shape="rect">M.1.1.&nbsp;System Requirements</a></h3>
                        <div class="body conbody">
                           <div class="p">Unified Memory has two basic requirements:<a name="um-requirements__ul_apk_ktt_vm" shape="rect">
                                 <!-- --></a><ul class="ul" id="um-requirements__ul_apk_ktt_vm">
                                 <li class="li">a GPU with SM architecture 3.0 or higher (Kepler class or newer)</li>
                                 <li class="li">a 64-bit host application and non-embedded operating system (Linux or Windows)</li>
                              </ul>
                           </div>
                           <p class="p">GPUs with SM architecture 6.x or higher (Pascal class or newer) provide additional Unified Memory 
                              features such as on-demand page migration and GPU memory oversubscription that are outlined throughout 
                              this document. Note that currently these features are <em class="ph i">only</em> supported on Linux operating systems. 
                              Applications running on Windows (whether in TCC or WDDM mode) will use the basic Unified Memory 
                              model as on pre-6.x architectures even when they are running on hardware with compute capability 6.x or 
                              higher. See <a class="xref" href="index.html#um-data-migration" shape="rect">Data Migration and Coherency</a> for details.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-simplifying"><a name="um-simplifying" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-simplifying" name="um-simplifying" shape="rect">M.1.2.&nbsp;Simplifying GPU Programming</a></h3>
                        <div class="body conbody">
                           <p class="p">Unification of memory spaces means that there is no longer any need for explicit memory
                              transfers between host and device. Any allocation created in the managed memory space is
                              automatically migrated to where it is needed.
                           </p>
                           <div class="p">A program allocates managed memory in one of two ways: via the
                              <samp class="ph codeph">cudaMallocManaged()</samp> routine, which is semantically similar to
                              <samp class="ph codeph">cudaMalloc()</samp>; or by defining a global <samp class="ph codeph">__managed__</samp>
                              variable, which is semantically similar to a <samp class="ph codeph">__device__</samp> variable.
                              Precise definitions of these are found later in this document.
                              
                              <div class="note note"><span class="notetitle">Note:</span> On supporting platforms with devices of compute capability 6.x and higher, Unified Memory 
                                 will enable applications to allocate and share data using the default system allocator. This 
                                 allows the GPU to access the entire system virtual memory without using a special allocator. 
                                 See <a class="xref" href="index.html#um-system-allocator" shape="rect">System Allocator</a> for more detail. 
                              </div>
                           </div>
                           <div class="p">The following code examples illustrate how the use of managed memory can change the way in
                              which host code is written. First, a simple program written without the benefit of
                              Unified Memory:
                              <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> AplusB(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> b) {
    ret[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = a + b + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret;
    cudaMalloc(&amp;ret, 1000 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>));
    AplusB<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1000 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(ret, 10, 100);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *host_ret = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *)malloc(1000 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>));
    cudaMemcpy(host_ret, ret, 1000 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>), cudaMemcpyDefault);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 1000; i++)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"%d: A+B = %d\n"</span>, i, host_ret[i]); 
    free(host_ret);
    cudaFree(ret); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           <p class="p">This first example combines two numbers together on the GPU with a per-thread ID and
                              returns the values in an array. Without managed memory, both host- and device-side
                              storage for the return values is required (<samp class="ph codeph">host_ret</samp> and
                              <samp class="ph codeph">ret</samp> in the example), as is an explicit copy between the two using
                              <samp class="ph codeph">cudaMemcpy()</samp>.
                           </p>
                           <div class="p">Compare this with the Unified Memory version of the program, which allows direct access of GPU
                              data from the host. Notice the <samp class="ph codeph">cudaMallocManaged()</samp> routine, which
                              returns a pointer valid from both host and device code. This allows <samp class="ph codeph">ret</samp>
                              to be used without a separate <samp class="ph codeph">host_ret</samp> copy, greatly simplifying and
                              reducing the size of the program. <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> AplusB(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> b) {
    ret[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = a + b + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret;
    cudaMallocManaged(&amp;ret, 1000 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>));
    AplusB<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1000 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(ret, 10, 100);
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 1000; i++)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"%d: A+B = %d\n"</span>, i, ret[i]);
    cudaFree(ret); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           <div class="p">Finally, language integration allows direct reference of a GPU-declared
                              <samp class="ph codeph">__managed__</samp> variable and simplifies a program further when global
                              variables are used.<pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> ret[1000];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> AplusB(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> b) {
    ret[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = a + b + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    AplusB<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1000 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(10, 100);
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 1000; i++)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"%d: A+B = %d\n"</span>, i, ret[i]);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre></div>
                           <p class="p">Note the absence of explicit <samp class="ph codeph">cudaMemcpy()</samp> commands and the fact that the
                              return array <samp class="ph codeph">ret</samp> is visible on both CPU and GPU.
                           </p>
                           <p class="p">It is worth a comment on the synchronization between host and device. Notice how in the
                              non-managed example, the synchronous <samp class="ph codeph">cudaMemcpy()</samp> routine is used both
                              to synchronize the kernel (that is, to wait for it to finish running), and to transfer
                              the data to the host. The Unified Memory examples do not call
                              <samp class="ph codeph">cudaMemcpy()</samp> and so require an explicit
                              <samp class="ph codeph">cudaDeviceSynchronize()</samp> before the host program can safely use the
                              output from the GPU.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-data-migration"><a name="um-data-migration" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-data-migration" name="um-data-migration" shape="rect">M.1.3.&nbsp;Data Migration and Coherency</a></h3>
                        <div class="body conbody">
                           <p class="p">Unified Memory attempts to optimize memory performance by migrating data towards the device
                              where it is being accessed (that is, moving data to host memory if the CPU is accessing
                              it and to device memory if the GPU will access it). Data migration is fundamental to
                              Unified Memory, but is transparent to a program. The system will try to place data in
                              the location where it can most efficiently be accessed without violating coherency.
                           </p>
                           <p class="p">The physical location of data is invisible to a program and may be changed at any time,
                              but accesses to the datas virtual address will remain valid and coherent from any
                              processor regardless of locality. Note that maintaining coherence is the primary
                              requirement, ahead of performance; within the constraints of the host operating system,
                              the system is permitted to either fail accesses or move data in order to maintain global
                              coherence between processors.
                           </p>
                           <p class="p">GPU architectures of compute capability lower than 6.x do not support fine-grained movement 
                              of the managed data to GPU on-demand. Whenever a GPU kernel is launched all managed memory 
                              generally has to be transfered to GPU memory to avoid faulting on memory access. With compute 
                              capability 6.x a new GPU page faulting mechanism is introduced that provides more seamless 
                              Unified Memory functionality. Combined with the system-wide virtual address space, page faulting 
                              provides several benefits. First, page faulting means that the CUDA system software doesnt need 
                              to synchronize all managed memory allocations to the GPU before each kernel launch. If a kernel 
                              running on the GPU accesses a page that is not resident in its memory, it faults, allowing the 
                              page to be automatically migrated to the GPU memory on-demand. Alternatively, the page may be 
                              mapped into the GPU address space for access over the PCIe or NVLink interconnects (mapping on 
                              access can sometimes be faster than migration). Note that Unified Memory is system-wide: GPUs 
                              (and CPUs) can fault on and migrate memory pages either from CPU memory or from the memory of 
                              other GPUs in the system.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-oversubscription"><a name="um-oversubscription" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-oversubscription" name="um-oversubscription" shape="rect">M.1.4.&nbsp;GPU Memory Oversubscription</a></h3>
                        <div class="body conbody">
                           <p class="p">Devices of compute capability lower than 6.x cannot allocate more managed memory than the 
                              physical size of GPU memory.
                           </p>
                           <p class="p">Devices of compute capability 6.x extend addressing mode to support 49-bit virtual addressing. 
                              This is large enough to cover the 48-bit virtual address spaces of modern CPUs, as well as the 
                              GPUs own memory. The large virtual address space and page faulting capability enable applications 
                              to access the entire system virtual memory, not limited by the physical memory size of any one 
                              processor. This means that applications can oversubscribe the memory system: in other words they 
                              can allocate, access, and share arrays larger than the total physical capacity of the system, 
                              enabling out-of-core processing of very large datasets. <samp class="ph codeph">cudaMallocManaged</samp> will 
                              not run out of memory as long as there is enough system memory available for the allocation.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-multi-gpu"><a name="um-multi-gpu" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-multi-gpu" name="um-multi-gpu" shape="rect">M.1.5.&nbsp;Multi-GPU</a></h3>
                        <div class="body conbody">
                           <p class="p">For devices of compute capability lower than 6.x managed memory allocation behaves identically 
                              to unmanaged memory allocated using <samp class="ph codeph">cudaMalloc()</samp>: the current active device is 
                              the home for the physical allocation, and all other GPUs receive peer mappings to the memory. 
                              This means that other GPUs in the system will access the memory at reduced bandwidth over the PCIe
                              bus. Note that if peer mappings are not supported between the GPUs in the system, then the managed
                              memory pages are placed in CPU system memory (zero-copy memory), and all GPUs will
                              experience PCIe bandwidth restrictions. See <a class="xref" href="index.html#um-managed-memory" shape="rect">Managed Memory with Multi-GPU Programs on pre-6.x Architectures</a> for details.
                           </p>
                           <p class="p">Managed allocations on systems with devices of compute capability 6.x are visible to all GPUs and 
                              can migrate to any processor on-demand. Unified Memory performance hints (see 
                              <a class="xref" href="index.html#um-performance-tuning" shape="rect">Performance Tuning</a>) allow developers to explore custom usage patterns, such as 
                              read duplication of data across GPUs and direct access to peer GPU memory without migration. 
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-system-allocator"><a name="um-system-allocator" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-system-allocator" name="um-system-allocator" shape="rect">M.1.6.&nbsp;System Allocator</a></h3>
                        <div class="body conbody">
                           <p class="p">Devices of compute capability 7.0 support Address Translation Services (ATS) over NVLink. If
                              supported by the host CPU and operating system, ATS allows the GPU to directly access the CPUs
                              page tables. A miss in the GPU MMU will result in an Address Translation Request (ATR) to the
                              CPU. The CPU looks in its page tables for the virtual-to-physical mapping for that address and 
                              supplies the translation back to the GPU. ATS provides the GPU full access to system memory, such
                              as memory allocated with <samp class="ph codeph">malloc</samp>, memory allocated on stack, global variables and
                              file-backed memory. An application can query whether the device supports coherently accessing
                              pageable memory via ATS by checking the new <samp class="ph codeph">pageableMemoryAccessUsesHostPageTables</samp> property.
                           </p>
                           <div class="p">Here is an example code that works on any system that satisfies the basic requirements for Unified 
                              Memory (see <a class="xref" href="index.html#um-requirements" shape="rect">System Requirements</a>):
                              <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data;
cudaMallocManaged(&amp;data, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * n);
kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid, block<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data);</pre></div>
                           <div class="p">These new access patterns are supported on systems with <samp class="ph codeph">pageableMemoryAccess</samp>
                              property:
                              <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>*)malloc(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>) * n);
kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid, block<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data);
</pre><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> data[1024];
kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid, block<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data);
</pre><pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">extern</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data;
kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>grid, block<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data);
</pre></div>
                           <p class="p">In the example above, <samp class="ph codeph">data</samp> could be initialized by a third party CPU library, and then 
                              directly accessed by the GPU kernel. On systems with <samp class="ph codeph">pageableMemoryAccess</samp>, users may also
                              prefetch pageable memory to the GPU by using <samp class="ph codeph">cudaMemPrefetchAsync</samp>. This could yield performance 
                              benefits through optimized data locality.
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span> ATS over NVLink is currently supported only on IBM Power9 systems.
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-hw-coherency"><a name="um-hw-coherency" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-hw-coherency" name="um-hw-coherency" shape="rect">M.1.7.&nbsp;Hardware Coherency</a></h3>
                        <div class="body conbody">
                           <p class="p">The second generation of NVLink allows direct load/store/atomic access from 
                              the CPU to each GPUs memory. Coupled with a new CPU mastering capability, NVLink supports coherency
                              operations allowing data reads from GPU memory to be stored in the CPUs cache hierarchy. The lower 
                              latency of access from the CPUs cache is key for CPU performance. Devices of compute capability 6.x
                              support only peer GPU atomics. Devices of compute capability 7.x can send GPU atomics across NVLink 
                              and have them completed at the target CPU, thus the second generation of NVLink adds support for 
                              atomics initiated by either the GPU or the CPU. 
                           </p>
                           <p class="p">Note that <samp class="ph codeph">cudaMalloc</samp> allocations are not accessible from the CPU. Therefore, to take 
                              advantage of hardware coherency users must use Unified Memory allocators such as 
                              <samp class="ph codeph">cudaMallocManaged</samp> or system allocator with ATS support 
                              (see <a class="xref" href="index.html#um-system-allocator" shape="rect">System Allocator</a>). The new property <samp class="ph codeph">directManagedMemAccessFromHost</samp> 
                              indicates if the host can directly access managed memory on the device without migration. By default, 
                              any CPU access of <samp class="ph codeph">cudaMallocManaged</samp> allocations resident in GPU memory will trigger 
                              page faults and data migration. Applications can use <samp class="ph codeph">cudaMemAdviseSetAccessedBy</samp> 
                              performance hint with <samp class="ph codeph">cudaCpuDeviceId</samp> to enable direct access of GPU memory on 
                              supported systems. 
                           </p>
                           <div class="p">Consider an example code below:
                              <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> write(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> b) {
    ret[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] = a + b + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> append(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> a, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> b) {
    ret[<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x] += a + b + <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">threadIdx</span>.x;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *ret;
    cudaMallocManaged(&amp;ret, 1000 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>));
    cudaMemAdvise(ret, 1000 * <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">sizeof</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>), cudaMemAdviseSetAccessedBy, cudaCpuDeviceId);  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// set direct access hint</span>

    write<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1000 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(ret, 10, 100);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// pages populated in GPU memory</span>
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i = 0; i &lt; 1000; i++)
        printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"%d: A+B = %d\n"</span>, i, ret[i]);        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// directManagedMemAccessFromHost=1: CPU accesses GPU memory directly without migrations</span>
                                                    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// directManagedMemAccessFromHost=0: CPU faults and triggers device-to-host migrations</span>
    append<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1000 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(ret, 10, 100);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations</span>
    cudaDeviceSynchronize();                        <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations</span>
    cudaFree(ret); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span> 0;
}</pre>
                              After <samp class="ph codeph">write</samp> kernel is completed, <samp class="ph codeph">ret</samp> will be created and initialized in GPU memory. 
                              Next, the CPU will access <samp class="ph codeph">ret</samp> followed by <samp class="ph codeph">append</samp> kernel using the same 
                              <samp class="ph codeph">ret</samp> memory again. This code will show different behavior depending on the system 
                              architecture and support of hardware coherency:
                              
                              <ul class="ul">
                                 <li class="li">On systems with <samp class="ph codeph">directManagedMemAccessFromHost=1</samp>: CPU accesses to the managed buffer will not trigger any migrations; the data will remain resident in GPU memory and any
                                    subsequent GPU kernels can continue to access it directly without inflicting faults or migrations. 
                                 </li>
                                 <li class="li">On systems with <samp class="ph codeph">directManagedMemAccessFromHost=0</samp>: CPU accesses to the managed buffer will page fault and initiate data migration; any GPU kernel trying to access the same
                                    data first time will page fault and migrate pages back to GPU memory. 
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-access-counters"><a name="um-access-counters" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-access-counters" name="um-access-counters" shape="rect">M.1.8.&nbsp;Access Counters</a></h3>
                        <div class="body conbody">
                           <p class="p">Devices of compute capability 7.0 introduce a new Access Counter feature that keeps track of 
                              the frequency of access that a GPU makes to memory located on other processors. Access 
                              Counters help ensure memory pages are moved to the physical memory of the processor that is 
                              accessing the pages most frequently. The Access Counters feature can guide migrations between 
                              CPU and GPU, and between peer GPUs. 
                           </p>
                           <p class="p">For <samp class="ph codeph">cudaMallocManaged</samp>, Access Counters migration can be opt-in by using 
                              <samp class="ph codeph">cudaMemAdviseSetAccessedBy</samp> hint with the corresponding device id. 
                              The driver may also use Access Counters for more efficient thrashing mitigation or memory 
                              oversubscription scenarios. 
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span> Access Counters are currently enabled only on IBM Power9 systems and only for the
                              <samp class="ph codeph">cudaMallocManaged</samp> allocator.
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="um-programming-model-hd"><a name="um-programming-model-hd" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#um-programming-model-hd" name="um-programming-model-hd" shape="rect">M.2.&nbsp;Programming Model</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-opt-in"><a name="um-opt-in" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-opt-in" name="um-opt-in" shape="rect">M.2.1.&nbsp;Managed Memory Opt In</a></h3>
                        <div class="body conbody">
                           <p class="p">Most platforms require a program to opt in to automatic data management by either annotating a
                              <samp class="ph codeph">__device__</samp> variable with the <samp class="ph codeph">__managed__</samp> keyword
                              (see the <a class="xref" href="index.html#um-language-integration" shape="rect">Language Integration</a> section) or by
                              using a new <samp class="ph codeph">cudaMallocManaged()</samp> call to allocate data.
                           </p>
                           <p class="p">Devices of compute capability lower than 6.x must always allocate managed memory on the heap, either with an allocator or
                              by
                              declaring global storage. It is not possible either to associate previously allocated
                              memory with Unified Memory, or to have the Unified Memory system manage a CPU or a GPU
                              stack pointer.
                           </p>
                           <p class="p">Starting with CUDA 8.0 and on supporting systems with devices of compute capability 6.x, memory allocated with the default
                              OS allocator (e.g. <samp class="ph codeph">malloc</samp> or <samp class="ph codeph">new</samp>) can be accessed from both GPU code and CPU code using the same pointer. On these systems, Unified Memory is the default:
                              there is no need to use a special allocator or the creation of a specially managed memory pool.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-explicit-allocation"><a name="um-explicit-allocation" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-explicit-allocation" name="um-explicit-allocation" shape="rect">M.2.1.1.&nbsp;Explicit Allocation Using <samp class="ph codeph">cudaMallocManaged()</samp></a></h3>
                           <div class="body conbody">
                              <div class="p">Unified memory is most commonly created using an allocation function that is semantically
                                 and syntactically similar to the standard CUDA allocator, <samp class="ph codeph">cudaMalloc()</samp>.
                                 The function description is as
                                 follows:<pre xml:space="preserve">    cudaError_t cudaMallocManaged(void **devPtr,
                                  size_t size,
                                  unsigned int flags=0);</pre></div>
                              <div class="p">The <samp class="ph codeph">cudaMallocManaged()</samp> function reserves <samp class="ph codeph">size</samp> bytes of
                                 managed memory and returns a pointer in <samp class="ph codeph">devPtr</samp>. Note the difference in 
                                 <samp class="ph codeph">cudaMallocManaged()</samp> behavior between various GPU architectures. By default, 
                                 the devices of compute capability lower than 6.x allocate managed memory directly on the GPU. 
                                 However, the devices of compute capability 6.x and greater do not allocate physical memory 
                                 when calling <samp class="ph codeph">cudaMallocManaged()</samp>: in this case physical memory is populated 
                                 on first touch and may be resident on the CPU or the GPU. The managed pointer is valid on all 
                                 GPUs and the CPU in the system, although program accesses to this pointer must obey the 
                                 concurrency rules of the Unified Memory programming model (see <a class="xref" href="index.html#um-coherency-hd" shape="rect">Coherency and Concurrency</a>). 
                                 Below is a simple example, showing the use of <samp class="ph codeph">cudaMallocManaged()</samp>: 
                                 <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> printme(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *str) {
    printf(str);
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate 100 bytes of memory, accessible to both Host and Device code</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *s;
    cudaMallocManaged(&amp;s, 100);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note direct Host-code use of "s"</span>
    strncpy(s, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"Hello Unified Memory\n"</span>, 99);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Here we pass "s" to a kernel without explicitly copying</span>
    printme<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(s);
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Free as for normal CUDA allocations</span>
    cudaFree(s); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                              <p class="p">A programs behavior is functionally unchanged when <samp class="ph codeph">cudaMalloc()</samp> is
                                 replaced with <samp class="ph codeph">cudaMallocManaged(</samp>); however, the program should go on to
                                 eliminate explicit memory copies and take advantage of automatic migration.
                                 Additionally, dual pointers (one to host and one to device memory) can be
                                 eliminated.
                              </p>
                              <p class="p">Device code is not able to call <samp class="ph codeph">cudaMallocManaged()</samp>. All
                                 managed memory must be allocated from the host or at global scope (see the next
                                 section). Allocations on the device heap using <samp class="ph codeph">malloc()</samp> in a kernel
                                 will not be created in the managed memory space, and so will not be accessible to CPU
                                 code.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-global-scope"><a name="um-global-scope" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-global-scope" name="um-global-scope" shape="rect">M.2.1.2.&nbsp;Global-Scope Managed Variables Using <samp class="ph codeph">__managed__</samp></a></h3>
                           <div class="body conbody">
                              <div class="p">File-scope and global-scope CUDA <samp class="ph codeph">__device__ </samp>variables may also opt-in to
                                 Unified Memory management by adding a new <samp class="ph codeph">__managed__</samp> annotation to the
                                 declaration. These may then be referenced directly from either host or device code, as
                                 follows: 
                                 <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x[2];
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> y;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> kernel() {
    x[1] = x[0] + y;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    x[0] = 3;
    y = 5;
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();
    printf(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"result = %d\n"</span>, x[1]); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                              <p class="p">All semantics of the original <samp class="ph codeph">__device__</samp> memory space, along with some
                                 additional unified-memory-specific constraints, are inherited by the managed variable
                                 (see <a class="xref" href="index.html#compilation-with-nvcc" shape="rect">Compilation with NVCC</a>).
                              </p>
                              <p class="p">Note that variables marked <samp class="ph codeph">__constant__</samp> may not also be marked as
                                 <samp class="ph codeph">__managed__</samp>; this annotation is reserved for
                                 <samp class="ph codeph">__device__</samp> variables only. Constant memory must be set either
                                 statically at compile time or by using <samp class="ph codeph">cudaMemcpyToSymbol()</samp> as usual in
                                 CUDA.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-coherency-hd"><a name="um-coherency-hd" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-coherency-hd" name="um-coherency-hd" shape="rect">M.2.2.&nbsp;Coherency and Concurrency</a></h3>
                        <div class="body conbody">
                           <p class="p">Simultaneous access to managed memory on devices of compute capability lower than 6.x is not possible, because coherence could
                              not be guaranteed if the CPU accessed a Unified Memory allocation while a GPU kernel was active. However, devices of compute
                              capability 6.x on supporting operating systems allow the CPUs and GPUs to access Unified Memory allocations simultaneously
                              via the new page faulting mechanism. A program can query whether a device supports concurrent access to managed memory by
                              checking a new <samp class="ph codeph">concurrentManagedAccess</samp> property. Note, as with any parallel application, developers need to ensure correct synchronization to avoid data hazards
                              between processors.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-gpu-exclusive"><a name="um-gpu-exclusive" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-gpu-exclusive" name="um-gpu-exclusive" shape="rect">M.2.2.1.&nbsp;GPU Exclusive Access To Managed Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">To ensure coherency on pre-6.x GPU architectures, the Unified Memory programming model puts constraints on data accesses
                                 while both the CPU and GPU are executing concurrently. In effect, the GPU has exclusive
                                 access to all managed data while any kernel operation is executing, regardless of
                                 whether the specific kernel is actively using the data. When managed data is used with
                                 <samp class="ph codeph">cudaMemcpy*()</samp> or <samp class="ph codeph">cudaMemset*()</samp>, the system may
                                 choose to access the source or destination from the host or the device, which will put
                                 constraints on concurrent CPU access to that data while the
                                 <samp class="ph codeph">cudaMemcpy*()</samp> or <samp class="ph codeph">cudaMemset*()</samp> is executing. See
                                 <a class="xref" href="index.html#um-memcpy-memset" shape="rect">Memcpy()/Memset() Behavior With Managed Memory</a> for further details.
                              </p>
                              <div class="p">It is not permitted for the CPU to access any managed allocations or variables
                                 while the GPU is active for devices with <samp class="ph codeph">concurrentManagedAccess</samp> property set to 0. On these systems concurrent CPU/GPU accesses, even to different managed memory
                                 allocations, will cause a segmentation fault because the page is considered inaccessible
                                 to the CPU. <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, y=2;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>  kernel() {
    x = 10;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    y = 20;            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Error on GPUs not supporting concurrent access</span>
                       
    cudaDeviceSynchronize();
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                              <div class="p">In example above, the GPU program <samp class="ph codeph">kernel</samp> is still active when the CPU touches
                                 <samp class="ph codeph">y</samp>. (Note how it occurs before
                                 <samp class="ph codeph">cudaDeviceSynchronize()</samp>.) The code runs successfully on devices of compute capability 6.x due to the GPU page faulting capability which lifts all
                                 restrictions on simultaneous access. However, such memory access is invalid on pre-6.x architectures even though the
                                 CPU is accessing different data than the GPU. The program must explicitly synchronize
                                 with the GPU before accessing <samp class="ph codeph">y</samp>: <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, y=2;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>  kernel() {
    x = 10;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();
    cudaDeviceSynchronize();
    y = 20;            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//  Success on GPUs not supporing concurrent access</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                              <p class="p">As this example shows, on systems with pre-6.x GPU architectures, a CPU thread may not access any managed data in between
                                 performing
                                 a kernel launch and a subsequent synchronization call, regardless of whether the GPU
                                 kernel actually touches that same data (or any managed data at all). The mere potential
                                 for concurrent CPU and GPU access is sufficient for a process-level exception to be
                                 raised.
                              </p>
                              <p class="p">Note that if memory is dynamically allocated with <samp class="ph codeph">cudaMallocManaged()</samp> or
                                 <samp class="ph codeph">cuMemAllocManaged()</samp> while the GPU is active, the behavior of the memory
                                 is unspecified until additional work is launched or the GPU is synchronized. Attempting
                                 to access the memory on the CPU during this time may or may not cause a segmentation
                                 fault. This does not apply to memory allocated using the flag
                                 <samp class="ph codeph">cudaMemAttachHost</samp> or <samp class="ph codeph">CU_MEM_ATTACH_HOST</samp>.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-explicit-synchronization"><a name="um-explicit-synchronization" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-explicit-synchronization" name="um-explicit-synchronization" shape="rect">M.2.2.2.&nbsp;Explicit Synchronization and Logical GPU Activity</a></h3>
                           <div class="body conbody">
                              <p class="p">Note that explicit synchronization is required even if <samp class="ph codeph">kernel</samp> runs quickly and
                                 finishes before the CPU touches <samp class="ph codeph">y</samp> in the above example. Unified Memory
                                 uses logical activity to determine whether the GPU is idle. This aligns with the CUDA
                                 programming model, which specifies that a kernel can run at any time following a launch
                                 and is not guaranteed to have finished until the host issues a synchronization call.
                              </p>
                              <p class="p">Any function call that logically guarantees the GPU completes its work is valid. This
                                 includes <samp class="ph codeph">cudaDeviceSynchronize()</samp>;
                                 <samp class="ph codeph">cudaStreamSynchronize()</samp> and <samp class="ph codeph">cudaStreamQuery()</samp>
                                 (provided it returns <samp class="ph codeph">cudaSuccess</samp> and not
                                 <samp class="ph codeph">cudaErrorNotReady</samp>) where the specified stream is the only stream
                                 still executing on the GPU; <samp class="ph codeph">cudaEventSynchronize()</samp> and
                                 <samp class="ph codeph">cudaEventQuery()</samp> in cases where the specified event is not followed
                                 by any device work; as well as uses of <samp class="ph codeph">cudaMemcpy()</samp> and
                                 <samp class="ph codeph">cudaMemset()</samp> that are documented as being fully synchronous with
                                 respect to the host.
                              </p>
                              <p class="p">Dependencies created between streams will be followed to infer completion of other
                                 streams by synchronizing on a stream or event. Dependencies can be created via
                                 <samp class="ph codeph">cudaStreamWaitEvent()</samp> or implicitly when using the default (NULL)
                                 stream.
                              </p>
                              <p class="p">It is legal for the CPU to access managed data from within a stream callback, provided no
                                 other stream that could potentially be accessing managed data is active on the GPU. In
                                 addition, a callback that is not followed by any device work can be used for
                                 synchronization: for example, by signaling a condition variable from inside the
                                 callback; otherwise, CPU access is valid only for the duration of the callback(s).
                              </p>
                              <div class="p">There are several important points of note:<a name="um-explicit-synchronization__ul_axk_pxn_ym" shape="rect">
                                    <!-- --></a><ul class="ul" id="um-explicit-synchronization__ul_axk_pxn_ym">
                                    <li class="li">It is always permitted for the CPU to access non-managed zero-copy data while
                                       the GPU is active.
                                    </li>
                                    <li class="li">The GPU is considered active when it is running any kernel, even if that kernel
                                       does not make use of managed data. If a kernel might use data, then access is
                                       forbidden, unless device property <samp class="ph codeph">concurrentManagedAccess</samp> is 1.
                                    </li>
                                    <li class="li">There are no constraints on concurrent inter-GPU access of managed memory, other
                                       than those that apply to multi-GPU access of non-managed memory.
                                    </li>
                                    <li class="li">There are no constraints on concurrent GPU kernels accessing managed data.</li>
                                 </ul>
                              </div>
                              <div class="p">Note how the last point allows for races between GPU kernels, as is currently the case for
                                 non-managed GPU memory. As mentioned previously, managed memory functions identically to
                                 non-managed memory from the perspective of the GPU. The following code example
                                 illustrates these points: <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    cudaStream_t stream1, stream2;
    cudaStreamCreate(&amp;stream1);
    cudaStreamCreate(&amp;stream2);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *non_managed, *managed, *also_managed;
    cudaMallocHost(&amp;non_managed, 4);    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Non-managed, CPU-accessible memory</span>
    cudaMallocManaged(&amp;managed, 4);
    cudaMallocManaged(&amp;also_managed, 4);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Point 1: CPU can access non-managed data.</span>
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1, 0, stream1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(managed);
    *non_managed = 1;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Point 2: CPU cannot access any managed data while GPU is busy,</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">//          unless concurrentManagedAccess = 1</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note we have not yet synchronized, so "kernel" is still active.</span>
    *also_managed = 2;      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Will issue segmentation fault</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Point 3: Concurrent GPU kernels can access the same data.</span>
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1, 0, stream2 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(managed);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Point 4: Multi-GPU concurrent access is also permitted.</span>
    cudaSetDevice(1);
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(managed);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-managing-data"><a name="um-managing-data" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-managing-data" name="um-managing-data" shape="rect">M.2.2.3.&nbsp;Managing Data Visibility and Concurrent CPU + GPU Access with Streams</a></h3>
                           <div class="body conbody">
                              <p class="p">Until now it was assumed that for SM architectures before 6.x: 1) any active kernel may use any managed memory, and 2) it
                                 was
                                 invalid to use managed memory from the CPU while a kernel is active. Here we present a
                                 system for finer-grained control of managed memory designed to work on all devices supporting managed memory, including older
                                 architectures with <samp class="ph codeph">concurrentManagedAccess</samp> equal to 0.
                              </p>
                              <p class="p">The CUDA programming model provides streams as a mechanism for programs to indicate
                                 dependence and independence among kernel launches. Kernels launched into the same stream
                                 are guaranteed to execute consecutively, while kernels launched into different streams
                                 are permitted to execute concurrently. Streams describe independence between work items
                                 and hence allow potentially greater efficiency through concurrency.
                              </p>
                              <div class="p">Unified Memory builds upon the stream-independence model by allowing a CUDA program to
                                 explicitly associate managed allocations with a CUDA stream. In this way, the programmer
                                 indicates the use of data by kernels based on whether they are launched into a specified
                                 stream or not. This enables opportunities for concurrency based on program-specific data
                                 access patterns. The function to control this behaviour is:
                                 <pre xml:space="preserve">    cudaError_t cudaStreamAttachMemAsync(cudaStream_t stream,
                                         void *ptr,
                                         size_t length=0,
                                         unsigned int flags=0);</pre></div>
                              <p class="p">The <samp class="ph codeph">cudaStreamAttachMemAsync()</samp> function associates
                                 <samp class="ph codeph">length</samp> bytes of memory starting from <samp class="ph codeph">ptr</samp> with the
                                 specified <samp class="ph codeph">stream</samp>. (Currently, <samp class="ph codeph">length</samp> must always be 0
                                 to indicate that the entire region should be attached.) Because of this association, the
                                 Unified Memory system allows CPU access to this memory region so long as all operations
                                 in <samp class="ph codeph">stream</samp> have completed, regardless of whether other streams are
                                 active. In effect, this constrains exclusive ownership of the managed memory region by
                                 an active GPU to per-stream activity instead of whole-GPU activity.
                              </p>
                              <p class="p">Most importantly, if an allocation is not associated with a specific stream, it is
                                 visible to all running kernels regardless of their stream. This is the default
                                 visibility for a <samp class="ph codeph">cudaMallocManaged()</samp> allocation or a
                                 <samp class="ph codeph">__managed__</samp> variable; hence, the simple-case rule that the CPU may
                                 not touch the data while any kernel is running.
                              </p>
                              <p class="p">By associating an allocation with a specific stream, the program makes a guarantee that
                                 only kernels launched into that stream will touch that data. No error checking is
                                 performed by the Unified Memory system: it is the programmers responsibility to ensure
                                 that guarantee is honored.
                              </p>
                              <p class="p">In addition to allowing greater concurrency, the use of
                                 <samp class="ph codeph">cudaStreamAttachMemAsync()</samp> can (and typically does) enable data
                                 transfer optimizations within the Unified Memory system that may affect latencies and
                                 other overhead.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-stream-association"><a name="um-stream-association" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-stream-association" name="um-stream-association" shape="rect">M.2.2.4.&nbsp;Stream Association Examples</a></h3>
                           <div class="body conbody">
                              <div class="p">Associating data with a stream allows fine-grained control over CPU + GPU concurrency, but what
                                 data is visible to which streams must be kept in mind when using devices of compute capability lower than 6.x. Looking at
                                 the earlier
                                 synchronization example: <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, y=2;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>  kernel() {
    x = 10;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    cudaStream_t stream1;
    cudaStreamCreate(&amp;stream1);
    cudaStreamAttachMemAsync(stream1, &amp;y, 0, cudaMemAttachHost);
    cudaDeviceSynchronize();          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Wait for Host attachment to occur.</span>
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1, 0, stream1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note: Launches into stream1.</span>
    y = 20;                           <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Success  a kernel is running but y </span>
                                      <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// has been associated with no stream.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                              <p class="p">Here we explicitly associate <samp class="ph codeph">y</samp> with host accessibility, thus enabling
                                 access at all times from the CPU. (As before, note the absence of
                                 <samp class="ph codeph">cudaDeviceSynchronize()</samp> before the access.) Accesses to
                                 <samp class="ph codeph">y</samp> by the GPU running <samp class="ph codeph">kernel</samp> will now produce
                                 undefined results.
                              </p>
                              <div class="p">Note that associating a variable with a stream does not change the associating of any other variable. E.g. associating <samp class="ph codeph">x</samp> with <samp class="ph codeph">stream1</samp> does not ensure that only <samp class="ph codeph">x</samp> is accessed by kernels launched in <samp class="ph codeph">stream1</samp>, thus an error is caused by this code: <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__ <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> x, y=2;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>  kernel() {
    x = 10;
}
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> main() {
    cudaStream_t stream1;
    cudaStreamCreate(&amp;stream1);
    cudaStreamAttachMemAsync(stream1, &amp;x);<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Associate x with stream1.</span>
    cudaDeviceSynchronize();              <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Wait for x attachment to occur.</span>
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1, 0, stream1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>();     <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note: Launches into stream1.</span>
    y = 20;                               <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// ERROR: y is still associated globally </span>
                                          <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// with all streams by default</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre></div>
                              <p class="p">Note how the access to <samp class="ph codeph">y</samp> will cause an error because, even though
                                 <samp class="ph codeph">x</samp> has been associated with a stream, we have told the system
                                 nothing about who can see <samp class="ph codeph">y</samp>. The system therefore conservatively
                                 assumes that <samp class="ph codeph">kernel</samp> might access it and prevents the CPU from doing
                                 so.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-stream-attach"><a name="um-stream-attach" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-stream-attach" name="um-stream-attach" shape="rect">M.2.2.5.&nbsp;Stream Attach With Multithreaded Host Programs</a></h3>
                           <div class="body conbody">
                              <p class="p">The primary use for <samp class="ph codeph">cudaStreamAttachMemAsync()</samp> is to enable independent task
                                 parallelism using CPU threads. Typically in such a program, a CPU thread creates its own
                                 stream for all work that it generates because using CUDAs NULL stream would cause
                                 dependencies between threads.
                              </p>
                              <p class="p">The default global visibility of managed data to any GPU stream can make it difficult to
                                 avoid interactions between CPU threads in a multi-threaded program. Function
                                 <samp class="ph codeph">cudaStreamAttachMemAsync()</samp> is therefore used to associate a
                                 threads managed allocations with that threads own stream, and the association is
                                 typically not changed for the life of the thread.
                              </p>
                              <div class="p">Such a program would simply add a single call to <samp class="ph codeph">cudaStreamAttachMemAsync()</samp> to
                                 use unified memory for its data accesses: <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// This function performs some task, in its own private stream.</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> run_task(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *in, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *out, <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> length) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Create a stream for us to use.</span>
    cudaStream_t stream;
    cudaStreamCreate(&amp;stream);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate some managed data and associate with our stream.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Note the use of the host-attach flag to cudaMallocManaged();</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// we then associate the allocation with our stream so that</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// our GPU kernel launches can access it.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> *data;
    cudaMallocManaged((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> **)&amp;data, length, cudaMemAttachHost);
    cudaStreamAttachMemAsync(stream, data);
    cudaStreamSynchronize(stream);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Iterate on the data in some way, using both Host &amp; Device.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span>(<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> i=0; i&lt;N; i++) {
        transform<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 100, 256, 0, stream <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(in, data, length);
        cudaStreamSynchronize(stream);
        host_process(data, length);    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// CPU uses managed data.</span>
        convert<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 100, 256, 0, stream <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(out, data, length);
    }
    cudaStreamSynchronize(stream);
    cudaStreamDestroy(stream);
    cudaFree(data);
}</pre></div>
                              <p class="p">In this example, the allocation-stream association is established just once, and then
                                 <samp class="ph codeph">data</samp> is used repeatedly by both the host and device. The result is
                                 much simpler code than occurs with explicitly copying data between host and device,
                                 although the result is the same.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-advanced-modular"><a name="um-advanced-modular" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-advanced-modular" name="um-advanced-modular" shape="rect">M.2.2.6.&nbsp;Advanced Topic: Modular Programs and Data Access Constraints</a></h3>
                           <div class="body conbody">
                              <p class="p">In the previous example <samp class="ph codeph">cudaMallocManaged()</samp> specifies the
                                 <samp class="ph codeph">cudaMemAttachHost</samp> flag, which creates an allocation that is
                                 initially invisible to device-side execution. (The default allocation would be visible
                                 to all GPU kernels on all streams.) This ensures that there is no accidental interaction
                                 with another threads execution in the interval between the data allocation and when the
                                 data is acquired for a specific stream.
                              </p>
                              <div class="p">Without this flag, a new allocation would be considered in-use on the GPU if a kernel
                                 launched by another thread happens to be running. This might impact the threads ability
                                 to access the newly allocated data from the CPU (for example, within a base-class
                                 constructor) before it is able to explicitly attach it to a private stream. To enable
                                 safe independence between threads, therefore, allocations should be made specifying this
                                 flag.
                                 <div class="note note"><span class="notetitle">Note:</span> An alternative would be to place a process-wide barrier across all
                                    threads after the allocation has been attached to the stream. This would ensure that
                                    all threads complete their data/stream associations before any kernels are launched,
                                    avoiding the hazard. A second barrier would be needed before the stream is destroyed
                                    because stream destruction causes allocations to revert to their default visibility.
                                    The <samp class="ph codeph">cudaMemAttachHost</samp> flag exists both to simplify this process,
                                    and because it is not always possible to insert global barriers where
                                    required.
                                 </div>
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-memcpy-memset"><a name="um-memcpy-memset" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-memcpy-memset" name="um-memcpy-memset" shape="rect">M.2.2.7.&nbsp;Memcpy()/Memset() Behavior With Managed Memory </a></h3>
                           <div class="body conbody">
                              <p class="p">Since managed memory can be accessed from either the host or the device,
                                 <samp class="ph codeph">cudaMemcpy*()</samp> relies on the type of transfer, specified using
                                 <samp class="ph codeph">cudaMemcpyKind</samp>, to determine whether the data should be accessed as
                                 a host pointer or a device pointer.
                              </p>
                              <p class="p">If <samp class="ph codeph">cudaMemcpyHostTo*</samp> is specified and the source data is managed, then it will accessed from the host if it is coherently accessible from the
                                 host in the copy stream (1); otherwise it will be accessed from the device. Similar rules apply to the destination when <samp class="ph codeph">cudaMemcpy*ToHost</samp> is specified and the destination is managed memory. 
                              </p>
                              <p class="p">If <samp class="ph codeph">cudaMemcpyDeviceTo*</samp> is specified and the source data is managed, then it will be accessed from the device. The source must be coherently accessible
                                 from the device in the copy stream (2); otherwise, an error is returned. Similar rules apply to the destination when <samp class="ph codeph">cudaMemcpy*ToDevice</samp> is specified and the destination is managed memory.
                              </p>
                              <p class="p">If <samp class="ph codeph">cudaMemcpyDefault</samp> is specified, then managed data will be accessed from the host either if it cannot be coherently accessed from the device
                                 in the copy stream (2) or if the preferred location for the data is <samp class="ph codeph">cudaCpuDeviceId</samp> and it can be coherently accessed from the host in the copy stream (1); otherwise, it will be accessed from the device.
                              </p>
                              <p class="p">When using <samp class="ph codeph">cudaMemset*()</samp> with managed memory, the data is always
                                 accessed from the device. The data must be coherently accessible from the device in the stream being used for the <samp class="ph codeph">cudaMemset()</samp> operation (2); otherwise, an error is returned.
                              </p>
                              <p class="p">When data is accessed from the device either by <samp class="ph codeph">cudaMemcpy*</samp> or <samp class="ph codeph">cudaMemset*</samp>, the stream of operation is considered to be active on the GPU. During this time, any CPU access of data that is associated
                                 with that stream or data that has global visibility, will result in a segmentation fault if the GPU has a zero value for the
                                 device attribute <samp class="ph codeph">concurrentManagedAccess</samp>. The program must synchronize appropriately to ensure the operation has completed before accessing any associated data from
                                 the CPU.
                              </p>
                              <div class="p">(1) For managed memory to be coherently accessible from the host in a given stream, at least one of the following conditions
                                 must be satisfied:
                                 
                                 <ul class="ul">
                                    <li class="li">The given stream is associated with a device that has a non-zero value for the device attribute <samp class="ph codeph">concurrentManagedAccess</samp>.
                                    </li>
                                    <li class="li">The memory neither has global visibility nor is it associated with the given stream.</li>
                                 </ul>
                              </div>
                              <div class="p">(2) For managed memory to be coherently accessible from the device in a given stream, at least one of the following conditions
                                 must be satisfied:
                                 
                                 <ul class="ul">
                                    <li class="li">The device has a non-zero value for the device attribute <samp class="ph codeph">concurrentManagedAccess</samp>.
                                    </li>
                                    <li class="li">The memory either has global visibility or is associated with the given stream.</li>
                                 </ul>
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-language-integration"><a name="um-language-integration" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-language-integration" name="um-language-integration" shape="rect">M.2.3.&nbsp;Language Integration</a></h3>
                        <div class="body conbody">
                           <p class="p">Users of the CUDA Runtime API who compile their host code using <samp class="ph codeph">nvcc</samp>
                              have access to additional language integration features, such as shared symbol names and
                              inline kernel launch via the <samp class="ph codeph">&lt;&lt;&lt;...&gt;&gt;&gt;</samp> operator. Unified
                              Memory adds one additional element to CUDAs language integration: variables annotated
                              with the <samp class="ph codeph">__managed__</samp> keyword can be referenced directly from both host
                              and device code.
                           </p>
                           <div class="p">The following example, seen earlier in <a class="xref" href="index.html#um-simplifying" shape="rect">Simplifying GPU Programming</a>, illustrates a simple use of <samp class="ph codeph">__managed__</samp>
                              global declarations: 
                              <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Managed variable declaration is an extra annotation with __device__</span>
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__device__</span> __managed__  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>  x;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">__global__</span>  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span>  kernel() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Reference "x" directly - it's a normal variable on the GPU.</span>
    printf( <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-string">"GPU sees: x = %d\n"</span> , x);
} 
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span>  main() {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set "x" from Host code. Note it's just a normal variable on the CPU.</span>
    x = 1234;
 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Launch a kernel which uses "x" from the GPU.</span>
    kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span> 1, 1 <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(); 
    cudaDeviceSynchronize(); 
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">return</span>  0;
}</pre>The capability available with <samp class="ph codeph">__managed__</samp> variables is that
                              the symbol is available in both device code and in host code without the need to
                              dereference a pointer, and the data is shared by all. This makes it particularly easy to
                              exchange data between host and device programs without the need for explicit allocations
                              or copying.
                           </div>
                           <p class="p">Semantically, the behavior of <samp class="ph codeph">__managed__</samp> variables is identical to that of
                              storage allocated via <samp class="ph codeph">cudaMallocManaged()</samp>. See <a class="xref" href="index.html#um-explicit-allocation" shape="rect">Explicit Allocation Using cudaMallocManaged()</a> 
                              for detailed explanation. Stream visibility defaults to <samp class="ph codeph">cudaMemAttachGlobal</samp>, 
                              but may be constrained using <samp class="ph codeph">cudaStreamAttachMemAsync()</samp>.
                           </p>
                           <p class="p">A valid CUDA context is necessary for the correct operation of <samp class="ph codeph">__managed__</samp>
                              variables. Accessing <samp class="ph codeph">__managed__</samp> variables can trigger CUDA context
                              creation if a context for the current device hasnt already been created. In the example
                              above, accessing <samp class="ph codeph">x</samp> before the kernel launch triggers context creation
                              on device 0. In the absence of that access, the kernel launch would have triggered
                              context creation.
                           </p>
                           <p class="p">C++ objects declared as <samp class="ph codeph">__managed__</samp> are subject to certain specific
                              constraints, particularly where static initializers are concerned. Please refer to <a class="xref" href="index.html#c-cplusplus-language-support" shape="rect">C++ Language Support</a> in the <cite class="cite">CUDA C++ Programming Guide</cite>
                              for a list of these constraints.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-host-program-errors"><a name="um-host-program-errors" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-host-program-errors" name="um-host-program-errors" shape="rect">M.2.3.1.&nbsp;Host Program Errors with <samp class="ph codeph">__managed__</samp> Variables</a></h3>
                           <div class="body conbody">
                              <p class="p">The use of <samp class="ph codeph">__managed__</samp> variables depends upon the underlying Unified Memory
                                 system functioning correctly. Incorrect functioning can occur if, for example, the CUDA
                                 installation failed or if the CUDA context creation was unsuccessful.
                              </p>
                              <p class="p">When CUDA-specific operations fail, typically an error is returned that indicates the
                                 source of the failure. Using  <samp class="ph codeph">__managed__</samp> variables introduces a new
                                 failure mode whereby a non-CUDA operation (for example, CPU access to what should be a
                                 valid host memory address) can fail if the Unified Memory system is not operating
                                 correctly. Such invalid memory accesses cannot easily be attributed to the underlying
                                 CUDA subsystem, although a debugger such as <samp class="ph codeph">cuda-gdb</samp> will indicate that
                                 a managed memory address is the source of the failure.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-querying-um-hd"><a name="um-querying-um-hd" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-querying-um-hd" name="um-querying-um-hd" shape="rect">M.2.4.&nbsp;Querying Unified Memory Support</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-device-properties"><a name="um-device-properties" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-device-properties" name="um-device-properties" shape="rect">M.2.4.1.&nbsp;Device Properties</a></h3>
                           <div class="body conbody">
                              <p class="p">Unified Memory is supported only on devices with compute capability 3.0 or higher. A program
                                 may query whether a GPU device supports managed memory by using
                                 <samp class="ph codeph">cudaGetDeviceProperties()</samp> and checking the new
                                 <samp class="ph codeph">managedMemory</samp> property. The capability can also be determined
                                 using the individual attribute query function <samp class="ph codeph">cudaDeviceGetAttribute()</samp>
                                 with the attribute <samp class="ph codeph">cudaDevAttrManagedMemory</samp>. 
                              </p>
                              <p class="p">Either property will be set to 1 if managed memory allocations are permitted on the GPU
                                 and under the current operating system. Note that Unified Memory is not supported for
                                 32-bit applications (unless on Android), even if a GPU is of
                                 sufficient capability.
                              </p>
                              <p class="p">Devices of compute capability 6.x on supporting platforms can access pageable memory without calling cudaHostRegister on it.
                                 An application can query whether the device supports coherently accessing pageable memory by checking the new <samp class="ph codeph">pageableMemoryAccess</samp> property.
                              </p>
                              <p class="p">With the new page fault mechanism, global data coherency is guaranteed with Unified Memory. This means that the CPUs and GPUs
                                 can access Unified Memory allocations simultaneously. This was illegal on devices of compute capability lower than 6.x, because
                                 coherence could not be guaranteed if the CPU accessed a Unified Memory allocation while a GPU kernel was active. A program
                                 can query concurrent access support by checking <samp class="ph codeph">concurrentManagedAccess</samp> property. See <a class="xref" href="index.html#um-coherency-hd" shape="rect">Coherency and Concurrency</a> for details.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-pointer-attributes"><a name="um-pointer-attributes" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-pointer-attributes" name="um-pointer-attributes" shape="rect">M.2.4.2.&nbsp;Pointer Attributes</a></h3>
                           <div class="body conbody">
                              <p class="p">To determine if a given pointer refers to managed memory, a program can call
                                 <samp class="ph codeph">cudaPointerGetAttributes()</samp> and check the value of the
                                 <samp class="ph codeph">isManaged</samp> attribute. This attribute is set to 1 if the pointer
                                 refers to managed memory and to 0 if not.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-advanced-topics-hd"><a name="um-advanced-topics-hd" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-advanced-topics-hd" name="um-advanced-topics-hd" shape="rect">M.2.5.&nbsp;Advanced Topics</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-managed-memory"><a name="um-managed-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-managed-memory" name="um-managed-memory" shape="rect">M.2.5.1.&nbsp;Managed Memory with Multi-GPU Programs on pre-6.x Architectures</a></h3>
                           <div class="body conbody">
                              <p class="p">On systems with devices of compute capabilities lower than 6.x managed allocations are automatically visible to all GPUs in
                                 a system via the peer-to-peer capabilities of the GPUs. 
                              </p>
                              <p class="p">On Linux the managed memory is allocated in GPU memory as long as all GPUs that are actively being used by a program have
                                 the peer-to-peer support. If at any time the application starts using a GPU that doesnt have peer-to-peer support with any
                                 of the other GPUs that have managed allocations on them, then the driver will migrate all managed allocations to system memory.
                              </p>
                              <p class="p">On Windows if peer mappings are not available (for example, between GPUs
                                 of different architectures), then the system will automatically fall back to using zero-copy memory, regardless of
                                 whether both GPUs are actually used by a program. If only one GPU is actually going to be used, it is necessary to set the
                                 <samp class="ph codeph">CUDA_VISIBLE_DEVICES</samp> environment variable before launching the
                                 program. This constrains which GPUs are visible and allows managed memory to be
                                 allocated in GPU memory.
                              </p>
                              <p class="p">
                                 Alternatively, on Windows users can also set <samp class="ph codeph">CUDA_MANAGED_FORCE_DEVICE_ALLOC</samp> to a non-zero
                                 value to force the driver to always use device memory for physical storage.
                                 When this environment variable is set to a non-zero value, all devices used in
                                 that process that support managed memory have to be peer-to-peer compatible
                                 with each other. The error ::cudaErrorInvalidDevice will be returned if a device
                                 that supports managed memory is used and it is not peer-to-peer compatible with
                                 any of the other managed memory supporting devices that were previously used in
                                 that process, even if ::cudaDeviceReset has been called on those devices. These
                                 environment variables are described in Appendix <a class="xref" href="index.html#env-vars" shape="rect">CUDA Environment Variables</a>. 
                                 Note that starting from CUDA 8.0 <samp class="ph codeph">CUDA_MANAGED_FORCE_DEVICE_ALLOC</samp> has no effect on Linux operating systems.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="um-fork-managed-memory"><a name="um-fork-managed-memory" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#um-fork-managed-memory" name="um-fork-managed-memory" shape="rect">M.2.5.2.&nbsp;Using <samp class="ph codeph">fork()</samp> with Managed Memory</a></h3>
                           <div class="body conbody">
                              <p class="p">The Unified Memory system does not allow sharing of managed memory pointers between processes.
                                 It will not correctly manage memory handles that have been duplicated via a
                                 <samp class="ph codeph">fork()</samp> operation. Results will be undefined if either the child or
                                 parent accesses managed data following a <samp class="ph codeph">fork()</samp>.
                              </p>
                              <p class="p">It is safe, however, to <samp class="ph codeph">fork()</samp> a child process that then immediately
                                 exits via an <samp class="ph codeph">exec()</samp> call, because the child drops the memory handles
                                 and the parent becomes the sole owner once again. It is not safe for the parent to exit
                                 and leave the child to access the handles.
                              </p>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="um-performance-tuning"><a name="um-performance-tuning" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#um-performance-tuning" name="um-performance-tuning" shape="rect">M.3.&nbsp;Performance Tuning</a></h3>
                     <div class="body conbody">
                        <div class="p">In order to achieve good performance with Unified Memory, the following objectives must be met:<a name="um-performance-tuning__ul_um_perf_tuning" shape="rect">
                              <!-- --></a><ul class="ul" id="um-performance-tuning__ul_um_perf_tuning">
                              <li class="li"><cite class="cite">Faults should be avoided</cite>: While replayable faults are fundamental to enabling a simpler programming model, they can be severely detrimental to application
                                 performance. Fault handling can take tens of microseconds because it may involve TLB invalidates, data migrations and page
                                 table updates. All the while, execution in certain portions of the application will be halted, thereby potentially impacting
                                 overall performance.
                              </li>
                              <li class="li"><cite class="cite">Data should be local to the accessing processor</cite>: As mentioned before, memory access latencies and bandwidth are significantly better when the data is placed local to the
                                 processor accessing it. Therefore, data should be suitably migrated to take advantage of lower latencies and higher bandwidth.
                              </li>
                              <li class="li"><cite class="cite">Memory thrashing should be prevented</cite>: If data is frequently accessed by multiple processors and has to be constantly migrated around to achieve data locality,
                                 then the overhead of migration may exceed the benefits of locality. Memory thrashing should be prevented to the extent possible.
                                 If it cannot be prevented, it must be detected and resolved appropriately.
                              </li>
                           </ul>
                        </div>
                        <p class="p">To achieve the same level of performance as what's possible without using Unified Memory, the application has to guide the
                           Unified Memory driver subsystem into avoiding the aforementioned pitfalls. It is worthy to note that the Unified Memory driver
                           subsystem can detect common data access patterns and achieve some of these objectives automatically without application participation.
                           But when the data access patterns are non-obvious, explicit guidance from the application is crucial. CUDA 8.0 introduces
                           useful APIs for providing the runtime with memory usage hints (<samp class="ph codeph">cudaMemAdvise()</samp>) and for explicit prefetching (<samp class="ph codeph">cudaMemPrefetchAsync()</samp>). These tools allow the same capabilities as explicit memory copy and pinning APIs without reverting to the limitations of
                           explicit GPU memory allocation.
                        </p>
                        <div class="note note"><span class="notetitle">Note:</span><samp class="ph codeph">cudaMemPrefetchAsync()</samp> is not supported on Tegra devices.
                           
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-tuning-prefetch"><a name="um-tuning-prefetch" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-tuning-prefetch" name="um-tuning-prefetch" shape="rect">M.3.1.&nbsp;Data Prefetching</a></h3>
                        <div class="body conbody">
                           <div class="p">Data prefetching means migrating data to a processors memory and mapping it in that processors page tables before the processor
                              begins accessing that data. The intent of data prefetching is to avoid faults while also establishing data locality. This
                              is most valuable for applications that access data primarily from a single processor at any given time. As the accessing processor
                              changes during the lifetime of the application, the data can be prefetched accordingly to follow the execution flow of the
                              application. Since work is launched in streams in CUDA, it is expected of data prefetching to also be a streamed operation
                              as shown in the following API:
                              <pre xml:space="preserve">
    cudaError_t cudaMemPrefetchAsync(const void *devPtr, 
                                     size_t count, 
                                     int dstDevice, 
                                     cudaStream_t stream);</pre>
                              where the memory region specified by <samp class="ph codeph">devPtr</samp> pointer and <samp class="ph codeph">count</samp> number of bytes, with <samp class="ph codeph">ptr</samp> rounded down to the nearest page boundary and <samp class="ph codeph">count</samp> rounded up to the nearest page boundary, is migrated to the <samp class="ph codeph">dstDevice</samp> by enqueueing a migration operation in <samp class="ph codeph">stream</samp>. Passing in <samp class="ph codeph">cudaCpuDeviceId</samp> for <samp class="ph codeph">dstDevice</samp> will cause data to be migrated to CPU memory. 
                              
                           </div>
                           <div class="p">Consider a simple code example below:<pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">void</span> foo(cudaStream_t s) {
  <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *data;
  cudaMallocManaged(&amp;data, N);
  init_data(data, N);                                   <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// execute on CPU</span>
  cudaMemPrefetchAsync(data, N, myGpuId, s);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// prefetch to GPU</span>
  mykernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>..., s<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>(data, N, 1, compare);            <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// execute on GPU</span>
  cudaMemPrefetchAsync(data, N, cudaCpuDeviceId, s);    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// prefetch to CPU</span>
  cudaStreamSynchronize(s);
  use_data(data, N);
  cudaFree(data);
}</pre>Without performance hints the kernel <samp class="ph codeph">mykernel</samp> will fault on first access to <samp class="ph codeph">data</samp> which creates additional overhead of the fault processing and generally slows down the application. By prefetching <samp class="ph codeph">data</samp> in advance it is possible to avoid page faults and achieve better performance.
                           </div>
                           <p class="p">This API follows stream ordering semantics, i.e. the migration does not begin until all prior operations in the stream have
                              completed, and any subsequent operation in the stream does not begin until the migration has completed.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-tuning-usage"><a name="um-tuning-usage" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-tuning-usage" name="um-tuning-usage" shape="rect">M.3.2.&nbsp;Data Usage Hints</a></h3>
                        <div class="body conbody">
                           <div class="p">Data prefetching alone is insufficient when multiple processors need to simultaneously access the same data. In such scenarios,
                              it's useful for the application to provide hints on how the data will actually be used. The following advisory API can be
                              used to specify data usage:<pre xml:space="preserve">
    cudaError_t cudaMemAdvise(const void *devPtr, 
                              size_t count, 
                              enum cudaMemoryAdvise advice, 
                              int device);</pre>
                              where <samp class="ph codeph">advice</samp>, specified for data contained in region starting from <samp class="ph codeph">devPtr</samp> address and with the length of <samp class="ph codeph">count</samp> bytes, rounded to the nearest page boundary, can take the following values:
                           </div>
                           <ul class="ul">
                              <li class="li"><samp class="ph codeph">cudaMemAdviseSetReadMostly</samp>: This implies that the data is mostly going to be read from and only occasionally written to. This allows the driver to create
                                 read-only copies of the data in a processor's memory when that processor accesses it. Similarly, if <samp class="ph codeph">cudaMemPrefetchAsync</samp> is called on this region, it will create a read-only copy of the data on the destination processor. When a processor writes
                                 to this data, all copies of the corresponding page are invalidated except for the one where the write occurred. The <samp class="ph codeph">device</samp> argument is ignored for this advice.
                                 This advice allows multiple processors to simultaneously access the same data at maximal bandwidth as illustrated in the following
                                 code snippet:
                                 <pre xml:space="preserve"><span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *dataPtr;
size_t dataSize = 4096;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Allocate memory using malloc or cudaMallocManaged</span>
dataPtr = (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *)malloc(dataSize);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Set the advice on the memory region</span>
cudaMemAdvise(dataPtr, dataSize, cudaMemAdviseSetReadMostly, 0);
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> outerLoopIter = 0;
<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (outerLoopIter &lt; maxOuterLoopIter) {
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The data is written to in the outer loop on the CPU</span>
    initializeData(dataPtr, dataSize);
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The data is made available to all GPUs by prefetching.</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// Prefetching here causes read duplication of data instead</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// of data migration</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">for</span> (<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> device = 0; device &lt; maxDevices; device++) {
        cudaMemPrefetchAsync(dataPtr, dataSize, device, stream);
    }
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-comment">// The kernel only reads this data in the inner loop</span>
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">int</span> innerLoopIter = 0;
    <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">while</span> (innerLoopIter &lt; maxInnerLoopIter) {
        kernel<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&lt;&lt;&lt;</span>32,32<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-attribute">&gt;&gt;&gt;</span>((<span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">const</span> <span xmlns:xslthl="http://xslthl.sf.net" class="xslthl-keyword">char</span> *)dataPtr);
        innerLoopIter++;
    }
    outerLoopIter++;
}</pre></li>
                              <li class="li"><samp class="ph codeph">cudaMemAdviseSetPreferredLocation</samp>: This advice sets the preferred location for the data to be the memory belonging to <samp class="ph codeph">device</samp>. Passing in a value of <samp class="ph codeph">cudaCpuDeviceId</samp> for <samp class="ph codeph">device</samp> sets the preferred location as CPU memory. Setting the preferred location does not cause data to migrate to that location
                                 immediately. Instead, it guides the migration policy when a fault occurs on that memory region. If the data is already in
                                 its preferred location and the faulting processor can establish a mapping without requiring the data to be migrated, then
                                 the migration will be avoided. On the other hand, if the data is not in its preferred location or if a direct mapping cannot
                                 be established, then it will be migrated to the processor accessing it. It is important to note that setting the preferred
                                 location does not prevent data prefetching done using <samp class="ph codeph">cudaMemPrefetchAsync</samp>.
                              </li>
                              <li class="li"><samp class="ph codeph">cudaMemAdviseSetAccessedBy</samp>: This advice implies that the data will be accessed by <samp class="ph codeph">device</samp>. This does not cause data migration and has no impact on the location of the data per se. Instead, it causes the data to
                                 always be mapped in the specified processors page tables, as long as the location of the data permits a mapping to be established.
                                 If the data gets migrated for any reason, the mappings are updated accordingly. 
                                 This advice is useful in scenarios where data locality is not important, but avoiding faults is. Consider for example a system
                                 containing multiple GPUs with peer-to-peer access enabled, where the data located on one GPU is occasionally accessed by other
                                 GPUs. In such scenarios, migrating data over to the other GPUs is not as important because the accesses are infrequent and
                                 the overhead of migration may be too high. But preventing faults can still help improve performance, and so having a mapping
                                 set up in advance is useful. Note that on CPU access of this data, the data may be migrated to CPU memory because the CPU
                                 cannot access GPU memory directly. Any GPU that had the <samp class="ph codeph">cudaMemAdviceSetAccessedBy</samp> flag set for this data will now have its mapping updated to point to the page in CPU memory.
                                 
                              </li>
                           </ul>
                           <p class="p">Each advice can be also unset by using one of the following values: <samp class="ph codeph">cudaMemAdviseUnsetReadMostly</samp>, <samp class="ph codeph">cudaMemAdviseUnsetPreferredLocation</samp> and <samp class="ph codeph">cudaMemAdviseUnsetAccessedBy</samp>.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="um-querying-usage"><a name="um-querying-usage" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#um-querying-usage" name="um-querying-usage" shape="rect">M.3.3.&nbsp;Querying Usage Attributes</a></h3>
                        <div class="body conbody">
                           <div class="p">A program can query memory range attributes assigned through <samp class="ph codeph">cudaMemAdvise</samp> or <samp class="ph codeph">cudaMemPrefetchAsync</samp> by using the following API:<pre xml:space="preserve">
    cudaMemRangeGetAttribute(void *data, 
                             size_t dataSize, 
                             enum cudaMemRangeAttribute attribute, 
                             const void *devPtr, 
                             size_t count);</pre>
                              This function queries an attribute of the memory range starting at <samp class="ph codeph">devPtr</samp> with a size of <samp class="ph codeph">count</samp> bytes. The memory range must refer to managed memory allocated via <samp class="ph codeph">cudaMallocManaged</samp> or declared via <samp class="ph codeph">__managed__</samp> variables. It is possible to query the following attributes:
                              <ul class="ul">
                                 <li class="li"><samp class="ph codeph">cudaMemRangeAttributeReadMostly</samp>:  the result returned will be 1 if all pages in the given memory range have read-duplication enabled, or 0 otherwise.
                                 </li>
                                 <li class="li"><samp class="ph codeph">cudaMemRangeAttributePreferredLocation</samp>: the result returned will be a GPU device id or <samp class="ph codeph">cudaCpuDeviceId</samp> if all pages in the memory range have the corresponding processor as their preferred location, otherwise <samp class="ph codeph">cudaInvalidDeviceId</samp> will be returned.  An application can use this query API to make decision about staging data through CPU or GPU depending
                                    on the preferred location attribute of the managed pointer. Note that the actual location of the pages in the memory range
                                    at the time of the query may be different from the preferred location.
                                 </li>
                                 <li class="li"><samp class="ph codeph">cudaMemRangeAttributeAccessedBy</samp>: will return the list of devices that have that advise set for that memory range.
                                 </li>
                                 <li class="li"><samp class="ph codeph">cudaMemRangeAttributeLastPrefetchLocation</samp>: will return the last location to which all pages in the memory range were prefetched explicitly using <samp class="ph codeph">cudaMemPrefetchAsync</samp>. Note that this simply returns the last location that the application requested to prefetch the memory range to. It gives
                                    no indication as to whether the prefetch operation to that location has completed or even begun.
                                 </li>
                              </ul>
                           </div>
                           <p class="p">Additionally, multiple attributes can be queried by using corresponding <samp class="ph codeph">cudaMemRangeGetAttributes</samp> function. 
                           </p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" id="notices-header"><a name="notices-header" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#notices-header" name="notices-header" shape="rect">Notices</a></h2>
                  <div class="topic reference nested1" id="notice"><a name="notice" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#notice" name="notice" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">Notice</h3>
                           <p class="p" id="notice__notice-para-1"><a name="notice__notice-para-1" shape="rect">
                                 <!-- --></a>This document is provided for information
                              purposes only and shall not be regarded as a warranty of a
                              certain functionality, condition, or quality of a product.
                              NVIDIA Corporation (NVIDIA) makes no representations or
                              warranties, expressed or implied, as to the accuracy or
                              completeness of the information contained in this document
                              and assumes no responsibility for any errors contained
                              herein. NVIDIA shall have no liability for the consequences
                              or use of such information or for any infringement of
                              patents or other rights of third parties that may result
                              from its use. This document is not a commitment to develop,
                              release, or deliver any Material (defined below), code, or
                              functionality.
                           </p>
                           <p class="p" id="notice__notice-para-2"><a name="notice__notice-para-2" shape="rect">
                                 <!-- --></a>NVIDIA reserves the right to make corrections, modifications,
                              enhancements, improvements, and any other changes to this
                              document, at any time without notice.
                           </p>
                           <p class="p" id="notice__notice-para-3"><a name="notice__notice-para-3" shape="rect">
                                 <!-- --></a>Customer should obtain the latest relevant information before
                              placing orders and should verify that such information is
                              current and complete.
                           </p>
                           <p class="p" id="notice__notice-para-4"><a name="notice__notice-para-4" shape="rect">
                                 <!-- --></a>NVIDIA products are sold subject to the NVIDIA standard terms and
                              conditions of sale supplied at the time of order
                              acknowledgement, unless otherwise agreed in an individual
                              sales agreement signed by authorized representatives of
                              NVIDIA and customer (Terms of Sale). NVIDIA hereby
                              expressly objects to applying any customer general terms and
                              conditions with regards to the purchase of the NVIDIA
                              product referenced in this document. No contractual
                              obligations are formed either directly or indirectly by this
                              document.
                           </p>
                           <p class="p" id="notice__notice-para-5"><a name="notice__notice-para-5" shape="rect">
                                 <!-- --></a>NVIDIA products are not designed, authorized, or warranted to be
                              suitable for use in medical, military, aircraft, space, or
                              life support equipment, nor in applications where failure or
                              malfunction of the NVIDIA product can reasonably be expected
                              to result in personal injury, death, or property or
                              environmental damage. NVIDIA accepts no liability for
                              inclusion and/or use of NVIDIA products in such equipment or
                              applications and therefore such inclusion and/or use is at
                              customers own risk.
                           </p>
                           <p class="p" id="notice__notice-para-6"><a name="notice__notice-para-6" shape="rect">
                                 <!-- --></a>NVIDIA makes no representation or warranty that products based on
                              this document will be suitable for any specified use.
                              Testing of all parameters of each product is not necessarily
                              performed by NVIDIA. It is customers sole responsibility to
                              evaluate and determine the applicability of any information
                              contained in this document, ensure the product is suitable
                              and fit for the application planned by customer, and perform
                              the necessary testing for the application in order to avoid
                              a default of the application or the product. Weaknesses in
                              customers product designs may affect the quality and
                              reliability of the NVIDIA product and may result in
                              additional or different conditions and/or requirements
                              beyond those contained in this document. NVIDIA accepts no
                              liability related to any default, damage, costs, or problem
                              which may be based on or attributable to: (i) the use of the
                              NVIDIA product in any manner that is contrary to this
                              document or (ii) customer product designs.
                           </p>
                           <p class="p" id="notice__notice-para-7"><a name="notice__notice-para-7" shape="rect">
                                 <!-- --></a>No license, either expressed or implied, is granted under any NVIDIA
                              patent right, copyright, or other NVIDIA intellectual
                              property right under this document. Information published by
                              NVIDIA regarding third-party products or services does not
                              constitute a license from NVIDIA to use such products or
                              services or a warranty or endorsement thereof. Use of such
                              information may require a license from a third party under
                              the patents or other intellectual property rights of the
                              third party, or a license from NVIDIA under the patents or
                              other intellectual property rights of NVIDIA.
                           </p>
                           <p class="p" id="notice__notice-para-8"><a name="notice__notice-para-8" shape="rect">
                                 <!-- --></a>Reproduction of information in this document is permissible only if
                              approved in advance by NVIDIA in writing, reproduced without
                              alteration and in full compliance with all applicable export
                              laws and regulations, and accompanied by all associated
                              conditions, limitations, and notices.
                           </p>
                           <p class="p" id="notice__notice-para-9"><a name="notice__notice-para-9" shape="rect">
                                 <!-- --></a>THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE
                              BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER
                              DOCUMENTS (TOGETHER AND SEPARATELY, MATERIALS) ARE BEING
                              PROVIDED AS IS. NVIDIA MAKES NO WARRANTIES, EXPRESSED,
                              IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE
                              MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF
                              NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A
                              PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN
                              NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING
                              WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL,
                              INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER
                              CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING
                              OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN
                              ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding
                              any damages that customer might incur for any reason
                              whatsoever, NVIDIAs aggregate and cumulative liability
                              towards customer for the products described herein shall be
                              limited in accordance with the Terms of Sale for the
                              product.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="vesa"><a name="vesa" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#vesa" name="vesa" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">VESA DisplayPort</h3>
                           <p class="p">DisplayPort and DisplayPort Compliance Logo, DisplayPort Compliance Logo for
                              Dual-mode Sources, and DisplayPort Compliance Logo for Active Cables are
                              trademarks owned by the Video Electronics Standards Association in the United
                              States and other countries.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="hdmi"><a name="hdmi" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#hdmi" name="hdmi" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">HDMI</h3>
                           <p class="p">HDMI, the HDMI logo, and High-Definition Multimedia Interface are trademarks or
                              registered trademarks of HDMI Licensing LLC.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="opencl"><a name="opencl" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#opencl" name="opencl" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">OpenCL</h3>
                           <p class="p">OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.</p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="trademarks"><a name="trademarks" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#trademarks" name="trademarks" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">Trademarks</h3>
                           <p class="p">NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation
                              in the U.S. and other countries.  Other company and product names may be trademarks of
                              the respective companies with which they are associated.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="copyright-past-to-present"><a name="copyright-past-to-present" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#copyright-past-to-present" name="copyright-past-to-present" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section">
                           <h3 class="title sectiontitle">Copyright</h3>
                           <p class="p"> <span class="ph">2007</span>-<span class="ph">2021</span> NVIDIA
                              Corporation. All rights reserved.
                           </p>
                           <p class="p">This product includes software developed by the Syncro Soft SRL (http://www.sync.ro/).</p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="fn"><a name="fntarg_1" href="#fnsrc_1" shape="rect"><sup>1</sup></a>  
                  The <dfn class="term">graphics</dfn> qualifier comes from the fact that when the GPU was originally
                  created, two decades ago, it was designed as a specialized processor to accelerate
                  graphics rendering. Driven by the insatiable market demand for real-time, high-definition,
                  3D graphics, it has evolved into a general processor used for many more workloads than
                  just graphics rendering.
                  
               </div>
               <div class="fn"><a name="fntarg_2" href="#fnsrc_2" shape="rect"><sup>2</sup></a>  The term <dfn class="term">warp-synchronous</dfn> refers to code that implicitly assumes threads in the same warp are synchronized at every instruction.
               </div>
               <div class="fn"><a name="fntarg_3" href="#fnsrc_3" shape="rect"><sup>3</sup></a>  128 for __nv_bfloat16
               </div>
               <div class="fn"><a name="fntarg_4" href="#fnsrc_4" shape="rect"><sup>4</sup></a>  8 for GeForce GPUs, except for Titan GPUs
               </div>
               <div class="fn"><a name="fntarg_5" href="#fnsrc_5" shape="rect"><sup>5</sup></a>  2 for compute capability 7.5 GPUs
               </div>
               <div class="fn"><a name="fntarg_6" href="#fnsrc_6" shape="rect"><sup>6</sup></a>  32 for extended-precision
               </div>
               <div class="fn"><a name="fntarg_7" href="#fnsrc_7" shape="rect"><sup>7</sup></a>  32 for GeForce GPUs, except for Titan GPUs
               </div>
               <div class="fn"><a name="fntarg_8" href="#fnsrc_8" shape="rect"><sup>8</sup></a>  16 for compute capabilities 7.5 GPUs
               </div>
               <div class="fn"><a name="fntarg_9" href="#fnsrc_9" shape="rect"><sup>9</sup></a>  8 for GeForce GPUs, except for Titan GPUs
               </div>
               <div class="fn"><a name="fntarg_10" href="#fnsrc_10" shape="rect"><sup>10</sup></a>  2 for compute capabilities 7.5 GPUs
               </div>
               <div class="fn"><a name="fntarg_11" href="#fnsrc_11" shape="rect"><sup>11</sup></a>  
                  When the enclosing __host__ function is a template, nvcc
                  may currently fail to issue a diagnostic message in some
                  cases; this behavior may change in the future.
                  
               </div>
               <div class="fn"><a name="fntarg_12" href="#fnsrc_12" shape="rect"><sup>12</sup></a>  The intent is to prevent the host compiler from encountering the call to the function if 
                  the host compiler does not support it.
               </div>
               <div class="fn"><a name="fntarg_13" href="#fnsrc_13" shape="rect"><sup>13</sup></a>  See the C++ Standard for definition of integral
                  constant expression.
               </div>
               <div class="fn"><a name="fntarg_14" href="#fnsrc_14" shape="rect"><sup>1</sup></a>   Dynamically created texture and surface objects are an
                  addition to the CUDA memory model introduced with CUDA 5.0. Please see
                  the <em class="ph i">CUDA Programming Guide</em> for details.
               </div>
               <div class="fn"><a name="fntarg_15" href="#fnsrc_15" shape="rect"><sup>15</sup></a>  e.g., the <samp class="ph codeph">&lt;&lt;&lt;...&gt;&gt;&gt;</samp>  syntax for launching kernels.
               </div>
               <div class="fn"><a name="fntarg_16" href="#fnsrc_16" shape="rect"><sup>16</sup></a>  This does not apply to entities that may be defined in more 
                  than one translation unit, such as compiler generated 
                  template instantiations.
                  
               </div>
               <div class="fn"><a name="fntarg_17" href="#fnsrc_17" shape="rect"><sup>17</sup></a>  The intent is to allow variable memory space specifiers
                  for static variables in a <samp class="ph codeph">__host__ __device__</samp>
                  function during device  compilation, but disallow it during 
                  host compilation
               </div>
               <div class="fn"><a name="fntarg_18" href="#fnsrc_18" shape="rect"><sup>18</sup></a>  supported with architectures &gt;= sm_35
               </div>
               <div class="fn"><a name="fntarg_19" href="#fnsrc_19" shape="rect"><sup>19</sup></a>  One way to debug suspected layout mismatch of a type <samp class="ph codeph">C</samp> is to use <samp class="ph codeph">printf</samp> to output the values of 
                  <samp class="ph codeph">sizeof(C)</samp> and <samp class="ph codeph">offsetof(C, field)</samp> in host and device code. 
                  
               </div>
               <div class="fn"><a name="fntarg_20" href="#fnsrc_20" shape="rect"><sup>20</sup></a>  At
                  present, the <samp class="ph codeph">-std=c++11</samp> flag is supported only for the following
                  host compilers : gcc version &gt;= 4.7, clang, icc &gt;= 15, and xlc &gt;= 13.1
               </div>
               <div class="fn"><a name="fntarg_21" href="#fnsrc_21" shape="rect"><sup>21</sup></a>  including <samp class="ph codeph">operator()</samp></div>
               <div class="fn"><a name="fntarg_22" href="#fnsrc_22" shape="rect"><sup>22</sup></a>  The restrictions are the same as with a non-constexpr callee function.
               </div>
               <div class="fn"><a name="fntarg_23" href="#fnsrc_23" shape="rect"><sup>23</sup></a>  Note that the behavior of experimental flags may change in future compiler releases.
               </div>
               <div class="fn"><a name="fntarg_24" href="#fnsrc_24" shape="rect"><sup>24</sup></a>  C++ Standard Section <samp class="ph codeph">[basic.types]</samp></div>
               <div class="fn"><a name="fntarg_25" href="#fnsrc_25" shape="rect"><sup>25</sup></a>  C++ Standard Section <samp class="ph codeph">[expr.const]</samp></div>
               <div class="fn"><a name="fntarg_26" href="#fnsrc_26" shape="rect"><sup>26</sup></a>  At present,
                  the <samp class="ph codeph">-std=c++14</samp> flag is supported only for the following host
                  compilers : gcc version &gt;= 5.1,  clang version &gt;= 3.7 and icc version &gt;= 17
               </div>
               <div class="fn"><a name="fntarg_27" href="#fnsrc_27" shape="rect"><sup>27</sup></a>  At present,
                  the <samp class="ph codeph">-std=c++17</samp> flag is supported only for the following host
                  compilers : gcc version &gt;= 7.0,  clang version &gt;= 8.0, 
                  Visual Studio version &gt;= 2017, pgi compiler version &gt;= 19.0, 
                  icc compiler version &gt;= 19.0 
               </div>
               <div class="fn"><a name="fntarg_28" href="#fnsrc_28" shape="rect"><sup>28</sup></a>  When using the icc host compiler, this flag is only supported for icc &gt;= 1800. 
               </div>
               <div class="fn"><a name="fntarg_29" href="#fnsrc_29" shape="rect"><sup>29</sup></a>  The traits will always return false if extended lambda mode is not active.
               </div>
               <div class="fn"><a name="fntarg_30" href="#fnsrc_30" shape="rect"><sup>30</sup></a>  In contrast, the C++ standard specifies that the
                  captured variable is used to direct-initialize the field of the closure type.
               </div>
               <div class="fn"><a name="fntarg_31" href="#fnsrc_31" shape="rect"><sup>31</sup></a>  The closure object is stored in a type-elided container similar to <samp class="ph codeph">std::function</samp>.
               </div>
               <div class="fn"><a name="fntarg_32" href="#fnsrc_32" shape="rect"><sup>32</sup></a>  above 48 KB requires dynamic shared memory
               </div>
               <div class="fn"><a name="fntarg_33" href="#fnsrc_33" shape="rect"><sup>33</sup></a>  2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5
               </div>
               
               <hr id="contents-end"></hr>
               
            </article>
         </div>
      </div>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/formatting/common.min.js"></script>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/scripts/google-analytics/google-analytics-write.js"></script>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/scripts/google-analytics/google-analytics-tracker.js"></script>
      <script type="text/javascript">var switchTo5x=true;</script><script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script><script type="text/javascript">stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script><script type="text/javascript">_satellite.pageBottom();</script></body>
</html>